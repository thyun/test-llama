{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e389292-7c2a-487c-96b0-b7b97d25e6a5",
   "metadata": {},
   "source": [
    "* llama2를 ggmlv3로 변환해서 사용해 보기\n",
    "  * llama2 model download - https://huggingface.co/meta-llama/Llama-2-7b-hf\n",
    "  * 참고 gguf - https://huggingface.co/TheBloke/Llama-2-7B-GGUF\n",
    "* koalpaca를 ggmlv3로 변환해서 사용해 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bbe4ad-a072-47c7-8a40-6ccaebdf3adc",
   "metadata": {},
   "source": [
    "# KoAlpaca + pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67394d40-823e-4ab2-8401-0f734303816b",
   "metadata": {},
   "source": [
    "* https://github.com/Beomi/KoAlpaca\n",
    "* pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe1f49ae-2e8a-4873-9446-dda5ca7bf53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python==0.1.78 in /home/thyun/miniconda3/envs/nlu-ggmlv3/lib/python3.10/site-packages (0.1.78)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/thyun/miniconda3/envs/nlu-ggmlv3/lib/python3.10/site-packages (from llama-cpp-python==0.1.78) (4.8.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/thyun/miniconda3/envs/nlu-ggmlv3/lib/python3.10/site-packages (from llama-cpp-python==0.1.78) (1.26.0)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /home/thyun/miniconda3/envs/nlu-ggmlv3/lib/python3.10/site-packages (from llama-cpp-python==0.1.78) (5.6.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python==0.1.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3e3ebae-7108-416c-8286-b500a67ae3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/1a/06/3817f9bb923437ead9a794f0ac0d03b8b5e0478ab112db4c413dd37c09da/transformers-4.33.2-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\n",
      "Collecting tokenizers\n",
      "  Obtaining dependency information for tokenizers from https://files.pythonhosted.org/packages/57/bd/45b5ef6b088880779f70acf60027f7043ca5fa1b98f4a4345cf3aea09044/tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting accelerate\n",
      "  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/d9/92/2d3aecf9f4a192968035880be3e2fc8b48d541c7128f7c936f430d6f96da/accelerate-0.23.0-py3-none-any.whl.metadata\n",
      "  Using cached accelerate-0.23.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting safetensors\n",
      "  Obtaining dependency information for safetensors from https://files.pythonhosted.org/packages/6c/f0/c17bbdb1e5f9dab29d44cade445135789f75f8f08ea2728d04493ea8412b/safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/5e/5d/97afbafd9d584ff1b45fcb354a479a3609bd97f912f8f1f6c563cb1fae21/filelock-3.12.4-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.12.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/thyun/miniconda3/envs/nlu-ggmlv3/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: jinja2 in /home/thyun/miniconda3/envs/nlu-ggmlv3/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch)\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch)\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
      "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch)\n",
      "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch)\n",
      "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch)\n",
      "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch)\n",
      "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch)\n",
      "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Collecting triton==2.0.0 (from torch)\n",
      "  Using cached triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "Requirement already satisfied: setuptools in /home/thyun/miniconda3/envs/nlu-ggmlv3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (68.0.0)\n",
      "Requirement already satisfied: wheel in /home/thyun/miniconda3/envs/nlu-ggmlv3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Collecting cmake (from triton==2.0.0->torch)\n",
      "  Obtaining dependency information for cmake from https://files.pythonhosted.org/packages/de/94/cba4b3ddc0d4555967cce8fd6e9fbced98a6bf62857db71c2400a7b6e183/cmake-3.27.5-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached cmake-3.27.5-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lit (from triton==2.0.0->torch)\n",
      "  Using cached lit-16.0.6-py3-none-any.whl\n",
      "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.15.1 from https://files.pythonhosted.org/packages/72/21/51cddb8850ed3f4dbc21e57c3dabc49e64d5577857ddda7b2eb0ffc2ec0e/huggingface_hub-0.17.2-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/thyun/miniconda3/envs/nlu-ggmlv3/lib/python3.10/site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/thyun/miniconda3/envs/nlu-ggmlv3/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/thyun/miniconda3/envs/nlu-ggmlv3/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/d1/df/460ca6171a8494fcf37af43f52f6fac23e38784bb4a26563f6fa01ef6faf/regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/thyun/miniconda3/envs/nlu-ggmlv3/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Obtaining dependency information for tqdm>=4.27 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in /home/thyun/miniconda3/envs/nlu-ggmlv3/lib/python3.10/site-packages (from accelerate) (5.9.5)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.15.1->transformers)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/6a/af/c673e8c663e17bd4fb201a6f029153ad5d7023aa4442d81c7987743db379/fsspec-2023.9.1-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.9.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/thyun/miniconda3/envs/nlu-ggmlv3/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/thyun/miniconda3/envs/nlu-ggmlv3/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/thyun/miniconda3/envs/nlu-ggmlv3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/thyun/miniconda3/envs/nlu-ggmlv3/lib/python3.10/site-packages (from requests->transformers) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/thyun/miniconda3/envs/nlu-ggmlv3/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
      "Using cached accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
      "Using cached safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.12.4-py3-none-any.whl (11 kB)\n",
      "Using cached cmake-3.27.5-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)\n",
      "Downloading fsspec-2023.9.1-py3-none-any.whl (173 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.4/173.4 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, safetensors, mpmath, lit, cmake, tqdm, sympy, regex, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, networkx, fsspec, filelock, nvidia-cusolver-cu11, nvidia-cudnn-cu11, huggingface-hub, transformers, triton, torch, accelerate\n",
      "Successfully installed accelerate-0.23.0 cmake-3.27.5 filelock-3.12.4 fsspec-2023.9.1 huggingface-hub-0.17.2 lit-16.0.6 mpmath-1.3.0 networkx-3.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 regex-2023.8.8 safetensors-0.3.3 sympy-1.12 tokenizers-0.13.3 torch-2.0.1 tqdm-4.66.1 transformers-4.33.2 triton-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U torch transformers tokenizers accelerate safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d036d8b-1e64-4c7c-8c4f-24508155970e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1173c170620430580f8053c59b7ab36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thyun/miniconda3/envs/nlu-ggmlv3/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "딥러닝은 인공 신경망을 통해 더 복잡한 문제를 해결하는 기술입니다. 이를 위해 여러 층의 인공 신경망을 만들어 데이터를 입력하고 결과를 출력합니다. 이 과정에서 다양한 알고리즘을 사용하며, 이를 통해 보다 복잡한 문제를 해결합니다. 예를 들어, 이미지 인식과 같은 분야에서 딥러닝을 사용합니다. \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM\n",
    "\n",
    "MODEL = 'beomi/KoAlpaca-Polyglot-5.8B'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(device=f\"cuda\", non_blocking=True)\n",
    "model.eval()\n",
    "\n",
    "pipe = pipeline(\n",
    "    'text-generation', \n",
    "    model=model,\n",
    "    tokenizer=MODEL,\n",
    "    device=0\n",
    ")\n",
    "\n",
    "def ask(x, context='', is_input_full=False):\n",
    "    ans = pipe(\n",
    "        f\"### 질문: {x}\\n\\n### 맥락: {context}\\n\\n### 답변:\" if context else f\"### 질문: {x}\\n\\n### 답변:\", \n",
    "        do_sample=True, \n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        return_full_text=False,\n",
    "        eos_token_id=2,\n",
    "    )\n",
    "    print(ans[0]['generated_text'])\n",
    "\n",
    "ask(\"딥러닝이 뭐야?\")\n",
    "# 딥러닝은 인공신경망을 통해 입력과 출력 사이의 복잡한 관계를 학습하는 머신러닝의 한 분야입니다. 이 기술은 컴퓨터가 인간의 학습 능력과 유사한 방식으로 패턴을 학습하도록 하며, 인간의 개입 없이도 데이터를 처리할 수 있는 기술입니다. 최근에는 딥러닝을 활용한 인공지능 애플리케이션이 많이 개발되고 있습니다. 예를 들어, 의료 진단 애플리케이션에서는 딥러닝 기술을 활용하여 환자의 특징을 파악하고, 이를 통해 빠르고 정확한 진단을 내리는 데 사용됩니다. 또한, 금융 분야에서는 딥러닝 기술을 활용하여 주가 예측 모형을 학습하는 데 사용되기도 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f300337c-e454-4eb1-840b-901363c4779c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a858e2ec-3e7f-45dc-93cb-dfe01435e3c9",
   "metadata": {},
   "source": [
    "# llama 2 + llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834123a5-b848-4332-add2-adbcb9a76ba2",
   "metadata": {},
   "source": [
    "## llama 2 ggmlv3 - 성공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b2f5b2f-3819-48a4-a862-3f9df51cf4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../models/llama-2-13b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 6983.72 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "llama_new_context_with_model: compute buffer total size =   75.35 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-59208a2f-58b2-4f6b-b144-55a0fab77eee', 'object': 'text_completion', 'created': 1695342647, 'model': '../models/llama-2-13b-chat.ggmlv3.q4_0.bin', 'choices': [{'text': 'Q: Name the planets in the solar system? A:  Sure! Here are the planets in our solar system, listed in order from closest to farthest from the Sun:', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 15, 'completion_tokens': 26, 'total_tokens': 41}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   777.23 ms\n",
      "llama_print_timings:      sample time =    15.34 ms /    26 runs   (    0.59 ms per token,  1694.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   777.20 ms /    15 tokens (   51.81 ms per token,    19.30 tokens per second)\n",
      "llama_print_timings:        eval time =  5129.10 ms /    25 runs   (  205.16 ms per token,     4.87 tokens per second)\n",
      "llama_print_timings:       total time =  5964.39 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1679561337,\n",
       " 'model': './models/7B/llama-model.gguf',\n",
       " 'choices': [{'text': 'Q: Name the planets in the solar system? A: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune and Pluto.',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 14, 'completion_tokens': 28, 'total_tokens': 42}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/abetlen/llama-cpp-python\n",
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"../models/llama-2-13b-chat.ggmlv3.q4_0.bin\")\n",
    "output = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=32, stop=[\"Q:\", \"\\n\"], echo=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d97965-d06d-4cb1-b252-e5f3b0036ec7",
   "metadata": {},
   "source": [
    "## llama 2 convert pytorch -> ggmlv3 - 성공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a97cf44-220c-433d-a938-eddd2bbdf846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 model - https://huggingface.co/meta-llama/Llama-2-13b-chat-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27e0c075-43f6-45fa-9a11-6fbd299ddaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../models/Llama-2-13b-chat-hf/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 6983.72 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =   75.35 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-e7083914-cea7-4813-b5f8-4f6d6ca86a63', 'object': 'text_completion', 'created': 1695365590, 'model': '../models/Llama-2-13b-chat-hf/ggml-model-q4_0.bin', 'choices': [{'text': 'Q: Name the planets in the solar system? A: 1. Mercury, 2. Venus, 3. Earth, 4. Mars, 5. Jupiter, 6. Saturn', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 15, 'completion_tokens': 32, 'total_tokens': 47}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  2156.65 ms\n",
      "llama_print_timings:      sample time =    19.77 ms /    32 runs   (    0.62 ms per token,  1618.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2156.55 ms /    15 tokens (  143.77 ms per token,     6.96 tokens per second)\n",
      "llama_print_timings:        eval time = 12099.30 ms /    31 runs   (  390.30 ms per token,     2.56 tokens per second)\n",
      "llama_print_timings:       total time = 14351.35 ms\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"../models/Llama-2-13b-chat-hf/ggml-model-q4_0.bin\")\n",
    "output = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=32, stop=[\"Q:\", \"\\n\"], echo=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff5ab5-dd15-4879-a955-f9e52a64f134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba3965f6-04a9-4cea-8012-066069e084fa",
   "metadata": {},
   "source": [
    "# KoAlpaca + llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ce3664-ecf2-4ede-b047-c924bbf3d532",
   "metadata": {},
   "source": [
    "## KoAlpaca ggmlv3 - 실패"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a1cb996-80a4-4e1b-8d7d-fa6e99cd4d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../models/kullm-polyglot-12.8B-v2.ggmlv3.q4_0.bin\n",
      "error loading model: unexpectedly reached end of file\n",
      "llama_load_model_from_file: failed to load model\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[0;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../models/kullm-polyglot-12.8B-v2.ggmlv3.q4_0.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m output \u001b[38;5;241m=\u001b[39m llm(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ: Name the planets in the solar system? A: \u001b[39m\u001b[38;5;124m\"\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m], echo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu-ggmlv3/lib/python3.10/site-packages/llama_cpp/llama.py:328\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[0;34m(self, model_path, n_ctx, n_parts, n_gpu_layers, seed, f16_kv, logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads, n_batch, last_n_tokens_size, lora_base, lora_path, low_vram, tensor_split, rope_freq_base, rope_freq_scale, n_gqa, rms_norm_eps, mul_mat_q, verbose)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress_stdout_stderr():\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_load_model_from_file(\n\u001b[1;32m    326\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\n\u001b[1;32m    327\u001b[0m         )\n\u001b[0;32m--> 328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_new_context_with_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# kullm-polyglot-12.8B-v2.ggmlv3.q4_0.bin\n",
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"../models/kullm-polyglot-12.8B-v2.ggmlv3.q4_0.bin\")\n",
    "output = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=32, stop=[\"Q:\", \"\\n\"], echo=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7664d079-3cf6-4095-b62b-d4a120541a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../models/KoAlpaca-Polyglot-12.8B-ggml-model-q5_1.bin\n",
      "error loading model: unexpectedly reached end of file\n",
      "llama_load_model_from_file: failed to load model\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# https://huggingface.co/squarelike/ggml-KoAlpaca-Polyglot-12.8B/resolve/main/KoAlpaca-Polyglot-12.8B-ggml-model-q5_1.bin\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[0;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../models/KoAlpaca-Polyglot-12.8B-ggml-model-q5_1.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m output \u001b[38;5;241m=\u001b[39m llm(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ: Name the planets in the solar system? A: \u001b[39m\u001b[38;5;124m\"\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m], echo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu-ggmlv3/lib/python3.10/site-packages/llama_cpp/llama.py:328\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[0;34m(self, model_path, n_ctx, n_parts, n_gpu_layers, seed, f16_kv, logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads, n_batch, last_n_tokens_size, lora_base, lora_path, low_vram, tensor_split, rope_freq_base, rope_freq_scale, n_gqa, rms_norm_eps, mul_mat_q, verbose)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress_stdout_stderr():\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_load_model_from_file(\n\u001b[1;32m    326\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\n\u001b[1;32m    327\u001b[0m         )\n\u001b[0;32m--> 328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_new_context_with_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/squarelike/ggml-KoAlpaca-Polyglot-12.8B/resolve/main/KoAlpaca-Polyglot-12.8B-ggml-model-q5_1.bin\n",
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"../models/KoAlpaca-Polyglot-12.8B-ggml-model-q5_1.bin\")\n",
    "output = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=32, stop=[\"Q:\", \"\\n\"], echo=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac7aee8-f293-4145-adc0-9d09a17234e3",
   "metadata": {},
   "source": [
    "## KoAlpaca convert pytorch -> ggmlv3 - 실패"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a622f33d-ab22-40c2-bf42-0021cae4459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 model - https://huggingface.co/beomi/KoAlpaca-Polyglot-5.8B\n",
    "# convert 에러 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de84f34f-8c66-4985-be4f-c9eb7d0842cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"../models/KoAlpaca-Polyglot-5.8B/ggml-model-q4_0.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a41fb90-0454-4a82-bf50-df7ef1ba11e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=32, stop=[\"Q:\", \"\\n\"], echo=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3a8371-7f8e-4573-9076-3ecf205ae761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5df30053-1721-4631-95a7-b74a19bd0491",
   "metadata": {},
   "source": [
    "# serge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0645955a-69af-4822-a138-4e61fc5896e5",
   "metadata": {},
   "source": [
    "serge 안에서 python 사용 package 버전 확인\n",
    "llama-cpp-python        0.1.78\n",
    "\n",
    "```\n",
    "(nlu) thyun@rnd-pc:~/serge$ docker exec -it a345c764ca0a /bin/bash\n",
    "root@a345c764ca0a:/usr/src/app# pip list\n",
    "Package                 Version\n",
    "----------------------- -----------\n",
    "aiohttp                 3.8.5\n",
    "aiosignal               1.3.1\n",
    "anyio                   4.0.0\n",
    "async-timeout           4.0.3\n",
    "asyncio                 3.4.3\n",
    "attrs                   23.1.0\n",
    "certifi                 2023.7.22\n",
    "charset-normalizer      3.2.0\n",
    "click                   8.1.7\n",
    "dataclasses-json        0.5.14\n",
    "diskcache               5.6.3\n",
    "dnspython               2.4.2\n",
    "email-validator         2.0.0.post2\n",
    "fastapi                 0.95.1\n",
    "filelock                3.12.4\n",
    "frozenlist              1.4.0\n",
    "fsspec                  2023.9.1\n",
    "greenlet                2.0.2\n",
    "h11                     0.14.0\n",
    "hiredis                 2.2.3\n",
    "httpcore                0.18.0\n",
    "httptools               0.6.0\n",
    "huggingface-hub         0.16.4\n",
    "idna                    3.4\n",
    "iniconfig               2.0.0\n",
    "itsdangerous            2.1.2\n",
    "Jinja2                  3.1.2\n",
    "langchain               0.0.180\n",
    "lazy-model              0.2.0\n",
    "llama-cpp-python        0.1.78\n",
    "loguru                  0.7.2\n",
    "MarkupSafe              2.1.3\n",
    "marshmallow             3.20.1\n",
    "motor                   3.3.1\n",
    "multidict               6.0.4\n",
    "mypy-extensions         1.0.0\n",
    "numexpr                 2.8.6\n",
    "numpy                   1.26.0\n",
    "openapi-schema-pydantic 1.2.4\n",
    "orjson                  3.9.7\n",
    "packaging               23.1\n",
    "pip                     23.2.1\n",
    "pluggy                  1.3.0\n",
    "pydantic                1.10.12\n",
    "pymongo                 4.5.0\n",
    "pytest                  7.4.2\n",
    "python-dotenv           1.0.0\n",
    "python-multipart        0.0.6\n",
    "PyYAML                  6.0.1\n",
    "redis                   5.0.0\n",
    "requests                2.31.0\n",
    "rfc3986                 2.0.0\n",
    "sentencepiece           0.1.99\n",
    "serge                   0.1.0\n",
    "setuptools              65.5.1\n",
    "sniffio                 1.3.0\n",
    "SQLAlchemy              2.0.20\n",
    "sse-starlette           1.6.5\n",
    "starlette               0.26.1\n",
    "tenacity                8.2.3\n",
    "toml                    0.10.2\n",
    "tqdm                    4.66.1\n",
    "typing_extensions       4.7.1\n",
    "typing-inspect          0.9.0\n",
    "ujson                   5.8.0\n",
    "urllib3                 2.0.4\n",
    "uvicorn                 0.23.2\n",
    "uvloop                  0.17.0\n",
    "watchfiles              0.20.0\n",
    "websockets              11.0.3\n",
    "wheel                   0.41.2\n",
    "yarl                    1.9.2\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71247981-3743-45af-a178-5b144e0ed7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842711c0-baa8-431d-94e6-44bfe011fe0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0dff4e-a962-491f-bdd2-b671ac138b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390df5bb-124e-4150-92ac-134e3ee22c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e851c49-5235-4763-9df7-11661b41631f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b535b8-e1e8-422c-959e-3afd505666ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
