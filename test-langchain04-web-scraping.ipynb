{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0e083e9-f6d5-4027-a2f5-a94da156c7e9",
   "metadata": {},
   "source": [
    "# Web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e159fd85-aaa3-4ecf-bcd2-99803060b0db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c0769a-89d7-446d-a74e-61f04040a597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b92d7-f7ad-48ea-a3e1-ac68fad96183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a68219-8697-4d1e-ac6a-ce9443973661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a417560-ba6e-40b5-97f5-854bede4b3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e2f44-9f9d-4084-8a62-5fae7ff837cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb98417a-2e53-4f8f-87c6-567c18c80097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd976cf-cc36-4455-951f-4bc21f04b46b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e7c677f-75e5-4484-bdaf-66593ac776b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thyun/miniconda3/envs/langchain2/lib/python3.10/site-packages/langchain/indexes/_sql_record_manager.py:46: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "from parser import langchain_docs_extractor\n",
    "\n",
    "import weaviate\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from langchain.document_loaders import RecursiveUrlLoader, SitemapLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.indexes import SQLRecordManager, index\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.utils.html import PREFIXES_TO_IGNORE_REGEX, SUFFIXES_TO_IGNORE_REGEX\n",
    "from langchain.vectorstores import Weaviate\n",
    "\n",
    "from constants import WEAVIATE_DOCS_INDEX_NAME\n",
    "import datetime\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "WEAVIATE_URL = os.environ[\"WEAVIATE_URL\"]\n",
    "WEAVIATE_API_KEY = os.environ[\"WEAVIATE_API_KEY\"]\n",
    "RECORD_MANAGER_DB_URL = os.environ[\"RECORD_MANAGER_DB_URL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f33cffb-90a6-4c3b-a746-cb02e942a583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest-asyncio in /home/thyun/miniconda3/envs/langchain2/lib/python3.10/site-packages (1.5.8)\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/langchain-ai/langchain/issues/8494\n",
    "!pip install nest-asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b44dda1d-09ae-4444-8d09-b0a8955862e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recursive_url_loader import RecursiveUrlLoader2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0dcff972-68b5-4515-a458-f935a9fc3ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_extractor(meta: dict, soup: BeautifulSoup) -> dict:                                \n",
    "    title = soup.find(\"title\")                                \n",
    "    description = soup.find(\"meta\", attrs={\"name\": \"description\"})                                \n",
    "    html = soup.find(\"html\")                                \n",
    "    return {                                \n",
    "        \"source\": meta[\"loc\"],                                \n",
    "        \"title\": title.get_text() if title else \"\",                                \n",
    "        \"description\": description.get(\"content\", \"\") if description else \"\",                                \n",
    "        \"language\": html.get(\"lang\", \"\") if html else \"\",                                \n",
    "        **meta,                                \n",
    "    }                                \n",
    "                                \n",
    "                                \n",
    "def load_langchain_docs():                                \n",
    "    return SitemapLoader(                                \n",
    "        \"https://python.langchain.com/sitemap.xml\",                                \n",
    "        filter_urls=[\"https://python.langchain.com/\"],                                \n",
    "        parsing_function=langchain_docs_extractor,                                \n",
    "        default_parser=\"lxml\",                                \n",
    "        bs_kwargs={                                \n",
    "            \"parse_only\": SoupStrainer(                                \n",
    "                name=(\"article\", \"title\", \"html\", \"lang\", \"content\")                                \n",
    "            ),                                \n",
    "        },                                \n",
    "        meta_function=metadata_extractor,                                \n",
    "    ).load()                                \n",
    "                                \n",
    "                                \n",
    "def simple_extractor(html: str) -> str:                                \n",
    "    soup = BeautifulSoup(html, \"lxml\")                                \n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n",
    "\n",
    "def simple_extractor2(html: str) -> str:                                \n",
    "    soup = BeautifulSoup(html, \"html.parser\")                                \n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.get_text(separator=\"\\n\")).strip()\n",
    "                                \n",
    "                                \n",
    "def load_api_docs():                                \n",
    "    return RecursiveUrlLoader(                                \n",
    "        url=\"https://api.python.langchain.com/en/latest/api_reference.html\",                                \n",
    "        max_depth=1,                                \n",
    "        extractor=simple_extractor,                                \n",
    "        prevent_outside=True,\n",
    "        use_async=True,\n",
    "        timeout=600,\n",
    "        # Drop trailing / to avoid duplicate pages.\n",
    "        link_regex=(\n",
    "            f\"href=[\\\"']{PREFIXES_TO_IGNORE_REGEX}((?:{SUFFIXES_TO_IGNORE_REGEX}.)*?)\"\n",
    "            r\"(?:[\\#'\\\"]|\\/[\\#'\\\"])\"\n",
    "        ),\n",
    "        check_response_status=True,\n",
    "        exclude_dirs=(\n",
    "            \"https://api.python.langchain.com/en/latest/_sources\",\n",
    "            \"https://api.python.langchain.com/en/latest/_modules\",\n",
    "        ),\n",
    "    ).load()\n",
    "\n",
    "def load_wiki_docs():                                \n",
    "    return RecursiveUrlLoader2(                                \n",
    "        url=\"http://wiki.skplanet.com/pages/viewpage.action?pageId=295656385\",                                \n",
    "        max_depth=1,                                \n",
    "        extractor=simple_extractor,                                \n",
    "        prevent_outside=True,\n",
    "        use_async=False,\n",
    "        timeout=600,\n",
    "        # Drop trailing / to avoid duplicate pages.\n",
    "        link_regex=(\n",
    "            f\"href=[\\\"']{PREFIXES_TO_IGNORE_REGEX}((?:{SUFFIXES_TO_IGNORE_REGEX}.)*?)\"\n",
    "            r\"(?:[\\#'\\\"]|\\/[\\#'\\\"])\"\n",
    "        ),\n",
    "        check_response_status=True,\n",
    "        exclude_dirs=(\n",
    "            \"https://api.python.langchain.com/en/latest/_sources\",\n",
    "            \"https://api.python.langchain.com/en/latest/_modules\",\n",
    "        ),\n",
    "    ).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64732de3-03a9-4e3c-ac36-1d293e265f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_docs(docs_from_documentation, docs_from_api):\n",
    "    # docs_from_documentation = load_langchain_docs()\n",
    "    # logger.info(f\"Loaded {len(docs_from_documentation)} docs from documentation\")\n",
    "    # docs_from_api = load_api_docs()\n",
    "    # logger.info(f\"Loaded {len(docs_from_api)} docs from API\")\n",
    "\n",
    "    print(\"Start docs transform\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=200)\n",
    "    docs_transformed = text_splitter.split_documents(\n",
    "        docs_from_documentation + docs_from_api\n",
    "    )\n",
    "\n",
    "    # We try to return 'source' and 'title' metadata when querying vector store and\n",
    "    # Weaviate will error at query time if one of the attributes is missing from a\n",
    "    # retrieved document.\n",
    "    for doc in docs_transformed:\n",
    "        if \"source\" not in doc.metadata:\n",
    "            doc.metadata[\"source\"] = \"\"\n",
    "        if \"title\" not in doc.metadata:\n",
    "            doc.metadata[\"title\"] = \"\"\n",
    "\n",
    "    # client = weaviate.Client(\n",
    "    #     url=WEAVIATE_URL,\n",
    "    #     auth_client_secret=weaviate.AuthApiKey(api_key=WEAVIATE_API_KEY),\n",
    "    # )\n",
    "    # embedding = OpenAIEmbeddings(\n",
    "    #     chunk_size=200,\n",
    "    # )  # rate limit\n",
    "    # vectorstore = Weaviate(\n",
    "    #     client=client,\n",
    "    #     index_name=WEAVIATE_DOCS_INDEX_NAME,\n",
    "    #     text_key=\"text\",\n",
    "    #     embedding=embedding,\n",
    "    #     by_text=False,\n",
    "    #     attributes=[\"source\", \"title\"],\n",
    "    # )\n",
    "\n",
    "    # record_manager = SQLRecordManager(\n",
    "    #     f\"weaviate/{WEAVIATE_DOCS_INDEX_NAME}\", db_url=RECORD_MANAGER_DB_URL\n",
    "    # )\n",
    "    # record_manager.create_schema()\n",
    "\n",
    "    print(\"Start index\")\n",
    "    indexing_stats = index(\n",
    "        docs_transformed,\n",
    "        record_manager,\n",
    "        vectorstore,\n",
    "        cleanup=\"full\",\n",
    "        source_id_key=\"source\",\n",
    "    )\n",
    "\n",
    "    print(\"Indexing stats: \", indexing_stats)\n",
    "    # print(\n",
    "    #     \"LangChain now has this many vectors: \",\n",
    "    #     client.query.aggregate(WEAVIATE_DOCS_INDEX_NAME).with_meta_count().do(),\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66776a1-1845-4840-9845-059cd66dea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = RecursiveUrlLoader2(                                \n",
    "        url=\"http://wiki.skplanet.com/pages/viewpage.action?pageId=295656385\",                                \n",
    "        max_depth=1,                                \n",
    "        extractor=simple_extractor2,                                \n",
    "        prevent_outside=True,\n",
    "        use_async=False,\n",
    "        timeout=600,\n",
    "        # Drop trailing / to avoid duplicate pages.\n",
    "        link_regex=(\n",
    "            f\"href=[\\\"']{PREFIXES_TO_IGNORE_REGEX}((?:{SUFFIXES_TO_IGNORE_REGEX}.)*?)\"\n",
    "            r\"(?:[\\#'\\\"]|\\/[\\#'\\\"])\"\n",
    "        ),\n",
    "        check_response_status=True,\n",
    "        exclude_dirs=(\n",
    "            \"https://api.python.langchain.com/en/latest/_sources\",\n",
    "            \"https://api.python.langchain.com/en/latest/_modules\",\n",
    "        ),\n",
    "    ).load()\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ce6e9-9ed6-4ef1-a4d6-0563d2842f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "827b7de4-b942-47ad-a89a-28bded102225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|#################################################################################################################################################################| 1053/1053 [00:23<00:00, 43.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1053 docs from documentation\n"
     ]
    }
   ],
   "source": [
    "docs_from_documentation = load_langchain_docs()\n",
    "print(f\"Loaded {len(docs_from_documentation)} docs from documentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "776a2fd5-ccc0-4139-94a9-5735c212f9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 docs from API\n"
     ]
    }
   ],
   "source": [
    "docs_from_api = load_api_docs()\n",
    "print(f\"Loaded {len(docs_from_api)} docs from API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c98e079-db69-40d2-b138-f9b7dc86e08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='langchain API Reference — 🦜🔗 LangChain 0.0.339rc1\\n\\nAPI\\n\\nCore\\n\\nExperimental\\n\\nPython Docs\\n\\nToggle Menu\\n\\nPrev\\nUp\\nNext\\n\\nLangChain 0.0.339rc1\\n\\nlangchain API Reference\\nlangchain.adapters\\nClasses\\nFunctions\\n\\nlangchain.agents\\nClasses\\nFunctions\\n\\nlangchain.agents.format_scratchpad\\nFunctions\\n\\nlangchain.agents.output_parsers\\nClasses\\nFunctions\\n\\nlangchain.cache\\nClasses\\nFunctions\\n\\nlangchain.callbacks\\nClasses\\nFunctions\\n\\nlangchain.chains\\nClasses\\nFunctions\\n\\nlangchain.chat_loaders\\nClasses\\nFunctions\\n\\nlangchain.chat_models\\nClasses\\nFunctions\\n\\nlangchain.docstore\\nClasses\\n\\nlangchain.document_loaders\\nClasses\\nFunctions\\n\\nlangchain.document_transformers\\nClasses\\nFunctions\\n\\nlangchain.embeddings\\nClasses\\nFunctions\\n\\nlangchain.evaluation\\nClasses\\nFunctions\\n\\nlangchain.graphs\\nClasses\\nFunctions\\n\\nlangchain.hub\\nFunctions\\n\\nlangchain.indexes\\nClasses\\nFunctions\\n\\nlangchain.llms\\nClasses\\nFunctions\\n\\nlangchain.memory\\nClasses\\nFunctions\\n\\nlangchain.model_laboratory\\nClasses\\n\\nlangchain.output_parsers\\nClasses\\nFunctions\\n\\nlangchain.prompts\\nClasses\\nFunctions\\n\\nlangchain.retrievers\\nClasses\\nFunctions\\n\\nlangchain.runnables\\nClasses\\n\\nlangchain.smith\\nClasses\\nFunctions\\n\\nlangchain.storage\\nClasses\\n\\nlangchain.text_splitter\\nClasses\\nFunctions\\n\\nlangchain.tools\\nClasses\\nFunctions\\n\\nlangchain.tools.render\\nFunctions\\n\\nlangchain.utilities\\nClasses\\nFunctions\\n\\nlangchain.utils\\nClasses\\nFunctions\\n\\nlangchain.vectorstores\\nClasses\\nFunctions\\n\\nlangchain API Reference¶\\n\\nlangchain.adapters¶\\n\\nClasses¶\\n\\nadapters.openai.ChatCompletion()\\nChat completion.\\n\\nFunctions¶\\n\\nadapters.openai.aenumerate(iterable[,\\xa0start])\\nAsync version of enumerate function.\\n\\nadapters.openai.convert_dict_to_message(_dict)\\nConvert a dictionary to a LangChain message.\\n\\nadapters.openai.convert_message_to_dict(message)\\nConvert a LangChain message to a dictionary.\\n\\nadapters.openai.convert_messages_for_finetuning(...)\\nConvert messages to a list of lists of dictionaries for fine-tuning.\\n\\nadapters.openai.convert_openai_messages(messages)\\nConvert dictionaries representing OpenAI messages to LangChain format.\\n\\nlangchain.agents¶\\nAgent is a class that uses an LLM to choose a sequence of actions to take.\\nIn Chains, a sequence of actions is hardcoded. In Agents,\\na language model is used as a reasoning engine to determine which actions\\nto take and in which order.\\nAgents select and use Tools and Toolkits for actions.\\nClass hierarchy:\\nBaseSingleActionAgent --> LLMSingleActionAgent\\n                          OpenAIFunctionsAgent\\n                          XMLAgent\\n                          Agent --> <name>Agent  # Examples: ZeroShotAgent, ChatAgent\\n\\nBaseMultiActionAgent  --> OpenAIMultiFunctionsAgent\\n\\nMain helpers:\\nAgentType, AgentExecutor, AgentOutputParser, AgentExecutorIterator,\\nAgentAction, AgentFinish\\n\\nClasses¶\\n\\nagents.agent.Agent\\nAgent that calls the language model and deciding the action.\\n\\nagents.agent.AgentExecutor\\nAgent that is using tools.\\n\\nagents.agent.AgentOutputParser\\nBase class for parsing agent output into agent action/finish.\\n\\nagents.agent.BaseMultiActionAgent\\nBase Multi Action Agent class.\\n\\nagents.agent.BaseSingleActionAgent\\nBase Single Action Agent class.\\n\\nagents.agent.ExceptionTool\\nTool that just returns the query.\\n\\nagents.agent.LLMSingleActionAgent\\nBase class for single action agents.\\n\\nagents.agent.MultiActionAgentOutputParser\\nBase class for parsing agent output into agent actions/finish.\\n\\nagents.agent.RunnableAgent\\nAgent powered by runnables.\\n\\nagents.agent.RunnableMultiActionAgent\\nAgent powered by runnables.\\n\\nagents.agent_iterator.AgentExecutorIterator(...)\\nIterator for AgentExecutor.\\n\\nagents.agent_iterator.BaseAgentExecutorIterator()\\nBase class for AgentExecutorIterator.\\n\\nagents.agent_toolkits.ainetwork.toolkit.AINetworkToolkit\\nToolkit for interacting with AINetwork Blockchain.\\n\\nagents.agent_toolkits.amadeus.toolkit.AmadeusToolkit\\nToolkit for interacting with Amadeus which offers APIs for travel search.\\n\\nagents.agent_toolkits.azure_cognitive_services.AzureCognitiveServicesToolkit\\nToolkit for Azure Cognitive Services.\\n\\nagents.agent_toolkits.base.BaseToolkit\\nBase Toolkit representing a collection of related tools.\\n\\nagents.agent_toolkits.clickup.toolkit.ClickupToolkit\\nClickup Toolkit.\\n\\nagents.agent_toolkits.file_management.toolkit.FileManagementToolkit\\nToolkit for interacting with local files.\\n\\nagents.agent_toolkits.github.toolkit.GitHubToolkit\\nGitHub Toolkit.\\n\\nagents.agent_toolkits.gitlab.toolkit.GitLabToolkit\\nGitLab Toolkit.\\n\\nagents.agent_toolkits.gmail.toolkit.GmailToolkit\\nToolkit for interacting with Gmail.\\n\\nagents.agent_toolkits.jira.toolkit.JiraToolkit\\nJira Toolkit.\\n\\nagents.agent_toolkits.json.toolkit.JsonToolkit\\nToolkit for interacting with a JSON spec.\\n\\nagents.agent_toolkits.multion.toolkit.MultionToolkit\\nToolkit for interacting with the Browser Agent.\\n\\nagents.agent_toolkits.nla.tool.NLATool\\nNatural Language API Tool.\\n\\nagents.agent_toolkits.nla.toolkit.NLAToolkit\\nNatural Language API Toolkit.\\n\\nagents.agent_toolkits.office365.toolkit.O365Toolkit\\nToolkit for interacting with Office 365.\\n\\nagents.agent_toolkits.openapi.planner.RequestsDeleteToolWithParsing\\nA tool that sends a DELETE request and parses the response.\\n\\nagents.agent_toolkits.openapi.planner.RequestsGetToolWithParsing\\nRequests GET tool with LLM-instructed extraction of truncated responses.\\n\\nagents.agent_toolkits.openapi.planner.RequestsPatchToolWithParsing\\nRequests PATCH tool with LLM-instructed extraction of truncated responses.\\n\\nagents.agent_toolkits.openapi.planner.RequestsPostToolWithParsing\\nRequests POST tool with LLM-instructed extraction of truncated responses.\\n\\nagents.agent_toolkits.openapi.planner.RequestsPutToolWithParsing\\nRequests PUT tool with LLM-instructed extraction of truncated responses.\\n\\nagents.agent_toolkits.openapi.spec.ReducedOpenAPISpec(...)\\nA reduced OpenAPI spec.\\n\\nagents.agent_toolkits.openapi.toolkit.OpenAPIToolkit\\nToolkit for interacting with an OpenAPI API.\\n\\nagents.agent_toolkits.openapi.toolkit.RequestsToolkit\\nToolkit for making REST requests.\\n\\nagents.agent_toolkits.playwright.toolkit.PlayWrightBrowserToolkit\\nToolkit for PlayWright browser tools.\\n\\nagents.agent_toolkits.powerbi.toolkit.PowerBIToolkit\\nToolkit for interacting with Power BI dataset.\\n\\nagents.agent_toolkits.spark_sql.toolkit.SparkSQLToolkit\\nToolkit for interacting with Spark SQL.\\n\\nagents.agent_toolkits.sql.toolkit.SQLDatabaseToolkit\\nToolkit for interacting with SQL databases.\\n\\nagents.agent_toolkits.vectorstore.toolkit.VectorStoreInfo\\nInformation about a VectorStore.\\n\\nagents.agent_toolkits.vectorstore.toolkit.VectorStoreRouterToolkit\\nToolkit for routing between Vector Stores.\\n\\nagents.agent_toolkits.vectorstore.toolkit.VectorStoreToolkit\\nToolkit for interacting with a Vector Store.\\n\\nagents.agent_toolkits.zapier.toolkit.ZapierToolkit\\nZapier Toolkit.\\n\\nagents.agent_types.AgentType(value[,\\xa0names,\\xa0...])\\nAn enum for agent types.\\n\\nagents.chat.base.ChatAgent\\nChat Agent.\\n\\nagents.chat.output_parser.ChatOutputParser\\nOutput parser for the chat agent.\\n\\nagents.conversational.base.ConversationalAgent\\nAn agent that holds a conversation in addition to using tools.\\n\\nagents.conversational.output_parser.ConvoOutputParser\\nOutput parser for the conversational agent.\\n\\nagents.conversational_chat.base.ConversationalChatAgent\\nAn agent designed to hold a conversation in addition to using tools.\\n\\nagents.conversational_chat.output_parser.ConvoOutputParser\\nOutput parser for the conversational agent.\\n\\nagents.mrkl.base.ChainConfig(action_name,\\xa0...)\\nConfiguration for chain to use in MRKL system.\\n\\nagents.mrkl.base.MRKLChain\\n[Deprecated] Chain that implements the MRKL system.\\n\\nagents.mrkl.base.ZeroShotAgent\\nAgent for the MRKL chain.\\n\\nagents.mrkl.output_parser.MRKLOutputParser\\nMRKL Output parser for the chat agent.\\n\\nagents.openai_assistant.base.OpenAIAssistantAction\\nAgentAction with info needed to submit custom tool output to existing run.\\n\\nagents.openai_assistant.base.OpenAIAssistantFinish\\nAgentFinish with run and thread metadata.\\n\\nagents.openai_assistant.base.OpenAIAssistantRunnable\\nRun an OpenAI Assistant.\\n\\nagents.openai_functions_agent.agent_token_buffer_memory.AgentTokenBufferMemory\\nMemory used to save agent output AND intermediate steps.\\n\\nagents.openai_functions_agent.base.OpenAIFunctionsAgent\\nAn Agent driven by OpenAIs function powered API.\\n\\nagents.openai_functions_multi_agent.base.OpenAIMultiFunctionsAgent\\nAn Agent driven by OpenAIs function powered API.\\n\\nagents.output_parsers.json.JSONAgentOutputParser\\nParses tool invocations and final answers in JSON format.\\n\\nagents.output_parsers.openai_functions.OpenAIFunctionsAgentOutputParser\\nParses a message into agent action/finish.\\n\\nagents.output_parsers.openai_tools.OpenAIToolAgentAction\\nOverride init to support instantiation by position for backward compat.\\n\\nagents.output_parsers.openai_tools.OpenAIToolsAgentOutputParser\\nParses a message into agent actions/finish.\\n\\nagents.output_parsers.react_json_single_input.ReActJsonSingleInputOutputParser\\nParses ReAct-style LLM calls that have a single tool input in json format.\\n\\nagents.output_parsers.react_single_input.ReActSingleInputOutputParser\\nParses ReAct-style LLM calls that have a single tool input.\\n\\nagents.output_parsers.self_ask.SelfAskOutputParser\\nParses self-ask style LLM calls.\\n\\nagents.output_parsers.xml.XMLAgentOutputParser\\nParses tool invocations and final answers in XML format.\\n\\nagents.react.base.DocstoreExplorer(docstore)\\nClass to assist with exploration of a document store.\\n\\nagents.react.base.ReActChain\\n[Deprecated] Chain that implements the ReAct paper.\\n\\nagents.react.base.ReActDocstoreAgent\\nAgent for the ReAct chain.\\n\\nagents.react.base.ReActTextWorldAgent\\nAgent for the ReAct TextWorld chain.\\n\\nagents.react.output_parser.ReActOutputParser\\nOutput parser for the ReAct agent.\\n\\nagents.schema.AgentScratchPadChatPromptTemplate\\nChat prompt template for the agent scratchpad.\\n\\nagents.self_ask_with_search.base.SelfAskWithSearchAgent\\nAgent for the self-ask-with-search paper.\\n\\nagents.self_ask_with_search.base.SelfAskWithSearchChain\\n[Deprecated] Chain that does self-ask with search.\\n\\nagents.structured_chat.base.StructuredChatAgent\\nStructured Chat Agent.\\n\\nagents.structured_chat.output_parser.StructuredChatOutputParser\\nOutput parser for the structured chat agent.\\n\\nagents.structured_chat.output_parser.StructuredChatOutputParserWithRetries\\nOutput parser with retries for the structured chat agent.\\n\\nagents.tools.InvalidTool\\nTool that is run when invalid tool name is encountered by agent.\\n\\nagents.xml.base.XMLAgent\\nAgent that uses XML tags.\\n\\nFunctions¶\\n\\nagents.agent_iterator.rebuild_callback_manager_on_set(...)\\nDecorator to force setters to rebuild callback mgr\\n\\nagents.agent_toolkits.conversational_retrieval.openai_functions.create_conversational_retrieval_agent(...)\\nA convenience method for creating a conversational retrieval agent.\\n\\nagents.agent_toolkits.json.base.create_json_agent(...)\\nConstruct a json agent from an LLM and tools.\\n\\nagents.agent_toolkits.openapi.base.create_openapi_agent(...)\\nConstruct an OpenAPI agent from an LLM and tools.\\n\\nagents.agent_toolkits.openapi.planner.create_openapi_agent(...)\\nInstantiate OpenAI API planner and controller for a given spec.\\n\\nagents.agent_toolkits.openapi.spec.reduce_openapi_spec(spec)\\nSimplify/distill/minify a spec somehow.\\n\\nagents.agent_toolkits.powerbi.base.create_pbi_agent(llm)\\nConstruct a Power BI agent from an LLM and tools.\\n\\nagents.agent_toolkits.powerbi.chat_base.create_pbi_chat_agent(llm)\\nConstruct a Power BI agent from a Chat LLM and tools.\\n\\nagents.agent_toolkits.spark_sql.base.create_spark_sql_agent(...)\\nConstruct a Spark SQL agent from an LLM and tools.\\n\\nagents.agent_toolkits.sql.base.create_sql_agent(...)\\nConstruct an SQL agent from an LLM and tools.\\n\\nagents.agent_toolkits.vectorstore.base.create_vectorstore_agent(...)\\nConstruct a VectorStore agent from an LLM and tools.\\n\\nagents.agent_toolkits.vectorstore.base.create_vectorstore_router_agent(...)\\nConstruct a VectorStore router agent from an LLM and tools.\\n\\nagents.format_scratchpad.log.format_log_to_str(...)\\nConstruct the scratchpad that lets the agent continue its thought process.\\n\\nagents.format_scratchpad.log_to_messages.format_log_to_messages(...)\\nConstruct the scratchpad that lets the agent continue its thought process.\\n\\nagents.format_scratchpad.openai_functions.format_to_openai_function_messages(...)\\nConvert (AgentAction, tool output) tuples into FunctionMessages.\\n\\nagents.format_scratchpad.openai_functions.format_to_openai_functions(...)\\nConvert (AgentAction, tool output) tuples into FunctionMessages.\\n\\nagents.format_scratchpad.openai_tools.format_to_openai_tool_messages(...)\\nConvert (AgentAction, tool output) tuples into FunctionMessages.\\n\\nagents.format_scratchpad.xml.format_xml(...)\\nFormat the intermediate steps as XML.\\n\\nagents.initialize.initialize_agent(tools,\\xa0llm)\\nLoad an agent executor given tools and LLM.\\n\\nagents.load_tools.get_all_tool_names()\\nGet a list of all possible tool names.\\n\\nagents.load_tools.load_huggingface_tool(...)\\nLoads a tool from the HuggingFace Hub.\\n\\nagents.load_tools.load_tools(tool_names[,\\xa0...])\\nLoad tools based on their name.\\n\\nagents.loading.load_agent(path,\\xa0**kwargs)\\nUnified method for loading an agent from LangChainHub or local fs.\\n\\nagents.loading.load_agent_from_config(config)\\nLoad agent from Config Dict.\\n\\nagents.output_parsers.openai_tools.parse_ai_message_to_openai_tool_action(message)\\nParse an AI message potentially containing tool_calls.\\n\\nagents.utils.validate_tools_single_input(...)\\nValidate tools for single input.\\n\\nlangchain.agents.format_scratchpad¶\\nLogic for formatting intermediate steps into an agent scratchpad.\\nIntermediate steps refers to the list of (AgentAction, observation) tuples\\nthat result from previous iterations of the agent.\\nDepending on the prompting strategy you are using, you may want to format these\\ndifferently before passing them into the LLM.\\n\\nFunctions¶\\n\\nagents.format_scratchpad.log.format_log_to_str(...)\\nConstruct the scratchpad that lets the agent continue its thought process.\\n\\nagents.format_scratchpad.log_to_messages.format_log_to_messages(...)\\nConstruct the scratchpad that lets the agent continue its thought process.\\n\\nagents.format_scratchpad.openai_functions.format_to_openai_function_messages(...)\\nConvert (AgentAction, tool output) tuples into FunctionMessages.\\n\\nagents.format_scratchpad.openai_functions.format_to_openai_functions(...)\\nConvert (AgentAction, tool output) tuples into FunctionMessages.\\n\\nagents.format_scratchpad.openai_tools.format_to_openai_tool_messages(...)\\nConvert (AgentAction, tool output) tuples into FunctionMessages.\\n\\nagents.format_scratchpad.xml.format_xml(...)\\nFormat the intermediate steps as XML.\\n\\nlangchain.agents.output_parsers¶\\nParsing utils to go from string to AgentAction or Agent Finish.\\nAgentAction means that an action should be taken.\\nThis contains the name of the tool to use, the input to pass to that tool,\\nand a log variable (which contains a log of the agent’s thinking).\\nAgentFinish means that a response should be given.\\nThis contains a return_values dictionary. This usually contains a\\nsingle output key, but can be extended to contain more.\\nThis also contains a log variable (which contains a log of the agent’s thinking).\\n\\nClasses¶\\n\\nagents.output_parsers.json.JSONAgentOutputParser\\nParses tool invocations and final answers in JSON format.\\n\\nagents.output_parsers.openai_functions.OpenAIFunctionsAgentOutputParser\\nParses a message into agent action/finish.\\n\\nagents.output_parsers.openai_tools.OpenAIToolAgentAction\\nOverride init to support instantiation by position for backward compat.\\n\\nagents.output_parsers.openai_tools.OpenAIToolsAgentOutputParser\\nParses a message into agent actions/finish.\\n\\nagents.output_parsers.react_json_single_input.ReActJsonSingleInputOutputParser\\nParses ReAct-style LLM calls that have a single tool input in json format.\\n\\nagents.output_parsers.react_single_input.ReActSingleInputOutputParser\\nParses ReAct-style LLM calls that have a single tool input.\\n\\nagents.output_parsers.self_ask.SelfAskOutputParser\\nParses self-ask style LLM calls.\\n\\nagents.output_parsers.xml.XMLAgentOutputParser\\nParses tool invocations and final answers in XML format.\\n\\nFunctions¶\\n\\nagents.output_parsers.openai_tools.parse_ai_message_to_openai_tool_action(message)\\nParse an AI message potentially containing tool_calls.\\n\\nlangchain.cache¶\\n\\nWarning\\nBeta Feature!\\n\\nCache provides an optional caching layer for LLMs.\\nCache is useful for two reasons:\\n\\nIt can save you money by reducing the number of API calls you make to the LLM\\nprovider if you’re often requesting the same completion multiple times.\\nIt can speed up your application by reducing the number of API calls you make\\nto the LLM provider.\\n\\nCache directly competes with Memory. See documentation for Pros and Cons.\\nClass hierarchy:\\nBaseCache --> <name>Cache  # Examples: InMemoryCache, RedisCache, GPTCache\\n\\nClasses¶\\n\\ncache.CassandraCache([session,\\xa0keyspace,\\xa0...])\\nCache that uses Cassandra / Astra DB as a backend.\\n\\ncache.CassandraSemanticCache(session,\\xa0...[,\\xa0...])\\nCache that uses Cassandra as a vector-store backend for semantic (i.e.\\n\\ncache.FullLLMCache(**kwargs)\\nSQLite table for full LLM Cache (all generations).\\n\\ncache.FullMd5LLMCache(**kwargs)\\nSQLite table for full LLM Cache (all generations).\\n\\ncache.GPTCache([init_func])\\nCache that uses GPTCache as a backend.\\n\\ncache.InMemoryCache()\\nCache that stores things in memory.\\n\\ncache.MomentoCache(cache_client,\\xa0cache_name,\\xa0*)\\nCache that uses Momento as a backend.\\n\\ncache.RedisCache(redis_,\\xa0*[,\\xa0ttl])\\nCache that uses Redis as a backend.\\n\\ncache.RedisSemanticCache(redis_url,\\xa0embedding)\\nCache that uses Redis as a vector-store backend.\\n\\ncache.SQLAlchemyCache(engine,\\xa0cache_schema)\\nCache that uses SQAlchemy as a backend.\\n\\ncache.SQLAlchemyMd5Cache(engine,\\xa0cache_schema)\\nCache that uses SQAlchemy as a backend.\\n\\ncache.SQLiteCache([database_path])\\nCache that uses SQLite as a backend.\\n\\ncache.UpstashRedisCache(redis_,\\xa0*[,\\xa0ttl])\\nCache that uses Upstash Redis as a backend.\\n\\nFunctions¶\\n\\nlangchain.callbacks¶\\nCallback handlers allow listening to events in LangChain.\\nClass hierarchy:\\nBaseCallbackHandler --> <name>CallbackHandler  # Example: AimCallbackHandler\\n\\nClasses¶\\n\\ncallbacks.aim_callback.AimCallbackHandler([...])\\nCallback Handler that logs to Aim.\\n\\ncallbacks.aim_callback.BaseMetadataCallbackHandler()\\nThis class handles the metadata and associated function states for callbacks.\\n\\ncallbacks.argilla_callback.ArgillaCallbackHandler(...)\\nCallback Handler that logs into Argilla.\\n\\ncallbacks.arize_callback.ArizeCallbackHandler([...])\\nCallback Handler that logs to Arize.\\n\\ncallbacks.arthur_callback.ArthurCallbackHandler(...)\\nCallback Handler that logs to Arthur platform.\\n\\ncallbacks.clearml_callback.ClearMLCallbackHandler([...])\\nCallback Handler that logs to ClearML.\\n\\ncallbacks.comet_ml_callback.CometCallbackHandler([...])\\nCallback Handler that logs to Comet.\\n\\ncallbacks.confident_callback.DeepEvalCallbackHandler(metrics)\\nCallback Handler that logs into deepeval.\\n\\ncallbacks.context_callback.ContextCallbackHandler([...])\\nCallback Handler that records transcripts to the Context service.\\n\\ncallbacks.file.FileCallbackHandler(filename)\\nCallback Handler that writes to a file.\\n\\ncallbacks.flyte_callback.FlyteCallbackHandler()\\nThis callback handler that is used within a Flyte task.\\n\\ncallbacks.human.HumanApprovalCallbackHandler(...)\\nCallback for manually validating values.\\n\\ncallbacks.human.HumanRejectedException\\nException to raise when a person manually review and rejects a value.\\n\\ncallbacks.infino_callback.InfinoCallbackHandler([...])\\nCallback Handler that logs to Infino.\\n\\ncallbacks.labelstudio_callback.LabelStudioCallbackHandler([...])\\nLabel Studio callback handler.\\n\\ncallbacks.labelstudio_callback.LabelStudioMode(value)\\nLabel Studio mode enumerator.\\n\\ncallbacks.llmonitor_callback.LLMonitorCallbackHandler([...])\\nCallback Handler for LLMonitor`.\\n\\ncallbacks.llmonitor_callback.UserContextManager(user_id)\\nContext manager for LLMonitor user context.\\n\\ncallbacks.mlflow_callback.MlflowCallbackHandler([...])\\nCallback Handler that logs metrics and artifacts to mlflow server.\\n\\ncallbacks.mlflow_callback.MlflowLogger(**kwargs)\\nCallback Handler that logs metrics and artifacts to mlflow server.\\n\\ncallbacks.openai_info.OpenAICallbackHandler()\\nCallback Handler that tracks OpenAI info.\\n\\ncallbacks.promptlayer_callback.PromptLayerCallbackHandler([...])\\nCallback handler for promptlayer.\\n\\ncallbacks.sagemaker_callback.SageMakerCallbackHandler(run)\\nCallback Handler that logs prompt artifacts and metrics to SageMaker Experiments.\\n\\ncallbacks.streaming_aiter.AsyncIteratorCallbackHandler()\\nCallback handler that returns an async iterator.\\n\\ncallbacks.streaming_aiter_final_only.AsyncFinalIteratorCallbackHandler(*)\\nCallback handler that returns an async iterator.\\n\\ncallbacks.streaming_stdout_final_only.FinalStreamingStdOutCallbackHandler(*)\\nCallback handler for streaming in agents.\\n\\ncallbacks.streamlit.mutable_expander.ChildRecord(...)\\nThe child record as a NamedTuple.\\n\\ncallbacks.streamlit.mutable_expander.ChildType(value)\\nThe enumerator of the child type.\\n\\ncallbacks.streamlit.mutable_expander.MutableExpander(...)\\nA Streamlit expander that can be renamed and dynamically expanded/collapsed.\\n\\ncallbacks.streamlit.streamlit_callback_handler.LLMThought(...)\\nA thought in the LLM\\'s thought stream.\\n\\ncallbacks.streamlit.streamlit_callback_handler.LLMThoughtLabeler()\\nGenerates markdown labels for LLMThought containers.\\n\\ncallbacks.streamlit.streamlit_callback_handler.LLMThoughtState(value)\\nEnumerator of the LLMThought state.\\n\\ncallbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler(...)\\nA callback handler that writes to a Streamlit app.\\n\\ncallbacks.streamlit.streamlit_callback_handler.ToolRecord(...)\\nThe tool record as a NamedTuple.\\n\\ncallbacks.tracers.wandb.RunProcessor(...)\\nHandles the conversion of a LangChain Runs into a WBTraceTree.\\n\\ncallbacks.tracers.wandb.WandbRunArgs\\nArguments for the WandbTracer.\\n\\ncallbacks.tracers.wandb.WandbTracer([run_args])\\nCallback Handler that logs to Weights and Biases.\\n\\ncallbacks.trubrics_callback.TrubricsCallbackHandler([...])\\nCallback handler for Trubrics.\\n\\ncallbacks.utils.BaseMetadataCallbackHandler()\\nThis class handles the metadata and associated function states for callbacks.\\n\\ncallbacks.wandb_callback.WandbCallbackHandler([...])\\nCallback Handler that logs to Weights and Biases.\\n\\ncallbacks.whylabs_callback.WhyLabsCallbackHandler(...)\\nCallback Handler for logging to WhyLabs.\\n\\nFunctions¶\\n\\ncallbacks.aim_callback.import_aim()\\nImport the aim python package and raise an error if it is not installed.\\n\\ncallbacks.clearml_callback.import_clearml()\\nImport the clearml python package and raise an error if it is not installed.\\n\\ncallbacks.comet_ml_callback.import_comet_ml()\\nImport comet_ml and raise an error if it is not installed.\\n\\ncallbacks.context_callback.import_context()\\nImport the getcontext package.\\n\\ncallbacks.flyte_callback.analyze_text(text)\\nAnalyze text using textstat and spacy.\\n\\ncallbacks.flyte_callback.import_flytekit()\\nImport flytekit and flytekitplugins-deck-standard.\\n\\ncallbacks.infino_callback.get_num_tokens(...)\\nCalculate num tokens for OpenAI with tiktoken package.\\n\\ncallbacks.infino_callback.import_infino()\\nImport the infino client.\\n\\ncallbacks.infino_callback.import_tiktoken()\\nImport tiktoken for counting tokens for OpenAI models.\\n\\ncallbacks.labelstudio_callback.get_default_label_configs(mode)\\nGet default Label Studio configs for the given mode.\\n\\ncallbacks.llmonitor_callback.identify(user_id)\\nBuilds an LLMonitor UserContextManager\\n\\ncallbacks.manager.get_openai_callback()\\nGet the OpenAI callback handler in a context manager.\\n\\ncallbacks.manager.wandb_tracing_enabled([...])\\nGet the WandbTracer in a context manager.\\n\\ncallbacks.mlflow_callback.analyze_text(text)\\nAnalyze text using textstat and spacy.\\n\\ncallbacks.mlflow_callback.construct_html_from_prompt_and_generation(...)\\nConstruct an html element from a prompt and a generation.\\n\\ncallbacks.mlflow_callback.import_mlflow()\\nImport the mlflow python package and raise an error if it is not installed.\\n\\ncallbacks.openai_info.get_openai_token_cost_for_model(...)\\nGet the cost in USD for a given model and number of tokens.\\n\\ncallbacks.openai_info.standardize_model_name(...)\\nStandardize the model name to a format that can be used in the OpenAI API.\\n\\ncallbacks.sagemaker_callback.save_json(data,\\xa0...)\\nSave dict to local file path.\\n\\ncallbacks.utils.flatten_dict(nested_dict[,\\xa0...])\\nFlattens a nested dictionary into a flat dictionary.\\n\\ncallbacks.utils.hash_string(s)\\nHash a string using sha1.\\n\\ncallbacks.utils.import_pandas()\\nImport the pandas python package and raise an error if it is not installed.\\n\\ncallbacks.utils.import_spacy()\\nImport the spacy python package and raise an error if it is not installed.\\n\\ncallbacks.utils.import_textstat()\\nImport the textstat python package and raise an error if it is not installed.\\n\\ncallbacks.utils.load_json(json_path)\\nLoad json file to a string.\\n\\ncallbacks.wandb_callback.analyze_text(text)\\nAnalyze text using textstat and spacy.\\n\\ncallbacks.wandb_callback.construct_html_from_prompt_and_generation(...)\\nConstruct an html element from a prompt and a generation.\\n\\ncallbacks.wandb_callback.import_wandb()\\nImport the wandb python package and raise an error if it is not installed.\\n\\ncallbacks.wandb_callback.load_json_to_dict(...)\\nLoad json file to a dictionary.\\n\\ncallbacks.whylabs_callback.import_langkit([...])\\nImport the langkit python package and raise an error if it is not installed.\\n\\nlangchain.chains¶\\nChains are easily reusable components linked together.\\nChains encode a sequence of calls to components like models, document retrievers,\\nother Chains, etc., and provide a simple interface to this sequence.\\nThe Chain interface makes it easy to create apps that are:\\n\\nStateful: add Memory to any Chain to give it state,\\nObservable: pass Callbacks to a Chain to execute additional functionality,\\nlike logging, outside the main sequence of component calls,\\nComposable: combine Chains with other components, including other Chains.\\n\\nClass hierarchy:\\nChain --> <name>Chain  # Examples: LLMChain, MapReduceChain, RouterChain\\n\\nClasses¶\\n\\nchains.api.base.APIChain\\nChain that makes API calls and summarizes the responses to answer a question.\\n\\nchains.api.openapi.chain.OpenAPIEndpointChain\\nChain interacts with an OpenAPI endpoint using natural language.\\n\\nchains.api.openapi.requests_chain.APIRequesterChain\\nGet the request parser.\\n\\nchains.api.openapi.requests_chain.APIRequesterOutputParser\\nParse the request and error tags.\\n\\nchains.api.openapi.response_chain.APIResponderChain\\nGet the response parser.\\n\\nchains.api.openapi.response_chain.APIResponderOutputParser\\nParse the response and error tags.\\n\\nchains.base.Chain\\nAbstract base class for creating structured sequences of calls to components.\\n\\nchains.combine_documents.base.AnalyzeDocumentChain\\nChain that splits documents, then analyzes it in pieces.\\n\\nchains.combine_documents.base.BaseCombineDocumentsChain\\nBase interface for chains combining documents.\\n\\nchains.combine_documents.map_reduce.MapReduceDocumentsChain\\nCombining documents by mapping a chain over them, then combining results.\\n\\nchains.combine_documents.map_rerank.MapRerankDocumentsChain\\nCombining documents by mapping a chain over them, then reranking results.\\n\\nchains.combine_documents.reduce.AsyncCombineDocsProtocol(...)\\nInterface for the combine_docs method.\\n\\nchains.combine_documents.reduce.CombineDocsProtocol(...)\\nInterface for the combine_docs method.\\n\\nchains.combine_documents.reduce.ReduceDocumentsChain\\nCombine documents by recursively reducing them.\\n\\nchains.combine_documents.refine.RefineDocumentsChain\\nCombine documents by doing a first pass and then refining on more documents.\\n\\nchains.combine_documents.stuff.StuffDocumentsChain\\nChain that combines documents by stuffing into context.\\n\\nchains.constitutional_ai.base.ConstitutionalChain\\nChain for applying constitutional principles.\\n\\nchains.constitutional_ai.models.ConstitutionalPrinciple\\nClass for a constitutional principle.\\n\\nchains.conversation.base.ConversationChain\\nChain to have a conversation and load context from memory.\\n\\nchains.conversational_retrieval.base.BaseConversationalRetrievalChain\\nChain for chatting with an index.\\n\\nchains.conversational_retrieval.base.ChatVectorDBChain\\nChain for chatting with a vector database.\\n\\nchains.conversational_retrieval.base.ConversationalRetrievalChain\\nChain for having a conversation based on retrieved documents.\\n\\nchains.conversational_retrieval.base.InputType\\nCreate a new model by parsing and validating input data from keyword arguments.\\n\\nchains.elasticsearch_database.base.ElasticsearchDatabaseChain\\nChain for interacting with Elasticsearch Database.\\n\\nchains.flare.base.FlareChain\\nChain that combines a retriever, a question generator, and a response generator.\\n\\nchains.flare.base.QuestionGeneratorChain\\nChain that generates questions from uncertain spans.\\n\\nchains.flare.prompts.FinishedOutputParser\\nOutput parser that checks if the output is finished.\\n\\nchains.graph_qa.arangodb.ArangoGraphQAChain\\nChain for question-answering against a graph by generating AQL statements.\\n\\nchains.graph_qa.base.GraphQAChain\\nChain for question-answering against a graph.\\n\\nchains.graph_qa.cypher.GraphCypherQAChain\\nChain for question-answering against a graph by generating Cypher statements.\\n\\nchains.graph_qa.cypher_utils.CypherQueryCorrector(schemas)\\nUsed to correct relationship direction in generated Cypher statements.\\n\\nchains.graph_qa.cypher_utils.Schema(...)\\nCreate new instance of Schema(left_node, relation, right_node)\\n\\nchains.graph_qa.falkordb.FalkorDBQAChain\\nChain for question-answering against a graph by generating Cypher statements.\\n\\nchains.graph_qa.hugegraph.HugeGraphQAChain\\nChain for question-answering against a graph by generating gremlin statements.\\n\\nchains.graph_qa.kuzu.KuzuQAChain\\nQuestion-answering against a graph by generating Cypher statements for Kùzu.\\n\\nchains.graph_qa.nebulagraph.NebulaGraphQAChain\\nChain for question-answering against a graph by generating nGQL statements.\\n\\nchains.graph_qa.neptune_cypher.NeptuneOpenCypherQAChain\\nChain for question-answering against a Neptune graph by generating openCypher statements.\\n\\nchains.graph_qa.sparql.GraphSparqlQAChain\\nQuestion-answering against an RDF or OWL graph by generating SPARQL statements.\\n\\nchains.hyde.base.HypotheticalDocumentEmbedder\\nGenerate hypothetical document for query, and then embed that.\\n\\nchains.llm.LLMChain\\nChain to run queries against LLMs.\\n\\nchains.llm_checker.base.LLMCheckerChain\\nChain for question-answering with self-verification.\\n\\nchains.llm_math.base.LLMMathChain\\nChain that interprets a prompt and executes python code to do math.\\n\\nchains.llm_requests.LLMRequestsChain\\nChain that requests a URL and then uses an LLM to parse results.\\n\\nchains.llm_summarization_checker.base.LLMSummarizationCheckerChain\\nChain for question-answering with self-verification.\\n\\nchains.mapreduce.MapReduceChain\\nMap-reduce chain.\\n\\nchains.moderation.OpenAIModerationChain\\nPass input through a moderation endpoint.\\n\\nchains.natbot.base.NatBotChain\\nImplement an LLM driven browser.\\n\\nchains.natbot.crawler.Crawler()\\nA crawler for web pages.\\n\\nchains.natbot.crawler.ElementInViewPort\\nA typed dictionary containing information about elements in the viewport.\\n\\nchains.openai_functions.citation_fuzzy_match.FactWithEvidence\\nClass representing a single statement.\\n\\nchains.openai_functions.citation_fuzzy_match.QuestionAnswer\\nA question and its answer as a list of facts each one should have a source.\\n\\nchains.openai_functions.openapi.SimpleRequestChain\\nChain for making a simple request to an API endpoint.\\n\\nchains.openai_functions.qa_with_structure.AnswerWithSources\\nAn answer to the question, with sources.\\n\\nchains.prompt_selector.BasePromptSelector\\nBase class for prompt selectors.\\n\\nchains.prompt_selector.ConditionalPromptSelector\\nPrompt collection that goes through conditionals.\\n\\nchains.qa_generation.base.QAGenerationChain\\nBase class for question-answer generation chains.\\n\\nchains.qa_with_sources.base.BaseQAWithSourcesChain\\nQuestion answering chain with sources over documents.\\n\\nchains.qa_with_sources.base.QAWithSourcesChain\\nQuestion answering with sources over documents.\\n\\nchains.qa_with_sources.loading.LoadingCallable(...)\\nInterface for loading the combine documents chain.\\n\\nchains.qa_with_sources.retrieval.RetrievalQAWithSourcesChain\\nQuestion-answering with sources over an index.\\n\\nchains.qa_with_sources.vector_db.VectorDBQAWithSourcesChain\\nQuestion-answering with sources over a vector database.\\n\\nchains.query_constructor.base.StructuredQueryOutputParser\\nOutput parser that parses a structured query.\\n\\nchains.query_constructor.ir.Comparator(value)\\nEnumerator of the comparison operators.\\n\\nchains.query_constructor.ir.Comparison\\nA comparison to a value.\\n\\nchains.query_constructor.ir.Expr\\nBase class for all expressions.\\n\\nchains.query_constructor.ir.FilterDirective\\nA filtering expression.\\n\\nchains.query_constructor.ir.Operation\\nA logical operation over other directives.\\n\\nchains.query_constructor.ir.Operator(value)\\nEnumerator of the operations.\\n\\nchains.query_constructor.ir.StructuredQuery\\nA structured query.\\n\\nchains.query_constructor.ir.Visitor()\\nDefines interface for IR translation using visitor pattern.\\n\\nchains.query_constructor.parser.ISO8601Date\\nA date in ISO 8601 format (YYYY-MM-DD).\\n\\nchains.query_constructor.schema.AttributeInfo\\nInformation about a data source attribute.\\n\\nchains.retrieval_qa.base.BaseRetrievalQA\\nBase class for question-answering chains.\\n\\nchains.retrieval_qa.base.RetrievalQA\\nChain for question-answering against an index.\\n\\nchains.retrieval_qa.base.VectorDBQA\\nChain for question-answering against a vector database.\\n\\nchains.router.base.MultiRouteChain\\nUse a single chain to route an input to one of multiple candidate chains.\\n\\nchains.router.base.Route(destination,\\xa0...)\\nCreate new instance of Route(destination, next_inputs)\\n\\nchains.router.base.RouterChain\\nChain that outputs the name of a destination chain and the inputs to it.\\n\\nchains.router.embedding_router.EmbeddingRouterChain\\nChain that uses embeddings to route between options.\\n\\nchains.router.llm_router.LLMRouterChain\\nA router chain that uses an LLM chain to perform routing.\\n\\nchains.router.llm_router.RouterOutputParser\\nParser for output of router chain in the multi-prompt chain.\\n\\nchains.router.multi_prompt.MultiPromptChain\\nA multi-route chain that uses an LLM router chain to choose amongst prompts.\\n\\nchains.router.multi_retrieval_qa.MultiRetrievalQAChain\\nA multi-route chain that uses an LLM router chain to choose amongst retrieval qa chains.\\n\\nchains.sequential.SequentialChain\\nChain where the outputs of one chain feed directly into next.\\n\\nchains.sequential.SimpleSequentialChain\\nSimple chain where the outputs of one step feed directly into next.\\n\\nchains.sql_database.query.SQLInput\\nInput for a SQL Chain.\\n\\nchains.sql_database.query.SQLInputWithTables\\nInput for a SQL Chain.\\n\\nchains.transform.TransformChain\\nChain that transforms the chain output.\\n\\nFunctions¶\\n\\nchains.combine_documents.reduce.acollapse_docs(...)\\nExecute a collapse function on a set of documents and merge their metadatas.\\n\\nchains.combine_documents.reduce.collapse_docs(...)\\nExecute a collapse function on a set of documents and merge their metadatas.\\n\\nchains.combine_documents.reduce.split_list_of_docs(...)\\nSplit Documents into subsets that each meet a cumulative length constraint.\\n\\nchains.ernie_functions.base.convert_python_function_to_ernie_function(...)\\nConvert a Python function to an Ernie function-calling API compatible dict.\\n\\nchains.ernie_functions.base.convert_to_ernie_function(...)\\nConvert a raw function/class to an Ernie function.\\n\\nchains.ernie_functions.base.create_ernie_fn_chain(...)\\n[Legacy] Create an LLM chain that uses Ernie functions.\\n\\nchains.ernie_functions.base.create_ernie_fn_runnable(...)\\nCreate a runnable sequence that uses Ernie functions.\\n\\nchains.ernie_functions.base.create_structured_output_chain(...)\\n[Legacy] Create an LLMChain that uses an Ernie function to get a structured output.\\n\\nchains.ernie_functions.base.create_structured_output_runnable(...)\\nCreate a runnable that uses an Ernie function to get a structured output.\\n\\nchains.ernie_functions.base.get_ernie_output_parser(...)\\nGet the appropriate function output parser given the user functions.\\n\\nchains.example_generator.generate_example(...)\\nReturn another example given a list of examples for a prompt.\\n\\nchains.graph_qa.cypher.construct_schema(...)\\nFilter the schema based on included or excluded types\\n\\nchains.graph_qa.cypher.extract_cypher(text)\\nExtract Cypher code from a text.\\n\\nchains.graph_qa.falkordb.extract_cypher(text)\\nExtract Cypher code from a text.\\n\\nchains.graph_qa.neptune_cypher.extract_cypher(text)\\nExtract Cypher code from text using Regex.\\n\\nchains.graph_qa.neptune_cypher.trim_query(query)\\nTrim the query to only include Cypher keywords.\\n\\nchains.graph_qa.neptune_cypher.use_simple_prompt(llm)\\nDecides whether to use the simple prompt\\n\\nchains.loading.load_chain(path,\\xa0**kwargs)\\nUnified method for loading a chain from LangChainHub or local fs.\\n\\nchains.loading.load_chain_from_config(...)\\nLoad chain from Config Dict.\\n\\nchains.openai_functions.base.convert_python_function_to_openai_function(...)\\nConvert a Python function to an OpenAI function-calling API compatible dict.\\n\\nchains.openai_functions.base.convert_to_openai_function(...)\\nConvert a raw function/class to an OpenAI function.\\n\\nchains.openai_functions.base.create_openai_fn_chain(...)\\n[Legacy] Create an LLM chain that uses OpenAI functions.\\n\\nchains.openai_functions.base.create_openai_fn_runnable(...)\\nCreate a runnable sequence that uses OpenAI functions.\\n\\nchains.openai_functions.base.create_structured_output_chain(...)\\n[Legacy] Create an LLMChain that uses an OpenAI function to get a structured output.\\n\\nchains.openai_functions.base.create_structured_output_runnable(...)\\nCreate a runnable that uses an OpenAI function to get a structured output.\\n\\nchains.openai_functions.base.get_openai_output_parser(...)\\nGet the appropriate function output parser given the user functions.\\n\\nchains.openai_functions.citation_fuzzy_match.create_citation_fuzzy_match_chain(llm)\\nCreate a citation fuzzy match chain.\\n\\nchains.openai_functions.extraction.create_extraction_chain(...)\\nCreates a chain that extracts information from a passage.\\n\\nchains.openai_functions.extraction.create_extraction_chain_pydantic(...)\\nCreates a chain that extracts information from a passage using pydantic schema.\\n\\nchains.openai_functions.openapi.get_openapi_chain(spec)\\nCreate a chain for querying an API from a OpenAPI spec.\\n\\nchains.openai_functions.openapi.openapi_spec_to_openai_fn(spec)\\nConvert a valid OpenAPI spec to the JSON Schema format expected for OpenAI\\n\\nchains.openai_functions.qa_with_structure.create_qa_with_sources_chain(llm)\\nCreate a question answering chain that returns an answer with sources.\\n\\nchains.openai_functions.qa_with_structure.create_qa_with_structure_chain(...)\\nCreate a question answering chain that returns an answer with sources\\n\\nchains.openai_functions.tagging.create_tagging_chain(...)\\nCreates a chain that extracts information from a passage\\n\\nchains.openai_functions.tagging.create_tagging_chain_pydantic(...)\\nCreates a chain that extracts information from a passage\\n\\nchains.openai_functions.utils.get_llm_kwargs(...)\\nReturns the kwargs for the LLMChain constructor.\\n\\nchains.openai_tools.extraction.create_extraction_chain_pydantic(...)\\n\\nchains.prompt_selector.is_chat_model(llm)\\nCheck if the language model is a chat model.\\n\\nchains.prompt_selector.is_llm(llm)\\nCheck if the language model is a LLM.\\n\\nchains.qa_with_sources.loading.load_qa_with_sources_chain(llm)\\nLoad a question answering with sources chain.\\n\\nchains.query_constructor.base.construct_examples(...)\\nConstruct examples from input-output pairs.\\n\\nchains.query_constructor.base.fix_filter_directive(...)\\nFix invalid filter directive.\\n\\nchains.query_constructor.base.get_query_constructor_prompt(...)\\nCreate query construction prompt.\\n\\nchains.query_constructor.base.load_query_constructor_chain(...)\\nLoad a query constructor chain.\\n\\nchains.query_constructor.base.load_query_constructor_runnable(...)\\nLoad a query constructor runnable chain.\\n\\nchains.query_constructor.parser.get_parser([...])\\nReturns a parser for the query language.\\n\\nchains.query_constructor.parser.v_args(...)\\nDummy decorator for when lark is not installed.\\n\\nchains.sql_database.query.create_sql_query_chain(llm,\\xa0db)\\nCreate a chain that generates SQL queries.\\n\\nlangchain.chat_loaders¶\\nChat Loaders load chat messages from common communications platforms.\\nLoad chat messages from various\\ncommunications platforms such as Facebook Messenger, Telegram, and\\nWhatsApp. The loaded chat messages can be used for fine-tuning models.\\nClass hierarchy:\\nBaseChatLoader --> <name>ChatLoader  # Examples: WhatsAppChatLoader, IMessageChatLoader\\n\\nMain helpers:\\nChatSession\\n\\nClasses¶\\n\\nchat_loaders.base.BaseChatLoader()\\nBase class for chat loaders.\\n\\nchat_loaders.facebook_messenger.FolderFacebookMessengerChatLoader(path)\\nLoad Facebook Messenger chat data from a folder.\\n\\nchat_loaders.facebook_messenger.SingleFileFacebookMessengerChatLoader(path)\\nLoad Facebook Messenger chat data from a single file.\\n\\nchat_loaders.gmail.GMailLoader(creds[,\\xa0n,\\xa0...])\\nLoad data from GMail.\\n\\nchat_loaders.imessage.IMessageChatLoader([path])\\nLoad chat sessions from the iMessage chat.db SQLite file.\\n\\nchat_loaders.langsmith.LangSmithDatasetChatLoader(*,\\xa0...)\\nLoad chat sessions from a LangSmith dataset with the \"chat\" data type.\\n\\nchat_loaders.langsmith.LangSmithRunChatLoader(runs)\\nLoad chat sessions from a list of LangSmith \"llm\" runs.\\n\\nchat_loaders.slack.SlackChatLoader(path)\\nLoad Slack conversations from a dump zip file.\\n\\nchat_loaders.telegram.TelegramChatLoader(path)\\nLoad telegram conversations to LangChain chat messages.\\n\\nchat_loaders.whatsapp.WhatsAppChatLoader(path)\\nLoad WhatsApp conversations from a dump zip file or directory.\\n\\nFunctions¶\\n\\nchat_loaders.utils.map_ai_messages(...)\\nConvert messages from the specified \\'sender\\' to AI messages.\\n\\nchat_loaders.utils.map_ai_messages_in_session(...)\\nConvert messages from the specified \\'sender\\' to AI messages.\\n\\nchat_loaders.utils.merge_chat_runs(chat_sessions)\\nMerge chat runs together.\\n\\nchat_loaders.utils.merge_chat_runs_in_session(...)\\nMerge chat runs together in a chat session.\\n\\nlangchain.chat_models¶\\nChat Models are a variation on language models.\\nWhile Chat Models use language models under the hood, the interface they expose\\nis a bit different. Rather than expose a “text in, text out” API, they expose\\nan interface where “chat messages” are the inputs and outputs.\\nClass hierarchy:\\nBaseLanguageModel --> BaseChatModel --> <name>  # Examples: ChatOpenAI, ChatGooglePalm\\n\\nMain helpers:\\nAIMessage, BaseMessage, HumanMessage\\n\\nClasses¶\\n\\nchat_models.anthropic.ChatAnthropic\\nAnthropic chat large language models.\\n\\nchat_models.anyscale.ChatAnyscale\\nAnyscale Chat large language models.\\n\\nchat_models.azure_openai.AzureChatOpenAI\\nAzure OpenAI Chat Completion API.\\n\\nchat_models.azureml_endpoint.AzureMLChatOnlineEndpoint\\nAzureML Chat models API.\\n\\nchat_models.azureml_endpoint.LlamaContentFormatter()\\nContent formatter for LLaMA.\\n\\nchat_models.baichuan.ChatBaichuan\\nBaichuan chat models API by Baichuan Intelligent Technology.\\n\\nchat_models.baidu_qianfan_endpoint.QianfanChatEndpoint\\nBaidu Qianfan chat models.\\n\\nchat_models.bedrock.BedrockChat\\nA chat model that uses the Bedrock API.\\n\\nchat_models.bedrock.ChatPromptAdapter()\\nAdapter class to prepare the inputs from Langchain to prompt format that Chat model expects.\\n\\nchat_models.cohere.ChatCohere\\nCohere chat large language models.\\n\\nchat_models.ernie.ErnieBotChat\\nERNIE-Bot large language model.\\n\\nchat_models.everlyai.ChatEverlyAI\\nEverlyAI Chat large language models.\\n\\nchat_models.fake.FakeListChatModel\\nFake ChatModel for testing purposes.\\n\\nchat_models.fake.FakeMessagesListChatModel\\nFake ChatModel for testing purposes.\\n\\nchat_models.fireworks.ChatFireworks\\nFireworks Chat models.\\n\\nchat_models.gigachat.GigaChat\\nGigaChat large language models API.\\n\\nchat_models.google_palm.ChatGooglePalm\\nGoogle PaLM Chat models API.\\n\\nchat_models.google_palm.ChatGooglePalmError\\nError with the Google PaLM API.\\n\\nchat_models.human.HumanInputChatModel\\nChatModel which returns user input as the response.\\n\\nchat_models.hunyuan.ChatHunyuan\\nTencent Hunyuan chat models API by Tencent.\\n\\nchat_models.javelin_ai_gateway.ChatJavelinAIGateway\\nJavelin AI Gateway chat models API.\\n\\nchat_models.javelin_ai_gateway.ChatParams\\nParameters for the Javelin AI Gateway LLM.\\n\\nchat_models.jinachat.JinaChat\\nJina AI Chat models API.\\n\\nchat_models.konko.ChatKonko\\nChatKonko Chat large language models API.\\n\\nchat_models.litellm.ChatLiteLLM\\nA chat model that uses the LiteLLM API.\\n\\nchat_models.litellm.ChatLiteLLMException\\nError with the LiteLLM I/O library\\n\\nchat_models.minimax.MiniMaxChat\\nWrapper around Minimax large language models.\\n\\nchat_models.mlflow_ai_gateway.ChatMLflowAIGateway\\nMLflow AI Gateway chat models API.\\n\\nchat_models.mlflow_ai_gateway.ChatParams\\nParameters for the MLflow AI Gateway LLM.\\n\\nchat_models.ollama.ChatOllama\\nOllama locally runs large language models.\\n\\nchat_models.openai.ChatOpenAI\\nOpenAI Chat large language models API.\\n\\nchat_models.pai_eas_endpoint.PaiEasChatEndpoint\\nEas LLM Service chat model API.\\n\\nchat_models.promptlayer_openai.PromptLayerChatOpenAI\\nPromptLayer and OpenAI Chat large language models API.\\n\\nchat_models.tongyi.ChatTongyi\\nAlibaba Tongyi Qwen chat models API.\\n\\nchat_models.vertexai.ChatVertexAI\\nVertex AI Chat large language models API.\\n\\nchat_models.yandex.ChatYandexGPT\\nWrapper around YandexGPT large language models.\\n\\nFunctions¶\\n\\nchat_models.anthropic.convert_messages_to_prompt_anthropic(...)\\nFormat a list of messages into a full prompt for the Anthropic model\\n\\nchat_models.baidu_qianfan_endpoint.convert_message_to_dict(message)\\nConvert a message to a dictionary that can be passed to the API.\\n\\nchat_models.cohere.get_cohere_chat_request(...)\\nGet the request for the Cohere chat API.\\n\\nchat_models.cohere.get_role(message)\\nGet the role of the message.\\n\\nchat_models.fireworks.acompletion_with_retry(...)\\nUse tenacity to retry the async completion call.\\n\\nchat_models.fireworks.acompletion_with_retry_streaming(...)\\nUse tenacity to retry the completion call for streaming.\\n\\nchat_models.fireworks.completion_with_retry(...)\\nUse tenacity to retry the completion call.\\n\\nchat_models.fireworks.conditional_decorator(...)\\n\\nchat_models.fireworks.convert_dict_to_message(_dict)\\nConvert a dict response to a message.\\n\\nchat_models.google_palm.achat_with_retry(...)\\nUse tenacity to retry the async completion call.\\n\\nchat_models.google_palm.chat_with_retry(llm,\\xa0...)\\nUse tenacity to retry the completion call.\\n\\nchat_models.jinachat.acompletion_with_retry(...)\\nUse tenacity to retry the async completion call.\\n\\nchat_models.litellm.acompletion_with_retry(llm)\\nUse tenacity to retry the async completion call.\\n\\nchat_models.meta.convert_messages_to_prompt_llama(...)\\n\\nchat_models.openai.acompletion_with_retry(llm)\\nUse tenacity to retry the async completion call.\\n\\nchat_models.tongyi.convert_dict_to_message(_dict)\\n\\nchat_models.tongyi.convert_message_to_dict(message)\\n\\nlangchain.docstore¶\\nDocstores are classes to store and load Documents.\\nThe Docstore is a simplified version of the Document Loader.\\nClass hierarchy:\\nDocstore --> <name> # Examples: InMemoryDocstore, Wikipedia\\n\\nMain helpers:\\nDocument, AddableMixin\\n\\nClasses¶\\n\\ndocstore.arbitrary_fn.DocstoreFn(lookup_fn)\\nLangchain Docstore via arbitrary lookup function.\\n\\ndocstore.base.AddableMixin()\\nMixin class that supports adding texts.\\n\\ndocstore.base.Docstore()\\nInterface to access to place that stores documents.\\n\\ndocstore.in_memory.InMemoryDocstore([_dict])\\nSimple in memory docstore in the form of a dict.\\n\\ndocstore.wikipedia.Wikipedia()\\nWrapper around wikipedia API.\\n\\nlangchain.document_loaders¶\\nDocument Loaders  are classes to load Documents.\\nDocument Loaders are usually used to load a lot of Documents in a single run.\\nClass hierarchy:\\nBaseLoader --> <name>Loader  # Examples: TextLoader, UnstructuredFileLoader\\n\\nMain helpers:\\nDocument, <name>TextSplitter\\n\\nClasses¶\\n\\ndocument_loaders.acreom.AcreomLoader(path[,\\xa0...])\\nLoad acreom vault from a directory.\\n\\ndocument_loaders.airbyte.AirbyteCDKLoader(...)\\nLoad with an Airbyte source connector implemented using the CDK.\\n\\ndocument_loaders.airbyte.AirbyteGongLoader(...)\\nLoad from Gong using an Airbyte source connector.\\n\\ndocument_loaders.airbyte.AirbyteHubspotLoader(...)\\nLoad from Hubspot using an Airbyte source connector.\\n\\ndocument_loaders.airbyte.AirbyteSalesforceLoader(...)\\nLoad from Salesforce using an Airbyte source connector.\\n\\ndocument_loaders.airbyte.AirbyteShopifyLoader(...)\\nLoad from Shopify using an Airbyte source connector.\\n\\ndocument_loaders.airbyte.AirbyteStripeLoader(...)\\nLoad from Stripe using an Airbyte source connector.\\n\\ndocument_loaders.airbyte.AirbyteTypeformLoader(...)\\nLoad from Typeform using an Airbyte source connector.\\n\\ndocument_loaders.airbyte.AirbyteZendeskSupportLoader(...)\\nLoad from Zendesk Support using an Airbyte source connector.\\n\\ndocument_loaders.airbyte_json.AirbyteJSONLoader(...)\\nLoad local Airbyte json files.\\n\\ndocument_loaders.airtable.AirtableLoader(...)\\nLoad the Airtable tables.\\n\\ndocument_loaders.apify_dataset.ApifyDatasetLoader\\nLoad datasets from Apify web scraping, crawling, and data extraction platform.\\n\\ndocument_loaders.arcgis_loader.ArcGISLoader(layer)\\nLoad records from an ArcGIS FeatureLayer.\\n\\ndocument_loaders.arxiv.ArxivLoader(query[,\\xa0...])\\nLoad a query result from Arxiv.\\n\\ndocument_loaders.assemblyai.AssemblyAIAudioTranscriptLoader(...)\\nLoader for AssemblyAI audio transcripts.\\n\\ndocument_loaders.assemblyai.TranscriptFormat(value)\\nTranscript format to use for the document loader.\\n\\ndocument_loaders.async_html.AsyncHtmlLoader(...)\\nLoad HTML asynchronously.\\n\\ndocument_loaders.azlyrics.AZLyricsLoader([...])\\nLoad AZLyrics webpages.\\n\\ndocument_loaders.azure_blob_storage_container.AzureBlobStorageContainerLoader(...)\\nLoad from Azure Blob Storage container.\\n\\ndocument_loaders.azure_blob_storage_file.AzureBlobStorageFileLoader(...)\\nLoad from Azure Blob Storage files.\\n\\ndocument_loaders.baiducloud_bos_directory.BaiduBOSDirectoryLoader(...)\\nLoad from Baidu BOS directory.\\n\\ndocument_loaders.baiducloud_bos_file.BaiduBOSFileLoader(...)\\nLoad from Baidu Cloud BOS file.\\n\\ndocument_loaders.base.BaseBlobParser()\\nAbstract interface for blob parsers.\\n\\ndocument_loaders.base.BaseLoader()\\nInterface for Document Loader.\\n\\ndocument_loaders.base_o365.O365BaseLoader\\nBase class for all loaders that uses O365 Package\\n\\ndocument_loaders.bibtex.BibtexLoader(...[,\\xa0...])\\nLoad a bibtex file.\\n\\ndocument_loaders.bigquery.BigQueryLoader(query)\\nLoad from the Google Cloud Platform BigQuery.\\n\\ndocument_loaders.bilibili.BiliBiliLoader(...)\\nLoad BiliBili video transcripts.\\n\\ndocument_loaders.blackboard.BlackboardLoader(...)\\nLoad a Blackboard course.\\n\\ndocument_loaders.blob_loaders.file_system.FileSystemBlobLoader(path,\\xa0*)\\nLoad blobs in the local file system.\\n\\ndocument_loaders.blob_loaders.schema.Blob\\nBlob represents raw data by either reference or value.\\n\\ndocument_loaders.blob_loaders.schema.BlobLoader()\\nAbstract interface for blob loaders implementation.\\n\\ndocument_loaders.blob_loaders.youtube_audio.YoutubeAudioLoader(...)\\nLoad YouTube urls as audio file(s).\\n\\ndocument_loaders.blockchain.BlockchainDocumentLoader(...)\\nLoad elements from a blockchain smart contract.\\n\\ndocument_loaders.blockchain.BlockchainType(value)\\nEnumerator of the supported blockchains.\\n\\ndocument_loaders.brave_search.BraveSearchLoader(...)\\nLoad with Brave Search engine.\\n\\ndocument_loaders.browserless.BrowserlessLoader(...)\\nLoad webpages with Browserless /content endpoint.\\n\\ndocument_loaders.chatgpt.ChatGPTLoader(log_file)\\nLoad conversations from exported ChatGPT data.\\n\\ndocument_loaders.chromium.AsyncChromiumLoader(urls)\\nScrape HTML pages from URLs using a headless instance of the Chromium.\\n\\ndocument_loaders.college_confidential.CollegeConfidentialLoader([...])\\nLoad College Confidential webpages.\\n\\ndocument_loaders.concurrent.ConcurrentLoader(...)\\nLoad and pars Documents concurrently.\\n\\ndocument_loaders.confluence.ConfluenceLoader(url)\\nLoad Confluence pages.\\n\\ndocument_loaders.confluence.ContentFormat(value)\\nEnumerator of the content formats of Confluence page.\\n\\ndocument_loaders.conllu.CoNLLULoader(file_path)\\nLoad CoNLL-U files.\\n\\ndocument_loaders.csv_loader.CSVLoader(file_path)\\nLoad a CSV file into a list of Documents.\\n\\ndocument_loaders.csv_loader.UnstructuredCSVLoader(...)\\nLoad CSV files using Unstructured.\\n\\ndocument_loaders.cube_semantic.CubeSemanticLoader(...)\\nLoad Cube semantic layer metadata.\\n\\ndocument_loaders.datadog_logs.DatadogLogsLoader(...)\\nLoad Datadog logs.\\n\\ndocument_loaders.dataframe.BaseDataFrameLoader(...)\\nInitialize with dataframe object.\\n\\ndocument_loaders.dataframe.DataFrameLoader(...)\\nLoad Pandas DataFrame.\\n\\ndocument_loaders.diffbot.DiffbotLoader(...)\\nLoad Diffbot json file.\\n\\ndocument_loaders.directory.DirectoryLoader(...)\\nLoad from a directory.\\n\\ndocument_loaders.discord.DiscordChatLoader(...)\\nLoad Discord chat logs.\\n\\ndocument_loaders.docugami.DocugamiLoader\\nLoad from Docugami.\\n\\ndocument_loaders.docusaurus.DocusaurusLoader(url)\\nLoader that leverages the SitemapLoader to loop through the generated pages of a Docusaurus Documentation website and extracts the content by looking for specific HTML tags.\\n\\ndocument_loaders.dropbox.DropboxLoader\\nLoad files from Dropbox.\\n\\ndocument_loaders.duckdb_loader.DuckDBLoader(query)\\nLoad from DuckDB.\\n\\ndocument_loaders.email.OutlookMessageLoader(...)\\nLoads Outlook Message files using extract_msg.\\n\\ndocument_loaders.email.UnstructuredEmailLoader(...)\\nLoad email files using Unstructured.\\n\\ndocument_loaders.embaas.BaseEmbaasLoader\\nBase loader for Embaas document extraction API.\\n\\ndocument_loaders.embaas.EmbaasBlobLoader\\nLoad Embaas blob.\\n\\ndocument_loaders.embaas.EmbaasDocumentExtractionParameters\\nParameters for the embaas document extraction API.\\n\\ndocument_loaders.embaas.EmbaasDocumentExtractionPayload\\nPayload for the Embaas document extraction API.\\n\\ndocument_loaders.embaas.EmbaasLoader\\nLoad from Embaas.\\n\\ndocument_loaders.epub.UnstructuredEPubLoader(...)\\nLoad EPub files using Unstructured.\\n\\ndocument_loaders.etherscan.EtherscanLoader(...)\\nLoad transactions from Ethereum mainnet.\\n\\ndocument_loaders.evernote.EverNoteLoader(...)\\nLoad from EverNote.\\n\\ndocument_loaders.excel.UnstructuredExcelLoader(...)\\nLoad Microsoft Excel files using Unstructured.\\n\\ndocument_loaders.facebook_chat.FacebookChatLoader(path)\\nLoad Facebook Chat messages directory dump.\\n\\ndocument_loaders.fauna.FaunaLoader(query,\\xa0...)\\nLoad from FaunaDB.\\n\\ndocument_loaders.figma.FigmaFileLoader(...)\\nLoad Figma file.\\n\\ndocument_loaders.gcs_directory.GCSDirectoryLoader(...)\\nLoad from GCS directory.\\n\\ndocument_loaders.gcs_file.GCSFileLoader(...)\\nLoad from GCS file.\\n\\ndocument_loaders.generic.GenericLoader(...)\\nGeneric Document Loader.\\n\\ndocument_loaders.geodataframe.GeoDataFrameLoader(...)\\nLoad geopandas Dataframe.\\n\\ndocument_loaders.git.GitLoader(repo_path[,\\xa0...])\\nLoad Git repository files.\\n\\ndocument_loaders.gitbook.GitbookLoader(web_page)\\nLoad GitBook data.\\n\\ndocument_loaders.github.BaseGitHubLoader\\nLoad GitHub repository Issues.\\n\\ndocument_loaders.github.GitHubIssuesLoader\\nLoad issues of a GitHub repository.\\n\\ndocument_loaders.google_speech_to_text.GoogleSpeechToTextLoader(...)\\nLoader for Google Cloud Speech-to-Text audio transcripts.\\n\\ndocument_loaders.googledrive.GoogleDriveLoader\\nLoad Google Docs from Google Drive.\\n\\ndocument_loaders.gutenberg.GutenbergLoader(...)\\nLoad from Gutenberg.org.\\n\\ndocument_loaders.helpers.FileEncoding(...)\\nFile encoding as the NamedTuple.\\n\\ndocument_loaders.hn.HNLoader([web_path,\\xa0...])\\nLoad Hacker News data.\\n\\ndocument_loaders.html.UnstructuredHTMLLoader(...)\\nLoad HTML files using Unstructured.\\n\\ndocument_loaders.html_bs.BSHTMLLoader(file_path)\\nLoad HTML files and parse them with beautiful soup.\\n\\ndocument_loaders.hugging_face_dataset.HuggingFaceDatasetLoader(path)\\nLoad from Hugging Face Hub datasets.\\n\\ndocument_loaders.ifixit.IFixitLoader(web_path)\\nLoad iFixit repair guides, device wikis and answers.\\n\\ndocument_loaders.image.UnstructuredImageLoader(...)\\nLoad PNG and JPG files using Unstructured.\\n\\ndocument_loaders.image_captions.ImageCaptionLoader(images)\\nLoad image captions.\\n\\ndocument_loaders.imsdb.IMSDbLoader([...])\\nLoad IMSDb webpages.\\n\\ndocument_loaders.iugu.IuguLoader(resource[,\\xa0...])\\nLoad from IUGU.\\n\\ndocument_loaders.joplin.JoplinLoader([...])\\nLoad notes from Joplin.\\n\\ndocument_loaders.json_loader.JSONLoader(...)\\nLoad a JSON file using a jq schema.\\n\\ndocument_loaders.lakefs.LakeFSClient(...)\\n\\ndocument_loaders.lakefs.LakeFSLoader(...[,\\xa0...])\\nLoad from lakeFS.\\n\\ndocument_loaders.lakefs.UnstructuredLakeFSLoader(...)\\nArgs:\\n\\ndocument_loaders.larksuite.LarkSuiteDocLoader(...)\\nLoad from LarkSuite (FeiShu).\\n\\ndocument_loaders.markdown.UnstructuredMarkdownLoader(...)\\nLoad Markdown files using Unstructured.\\n\\ndocument_loaders.mastodon.MastodonTootsLoader(...)\\nLoad the Mastodon \\'toots\\'.\\n\\ndocument_loaders.max_compute.MaxComputeLoader(...)\\nLoad from Alibaba Cloud MaxCompute table.\\n\\ndocument_loaders.mediawikidump.MWDumpLoader(...)\\nLoad MediaWiki dump from an XML file.\\n\\ndocument_loaders.merge.MergedDataLoader(loaders)\\nMerge documents from a list of loaders\\n\\ndocument_loaders.mhtml.MHTMLLoader(file_path)\\nParse MHTML files with BeautifulSoup.\\n\\ndocument_loaders.modern_treasury.ModernTreasuryLoader(...)\\nLoad from Modern Treasury.\\n\\ndocument_loaders.mongodb.MongodbLoader(...)\\nLoad MongoDB documents.\\n\\ndocument_loaders.news.NewsURLLoader(urls[,\\xa0...])\\nLoad news articles from URLs using Unstructured.\\n\\ndocument_loaders.notebook.NotebookLoader(path)\\nLoad Jupyter notebook (.ipynb) files.\\n\\ndocument_loaders.notion.NotionDirectoryLoader(path,\\xa0*)\\nLoad Notion directory dump.\\n\\ndocument_loaders.notiondb.NotionDBLoader(...)\\nLoad from Notion DB.\\n\\ndocument_loaders.nuclia.NucliaLoader(path,\\xa0...)\\nLoad from any file type using Nuclia Understanding API.\\n\\ndocument_loaders.obs_directory.OBSDirectoryLoader(...)\\nLoad from Huawei OBS directory.\\n\\ndocument_loaders.obs_file.OBSFileLoader(...)\\nLoad from the Huawei OBS file.\\n\\ndocument_loaders.obsidian.ObsidianLoader(path)\\nLoad Obsidian files from directory.\\n\\ndocument_loaders.odt.UnstructuredODTLoader(...)\\nLoad OpenOffice ODT files using Unstructured.\\n\\ndocument_loaders.onedrive.OneDriveLoader\\nLoad from Microsoft OneDrive.\\n\\ndocument_loaders.onedrive_file.OneDriveFileLoader\\nLoad a file from Microsoft OneDrive.\\n\\ndocument_loaders.open_city_data.OpenCityDataLoader(...)\\nLoad from Open City.\\n\\ndocument_loaders.org_mode.UnstructuredOrgModeLoader(...)\\nLoad Org-Mode files using Unstructured.\\n\\ndocument_loaders.parsers.audio.OpenAIWhisperParser([...])\\nTranscribe and parse audio files.\\n\\ndocument_loaders.parsers.audio.OpenAIWhisperParserLocal([...])\\nTranscribe and parse audio files with OpenAI Whisper model.\\n\\ndocument_loaders.parsers.audio.YandexSTTParser(*)\\nTranscribe and parse audio files.\\n\\ndocument_loaders.parsers.docai.DocAIParser(*)\\nGoogle Cloud Document AI parser.\\n\\ndocument_loaders.parsers.docai.DocAIParsingResults(...)\\nA dataclass to store Document AI parsing results.\\n\\ndocument_loaders.parsers.generic.MimeTypeBasedParser(...)\\nParser that uses mime-types to parse a blob.\\n\\ndocument_loaders.parsers.grobid.GrobidParser(...)\\nLoad  article PDF files using Grobid.\\n\\ndocument_loaders.parsers.grobid.ServerUnavailableException\\nException raised when the Grobid server is unavailable.\\n\\ndocument_loaders.parsers.html.bs4.BS4HTMLParser(*)\\nPparse HTML files using Beautiful Soup.\\n\\ndocument_loaders.parsers.language.cobol.CobolSegmenter(code)\\nCode segmenter for COBOL.\\n\\ndocument_loaders.parsers.language.code_segmenter.CodeSegmenter(code)\\nAbstract class for the code segmenter.\\n\\ndocument_loaders.parsers.language.javascript.JavaScriptSegmenter(code)\\nCode segmenter for JavaScript.\\n\\ndocument_loaders.parsers.language.language_parser.LanguageParser([...])\\nParse using the respective programming language syntax.\\n\\ndocument_loaders.parsers.language.python.PythonSegmenter(code)\\nCode segmenter for Python.\\n\\ndocument_loaders.parsers.msword.MsWordParser()\\nParse the Microsoft Word documents from a blob.\\n\\ndocument_loaders.parsers.pdf.AmazonTextractPDFParser([...])\\nSend PDF files to Amazon Textract and parse them.\\n\\ndocument_loaders.parsers.pdf.DocumentIntelligenceParser(...)\\nLoads a PDF with Azure Document Intelligence (formerly Forms Recognizer) and chunks at character level.\\n\\ndocument_loaders.parsers.pdf.PDFMinerParser([...])\\nParse PDF using PDFMiner.\\n\\ndocument_loaders.parsers.pdf.PDFPlumberParser([...])\\nParse PDF with PDFPlumber.\\n\\ndocument_loaders.parsers.pdf.PyMuPDFParser([...])\\nParse PDF using PyMuPDF.\\n\\ndocument_loaders.parsers.pdf.PyPDFParser([...])\\nLoad PDF using pypdf\\n\\ndocument_loaders.parsers.pdf.PyPDFium2Parser([...])\\nParse PDF with PyPDFium2.\\n\\ndocument_loaders.parsers.txt.TextParser()\\nParser for text blobs.\\n\\ndocument_loaders.pdf.AmazonTextractPDFLoader(...)\\nLoad PDF files from a local file system, HTTP or S3.\\n\\ndocument_loaders.pdf.BasePDFLoader(file_path,\\xa0*)\\nBase Loader class for PDF files.\\n\\ndocument_loaders.pdf.DocumentIntelligenceLoader(...)\\nLoads a PDF with Azure Document Intelligence\\n\\ndocument_loaders.pdf.MathpixPDFLoader(file_path)\\nLoad PDF files using Mathpix service.\\n\\ndocument_loaders.pdf.OnlinePDFLoader(...[,\\xa0...])\\nLoad online PDF.\\n\\ndocument_loaders.pdf.PDFMinerLoader(file_path,\\xa0*)\\nLoad PDF files using PDFMiner.\\n\\ndocument_loaders.pdf.PDFMinerPDFasHTMLLoader(...)\\nLoad PDF files as HTML content using PDFMiner.\\n\\ndocument_loaders.pdf.PDFPlumberLoader(file_path)\\nLoad PDF files using pdfplumber.\\n\\ndocument_loaders.pdf.PyMuPDFLoader(file_path,\\xa0*)\\nLoad PDF files using PyMuPDF.\\n\\ndocument_loaders.pdf.PyPDFDirectoryLoader(path)\\nLoad a directory with PDF files using pypdf and chunks at character level.\\n\\ndocument_loaders.pdf.PyPDFLoader(file_path)\\nLoad PDF using pypdf into list of documents.\\n\\ndocument_loaders.pdf.PyPDFium2Loader(...[,\\xa0...])\\nLoad PDF using pypdfium2 and chunks at character level.\\n\\ndocument_loaders.pdf.UnstructuredPDFLoader(...)\\nLoad PDF files using Unstructured.\\n\\ndocument_loaders.polars_dataframe.PolarsDataFrameLoader(...)\\nLoad Polars DataFrame.\\n\\ndocument_loaders.powerpoint.UnstructuredPowerPointLoader(...)\\nLoad Microsoft PowerPoint files using Unstructured.\\n\\ndocument_loaders.psychic.PsychicLoader(...)\\nLoad from Psychic.dev.\\n\\ndocument_loaders.pubmed.PubMedLoader(query)\\nLoad from the PubMed biomedical library.\\n\\ndocument_loaders.pyspark_dataframe.PySparkDataFrameLoader([...])\\nLoad PySpark DataFrames.\\n\\ndocument_loaders.python.PythonLoader(file_path)\\nLoad Python files, respecting any non-default encoding if specified.\\n\\ndocument_loaders.quip.QuipLoader(api_url,\\xa0...)\\nLoad Quip pages.\\n\\ndocument_loaders.readthedocs.ReadTheDocsLoader(path)\\nLoad ReadTheDocs documentation directory.\\n\\ndocument_loaders.recursive_url_loader.RecursiveUrlLoader(url)\\nLoad all child links from a URL page.\\n\\ndocument_loaders.reddit.RedditPostsLoader(...)\\nLoad Reddit posts.\\n\\ndocument_loaders.roam.RoamLoader(path)\\nLoad Roam files from a directory.\\n\\ndocument_loaders.rocksetdb.ColumnNotFoundError(...)\\nColumn not found error.\\n\\ndocument_loaders.rocksetdb.RocksetLoader(...)\\nLoad from a Rockset database.\\n\\ndocument_loaders.rspace.RSpaceLoader(global_id)\\nLoads  content from RSpace notebooks, folders, documents or PDF Gallery files into Langchain documents.\\n\\ndocument_loaders.rss.RSSFeedLoader([urls,\\xa0...])\\nLoad news articles from RSS feeds using Unstructured.\\n\\ndocument_loaders.rst.UnstructuredRSTLoader(...)\\nLoad RST files using Unstructured.\\n\\ndocument_loaders.rtf.UnstructuredRTFLoader(...)\\nLoad RTF files using Unstructured.\\n\\ndocument_loaders.s3_directory.S3DirectoryLoader(bucket)\\nLoad from Amazon AWS S3 directory.\\n\\ndocument_loaders.s3_file.S3FileLoader(...[,\\xa0...])\\nLoad from Amazon AWS S3 file.\\n\\ndocument_loaders.sharepoint.SharePointLoader\\nLoad  from SharePoint.\\n\\ndocument_loaders.sitemap.SitemapLoader(web_path)\\nLoad a sitemap and its URLs.\\n\\ndocument_loaders.slack_directory.SlackDirectoryLoader(...)\\nLoad from a Slack directory dump.\\n\\ndocument_loaders.snowflake_loader.SnowflakeLoader(...)\\nLoad from Snowflake API.\\n\\ndocument_loaders.spreedly.SpreedlyLoader(...)\\nLoad from Spreedly API.\\n\\ndocument_loaders.srt.SRTLoader(file_path)\\nLoad .srt (subtitle) files.\\n\\ndocument_loaders.stripe.StripeLoader(resource)\\nLoad from Stripe API.\\n\\ndocument_loaders.telegram.TelegramChatApiLoader([...])\\nLoad Telegram chat json directory dump.\\n\\ndocument_loaders.telegram.TelegramChatFileLoader(path)\\nLoad from Telegram chat dump.\\n\\ndocument_loaders.tencent_cos_directory.TencentCOSDirectoryLoader(...)\\nLoad from Tencent Cloud COS directory.\\n\\ndocument_loaders.tencent_cos_file.TencentCOSFileLoader(...)\\nLoad from Tencent Cloud COS file.\\n\\ndocument_loaders.tensorflow_datasets.TensorflowDatasetLoader(...)\\nLoad from TensorFlow Dataset.\\n\\ndocument_loaders.text.TextLoader(file_path)\\nLoad text file.\\n\\ndocument_loaders.tomarkdown.ToMarkdownLoader(...)\\nLoad HTML using 2markdown API.\\n\\ndocument_loaders.toml.TomlLoader(source)\\nLoad TOML files.\\n\\ndocument_loaders.trello.TrelloLoader(client,\\xa0...)\\nLoad cards from a Trello board.\\n\\ndocument_loaders.tsv.UnstructuredTSVLoader(...)\\nLoad TSV files using Unstructured.\\n\\ndocument_loaders.twitter.TwitterTweetLoader(...)\\nLoad Twitter tweets.\\n\\ndocument_loaders.unstructured.UnstructuredAPIFileIOLoader(file)\\nLoad files using Unstructured API.\\n\\ndocument_loaders.unstructured.UnstructuredAPIFileLoader([...])\\nLoad files using Unstructured API.\\n\\ndocument_loaders.unstructured.UnstructuredBaseLoader([...])\\nBase Loader that uses Unstructured.\\n\\ndocument_loaders.unstructured.UnstructuredFileIOLoader(file)\\nLoad files using Unstructured.\\n\\ndocument_loaders.unstructured.UnstructuredFileLoader(...)\\nLoad files using Unstructured.\\n\\ndocument_loaders.url.UnstructuredURLLoader(urls)\\nLoad files from remote URLs using Unstructured.\\n\\ndocument_loaders.url_playwright.PlaywrightEvaluator()\\nAbstract base class for all evaluators.\\n\\ndocument_loaders.url_playwright.PlaywrightURLLoader(urls)\\nLoad HTML pages with Playwright and parse with Unstructured.\\n\\ndocument_loaders.url_playwright.UnstructuredHtmlEvaluator([...])\\nEvaluates the page HTML content using the unstructured library.\\n\\ndocument_loaders.url_selenium.SeleniumURLLoader(urls)\\nLoad HTML pages with Selenium and parse with Unstructured.\\n\\ndocument_loaders.weather.WeatherDataLoader(...)\\nLoad weather data with Open Weather Map API.\\n\\ndocument_loaders.web_base.WebBaseLoader([...])\\nLoad HTML pages using urllib and parse them with `BeautifulSoup\\'.\\n\\ndocument_loaders.whatsapp_chat.WhatsAppChatLoader(path)\\nLoad WhatsApp messages text file.\\n\\ndocument_loaders.wikipedia.WikipediaLoader(query)\\nLoad from Wikipedia.\\n\\ndocument_loaders.word_document.Docx2txtLoader(...)\\nLoad DOCX file using docx2txt and chunks at character level.\\n\\ndocument_loaders.word_document.UnstructuredWordDocumentLoader(...)\\nLoad Microsoft Word file using Unstructured.\\n\\ndocument_loaders.xml.UnstructuredXMLLoader(...)\\nLoad XML file using Unstructured.\\n\\ndocument_loaders.xorbits.XorbitsLoader(...)\\nLoad Xorbits DataFrame.\\n\\ndocument_loaders.youtube.GoogleApiClient([...])\\nGeneric Google API Client.\\n\\ndocument_loaders.youtube.GoogleApiYoutubeLoader(...)\\nLoad all Videos from a YouTube Channel.\\n\\ndocument_loaders.youtube.YoutubeLoader(video_id)\\nLoad YouTube transcripts.\\n\\nFunctions¶\\n\\ndocument_loaders.base_o365.fetch_mime_types(...)\\nFetch the mime types for the specified file types.\\n\\ndocument_loaders.chatgpt.concatenate_rows(...)\\nCombine message information in a readable format ready to be used.\\n\\ndocument_loaders.facebook_chat.concatenate_rows(row)\\nCombine message information in a readable format ready to be used.\\n\\ndocument_loaders.helpers.detect_file_encodings(...)\\nTry to detect the file encoding.\\n\\ndocument_loaders.notebook.concatenate_cells(...)\\nCombine cells information in a readable format ready to be used.\\n\\ndocument_loaders.notebook.remove_newlines(x)\\nRecursively remove newlines, no matter the data structure they are stored in.\\n\\ndocument_loaders.parsers.pdf.extract_from_images_with_rapidocr(images)\\nExtract text from images with RapidOCR.\\n\\ndocument_loaders.parsers.registry.get_parser(...)\\nGet a parser by parser name.\\n\\ndocument_loaders.rocksetdb.default_joiner(docs)\\nDefault joiner for content columns.\\n\\ndocument_loaders.telegram.concatenate_rows(row)\\nCombine message information in a readable format ready to be used.\\n\\ndocument_loaders.telegram.text_to_docs(text)\\nConvert a string or list of strings to a list of Documents with metadata.\\n\\ndocument_loaders.unstructured.get_elements_from_api([...])\\nRetrieve a list of elements from the Unstructured API.\\n\\ndocument_loaders.unstructured.satisfies_min_unstructured_version(...)\\nCheck if the installed Unstructured version exceeds the minimum version for the feature in question.\\n\\ndocument_loaders.unstructured.validate_unstructured_version(...)\\nRaise an error if the Unstructured version does not exceed the specified minimum.\\n\\ndocument_loaders.whatsapp_chat.concatenate_rows(...)\\nCombine message information in a readable format ready to be used.\\n\\nlangchain.document_transformers¶\\nDocument Transformers are classes to transform Documents.\\nDocument Transformers usually used to transform a lot of Documents in a single run.\\nClass hierarchy:\\nBaseDocumentTransformer --> <name>  # Examples: DoctranQATransformer, DoctranTextTranslator\\n\\nMain helpers:\\nDocument\\n\\nClasses¶\\n\\ndocument_transformers.beautiful_soup_transformer.BeautifulSoupTransformer()\\nTransform HTML content by extracting specific tags and removing unwanted ones.\\n\\ndocument_transformers.doctran_text_extract.DoctranPropertyExtractor(...)\\nExtract properties from text documents using doctran.\\n\\ndocument_transformers.doctran_text_qa.DoctranQATransformer([...])\\nExtract QA from text documents using doctran.\\n\\ndocument_transformers.doctran_text_translate.DoctranTextTranslator([...])\\nTranslate text documents using doctran.\\n\\ndocument_transformers.embeddings_redundant_filter.EmbeddingsClusteringFilter\\nPerform K-means clustering on document vectors.\\n\\ndocument_transformers.embeddings_redundant_filter.EmbeddingsRedundantFilter\\nFilter that drops redundant documents by comparing their embeddings.\\n\\ndocument_transformers.google_translate.GoogleTranslateTransformer(...)\\nTranslate text documents using Google Cloud Translation.\\n\\ndocument_transformers.html2text.Html2TextTransformer([...])\\nReplace occurrences of a particular search pattern with a replacement string\\n\\ndocument_transformers.long_context_reorder.LongContextReorder\\nLost in the middle: Performance degrades when models must access relevant information in the middle of long contexts.\\n\\ndocument_transformers.nuclia_text_transform.NucliaTextTransformer(nua)\\nThe Nuclia Understanding API splits into paragraphs and sentences, identifies entities, provides a summary of the text and generates embeddings for all sentences.\\n\\ndocument_transformers.openai_functions.OpenAIMetadataTagger\\nExtract metadata tags from document contents using OpenAI functions.\\n\\nFunctions¶\\n\\ndocument_transformers.beautiful_soup_transformer.get_navigable_strings(element)\\n\\ndocument_transformers.embeddings_redundant_filter.get_stateful_documents(...)\\nConvert a list of documents to a list of documents with state.\\n\\ndocument_transformers.openai_functions.create_metadata_tagger(...)\\nCreate a DocumentTransformer that uses an OpenAI function chain to automatically\\n\\nlangchain.embeddings¶\\nEmbedding models  are wrappers around embedding models\\nfrom different APIs and services.\\nEmbedding models can be LLMs or not.\\nClass hierarchy:\\nEmbeddings --> <name>Embeddings  # Examples: OpenAIEmbeddings, HuggingFaceEmbeddings\\n\\nClasses¶\\n\\nembeddings.aleph_alpha.AlephAlphaAsymmetricSemanticEmbedding\\nAleph Alpha\\'s asymmetric semantic embedding.\\n\\nembeddings.aleph_alpha.AlephAlphaSymmetricSemanticEmbedding\\nThe symmetric version of the Aleph Alpha\\'s semantic embeddings.\\n\\nembeddings.awa.AwaEmbeddings\\nEmbedding documents and queries with Awa DB.\\n\\nembeddings.azure_openai.AzureOpenAIEmbeddings\\nAzure OpenAI Embeddings API.\\n\\nembeddings.baidu_qianfan_endpoint.QianfanEmbeddingsEndpoint\\nBaidu Qianfan Embeddings embedding models.\\n\\nembeddings.bedrock.BedrockEmbeddings\\nBedrock embedding models.\\n\\nembeddings.cache.CacheBackedEmbeddings(...)\\nInterface for caching results from embedding models.\\n\\nembeddings.clarifai.ClarifaiEmbeddings\\nClarifai embedding models.\\n\\nembeddings.cohere.CohereEmbeddings\\nCohere embedding models.\\n\\nembeddings.dashscope.DashScopeEmbeddings\\nDashScope embedding models.\\n\\nembeddings.deepinfra.DeepInfraEmbeddings\\nDeep Infra\\'s embedding inference service.\\n\\nembeddings.edenai.EdenAiEmbeddings\\nEdenAI embedding.\\n\\nembeddings.elasticsearch.ElasticsearchEmbeddings(...)\\nElasticsearch embedding models.\\n\\nembeddings.embaas.EmbaasEmbeddings\\nEmbaas\\'s embedding service.\\n\\nembeddings.embaas.EmbaasEmbeddingsPayload\\nPayload for the Embaas embeddings API.\\n\\nembeddings.ernie.ErnieEmbeddings\\nErnie Embeddings V1 embedding models.\\n\\nembeddings.fake.DeterministicFakeEmbedding\\nFake embedding model that always returns the same embedding vector for the same text.\\n\\nembeddings.fake.FakeEmbeddings\\nFake embedding model.\\n\\nembeddings.fastembed.FastEmbedEmbeddings\\nQdrant FastEmbedding models.\\n\\nembeddings.google_palm.GooglePalmEmbeddings\\nGoogle\\'s PaLM Embeddings APIs.\\n\\nembeddings.gpt4all.GPT4AllEmbeddings\\nGPT4All embedding models.\\n\\nembeddings.gradient_ai.GradientEmbeddings\\nGradient.ai Embedding models.\\n\\nembeddings.gradient_ai.TinyAsyncGradientEmbeddingClient([...])\\nA helper tool to embed Gradient.\\n\\nembeddings.huggingface.HuggingFaceBgeEmbeddings\\nHuggingFace BGE sentence_transformers embedding models.\\n\\nembeddings.huggingface.HuggingFaceEmbeddings\\nHuggingFace sentence_transformers embedding models.\\n\\nembeddings.huggingface.HuggingFaceInferenceAPIEmbeddings\\nEmbed texts using the HuggingFace API.\\n\\nembeddings.huggingface.HuggingFaceInstructEmbeddings\\nWrapper around sentence_transformers embedding models.\\n\\nembeddings.huggingface_hub.HuggingFaceHubEmbeddings\\nHuggingFaceHub embedding models.\\n\\nembeddings.javelin_ai_gateway.JavelinAIGatewayEmbeddings\\nWrapper around embeddings LLMs in the Javelin AI Gateway.\\n\\nembeddings.jina.JinaEmbeddings\\nJina embedding models.\\n\\nembeddings.johnsnowlabs.JohnSnowLabsEmbeddings\\nJohnSnowLabs embedding models\\n\\nembeddings.llamacpp.LlamaCppEmbeddings\\nllama.cpp embedding models.\\n\\nembeddings.llm_rails.LLMRailsEmbeddings\\nLLMRails embedding models.\\n\\nembeddings.localai.LocalAIEmbeddings\\nLocalAI embedding models.\\n\\nembeddings.minimax.MiniMaxEmbeddings\\nMiniMax\\'s embedding service.\\n\\nembeddings.mlflow_gateway.MlflowAIGatewayEmbeddings\\nWrapper around embeddings LLMs in the MLflow AI Gateway.\\n\\nembeddings.modelscope_hub.ModelScopeEmbeddings\\nModelScopeHub embedding models.\\n\\nembeddings.mosaicml.MosaicMLInstructorEmbeddings\\nMosaicML embedding service.\\n\\nembeddings.nlpcloud.NLPCloudEmbeddings\\nNLP Cloud embedding models.\\n\\nembeddings.octoai_embeddings.OctoAIEmbeddings\\nOctoAI Compute Service embedding models.\\n\\nembeddings.ollama.OllamaEmbeddings\\nOllama locally runs large language models.\\n\\nembeddings.openai.OpenAIEmbeddings\\nOpenAI embedding models.\\n\\nembeddings.sagemaker_endpoint.EmbeddingsContentHandler()\\nContent handler for LLM class.\\n\\nembeddings.sagemaker_endpoint.SagemakerEndpointEmbeddings\\nCustom Sagemaker Inference Endpoints.\\n\\nembeddings.self_hosted.SelfHostedEmbeddings\\nCustom embedding models on self-hosted remote hardware.\\n\\nembeddings.self_hosted_hugging_face.SelfHostedHuggingFaceEmbeddings\\nHuggingFace embedding models on self-hosted remote hardware.\\n\\nembeddings.self_hosted_hugging_face.SelfHostedHuggingFaceInstructEmbeddings\\nHuggingFace InstructEmbedding models on self-hosted remote hardware.\\n\\nembeddings.spacy_embeddings.SpacyEmbeddings\\nEmbeddings by SpaCy models.\\n\\nembeddings.tensorflow_hub.TensorflowHubEmbeddings\\nTensorflowHub embedding models.\\n\\nembeddings.vertexai.VertexAIEmbeddings\\nGoogle Cloud VertexAI embedding models.\\n\\nembeddings.voyageai.VoyageEmbeddings\\nVoyage embedding models.\\n\\nembeddings.xinference.XinferenceEmbeddings([...])\\nXinference embedding models.\\n\\nFunctions¶\\n\\nembeddings.dashscope.embed_with_retry(...)\\nUse tenacity to retry the embedding call.\\n\\nembeddings.google_palm.embed_with_retry(...)\\nUse tenacity to retry the completion call.\\n\\nembeddings.localai.async_embed_with_retry(...)\\nUse tenacity to retry the embedding call.\\n\\nembeddings.localai.embed_with_retry(...)\\nUse tenacity to retry the embedding call.\\n\\nembeddings.minimax.embed_with_retry(...)\\nUse tenacity to retry the completion call.\\n\\nembeddings.openai.async_embed_with_retry(...)\\nUse tenacity to retry the embedding call.\\n\\nembeddings.openai.embed_with_retry(...)\\nUse tenacity to retry the embedding call.\\n\\nembeddings.self_hosted_hugging_face.load_embedding_model(...)\\nLoad the embedding model.\\n\\nembeddings.voyageai.embed_with_retry(...)\\nUse tenacity to retry the embedding call.\\n\\nlangchain.evaluation¶\\nEvaluation chains for grading LLM and Chain outputs.\\nThis module contains off-the-shelf evaluation chains for grading the output of\\nLangChain primitives such as language models and chains.\\nLoading an evaluator\\nTo load an evaluator, you can use the load_evaluators or\\nload_evaluator functions with the\\nnames of the evaluators to load.\\nfrom langchain.evaluation import load_evaluator\\n\\nevaluator = load_evaluator(\"qa\")\\nevaluator.evaluate_strings(\\n    prediction=\"We sold more than 40,000 units last week\",\\n    input=\"How many units did we sell last week?\",\\n    reference=\"We sold 32,378 units\",\\n)\\n\\nThe evaluator must be one of EvaluatorType.\\nDatasets\\nTo load one of the LangChain HuggingFace datasets, you can use the load_dataset function with the\\nname of the dataset to load.\\nfrom langchain.evaluation import load_dataset\\nds = load_dataset(\"llm-math\")\\n\\nSome common use cases for evaluation include:\\n\\nGrading the accuracy of a response against ground truth answers: QAEvalChain\\nComparing the output of two models: PairwiseStringEvalChain or LabeledPairwiseStringEvalChain when there is additionally a reference label.\\nJudging the efficacy of an agent’s tool usage: TrajectoryEvalChain\\nChecking whether an output complies with a set of criteria: CriteriaEvalChain or LabeledCriteriaEvalChain when there is additionally a reference label.\\nComputing semantic difference between a prediction and reference: EmbeddingDistanceEvalChain or between two predictions: PairwiseEmbeddingDistanceEvalChain\\nMeasuring the string distance between a prediction and reference StringDistanceEvalChain or between two predictions PairwiseStringDistanceEvalChain\\n\\nLow-level API\\nThese evaluators implement one of the following interfaces:\\n\\nStringEvaluator: Evaluate a prediction string against a reference label and/or input context.\\nPairwiseStringEvaluator: Evaluate two prediction strings against each other. Useful for scoring preferences, measuring similarity between two chain or llm agents, or comparing outputs on similar inputs.\\nAgentTrajectoryEvaluator Evaluate the full sequence of actions taken by an agent.\\n\\nThese interfaces enable easier composability and usage within a higher level evaluation framework.\\n\\nClasses¶\\n\\nevaluation.agents.trajectory_eval_chain.TrajectoryEval\\nA named tuple containing the score and reasoning for a trajectory.\\n\\nevaluation.agents.trajectory_eval_chain.TrajectoryEvalChain\\nA chain for evaluating ReAct style agents.\\n\\nevaluation.agents.trajectory_eval_chain.TrajectoryOutputParser\\nTrajectory output parser.\\n\\nevaluation.comparison.eval_chain.LabeledPairwiseStringEvalChain\\nA chain for comparing two outputs, such as the outputs\\n\\nevaluation.comparison.eval_chain.PairwiseStringEvalChain\\nA chain for comparing two outputs, such as the outputs\\n\\nevaluation.comparison.eval_chain.PairwiseStringResultOutputParser\\nA parser for the output of the PairwiseStringEvalChain.\\n\\nevaluation.criteria.eval_chain.Criteria(value)\\nA Criteria to evaluate.\\n\\nevaluation.criteria.eval_chain.CriteriaEvalChain\\nLLM Chain for evaluating runs against criteria.\\n\\nevaluation.criteria.eval_chain.CriteriaResultOutputParser\\nA parser for the output of the CriteriaEvalChain.\\n\\nevaluation.criteria.eval_chain.LabeledCriteriaEvalChain\\nCriteria evaluation chain that requires references.\\n\\nevaluation.embedding_distance.base.EmbeddingDistance(value)\\nEmbedding Distance Metric.\\n\\nevaluation.embedding_distance.base.EmbeddingDistanceEvalChain\\nUse embedding distances to score semantic difference between a prediction and reference.\\n\\nevaluation.embedding_distance.base.PairwiseEmbeddingDistanceEvalChain\\nUse embedding distances to score semantic difference between two predictions.\\n\\nevaluation.exact_match.base.ExactMatchStringEvaluator(*)\\nCompute an exact match between the prediction and the reference.\\n\\nevaluation.parsing.base.JsonEqualityEvaluator([...])\\nEvaluates whether the prediction is equal to the reference after\\n\\nevaluation.parsing.base.JsonValidityEvaluator(...)\\nEvaluates whether the prediction is valid JSON.\\n\\nevaluation.parsing.json_distance.JsonEditDistanceEvaluator([...])\\nAn evaluator that calculates the edit distance between JSON strings.\\n\\nevaluation.parsing.json_schema.JsonSchemaEvaluator(...)\\nAn evaluator that validates a JSON prediction against a JSON schema reference.\\n\\nevaluation.qa.eval_chain.ContextQAEvalChain\\nLLM Chain for evaluating QA w/o GT based on context\\n\\nevaluation.qa.eval_chain.CotQAEvalChain\\nLLM Chain for evaluating QA using chain of thought reasoning.\\n\\nevaluation.qa.eval_chain.QAEvalChain\\nLLM Chain for evaluating question answering.\\n\\nevaluation.qa.generate_chain.QAGenerateChain\\nLLM Chain for generating examples for question answering.\\n\\nevaluation.regex_match.base.RegexMatchStringEvaluator(*)\\nCompute a regex match between the prediction and the reference.\\n\\nevaluation.schema.AgentTrajectoryEvaluator()\\nInterface for evaluating agent trajectories.\\n\\nevaluation.schema.EvaluatorType(value[,\\xa0...])\\nThe types of the evaluators.\\n\\nevaluation.schema.LLMEvalChain\\nA base class for evaluators that use an LLM.\\n\\nevaluation.schema.PairwiseStringEvaluator()\\nCompare the output of two models (or two outputs of the same model).\\n\\nevaluation.schema.StringEvaluator()\\nGrade, tag, or otherwise evaluate predictions relative to their inputs and/or reference labels.\\n\\nevaluation.scoring.eval_chain.LabeledScoreStringEvalChain\\nA chain for scoring the output of a model on a scale of 1-10.\\n\\nevaluation.scoring.eval_chain.ScoreStringEvalChain\\nA chain for scoring on a scale of 1-10 the output of a model.\\n\\nevaluation.scoring.eval_chain.ScoreStringResultOutputParser\\nA parser for the output of the ScoreStringEvalChain.\\n\\nevaluation.string_distance.base.PairwiseStringDistanceEvalChain\\nCompute string edit distances between two predictions.\\n\\nevaluation.string_distance.base.StringDistance(value)\\nDistance metric to use.\\n\\nevaluation.string_distance.base.StringDistanceEvalChain\\nCompute string distances between the prediction and the reference.\\n\\nFunctions¶\\n\\nevaluation.comparison.eval_chain.resolve_pairwise_criteria(...)\\nResolve the criteria for the pairwise evaluator.\\n\\nevaluation.criteria.eval_chain.resolve_criteria(...)\\nResolve the criteria to evaluate.\\n\\nevaluation.loading.load_dataset(uri)\\nLoad a dataset from the LangChainDatasets on HuggingFace.\\n\\nevaluation.loading.load_evaluator(evaluator,\\xa0*)\\nLoad the requested evaluation chain specified by a string.\\n\\nevaluation.loading.load_evaluators(evaluators,\\xa0*)\\nLoad evaluators specified by a list of evaluator types.\\n\\nevaluation.scoring.eval_chain.resolve_criteria(...)\\nResolve the criteria for the pairwise evaluator.\\n\\nlangchain.graphs¶\\nGraphs provide a natural language interface to graph databases.\\n\\nClasses¶\\n\\ngraphs.arangodb_graph.ArangoGraph(db)\\nArangoDB wrapper for graph operations.\\n\\ngraphs.falkordb_graph.FalkorDBGraph(database)\\nFalkorDB wrapper for graph operations.\\n\\ngraphs.graph_document.GraphDocument\\nRepresents a graph document consisting of nodes and relationships.\\n\\ngraphs.graph_document.Node\\nRepresents a node in a graph with associated properties.\\n\\ngraphs.graph_document.Relationship\\nRepresents a directed relationship between two nodes in a graph.\\n\\ngraphs.graph_store.GraphStore()\\nAn abstract class wrapper for graph operations.\\n\\ngraphs.hugegraph.HugeGraph([username,\\xa0...])\\nHugeGraph wrapper for graph operations.\\n\\ngraphs.kuzu_graph.KuzuGraph(db[,\\xa0database])\\nKùzu wrapper for graph operations.\\n\\ngraphs.memgraph_graph.MemgraphGraph(url,\\xa0...)\\nMemgraph wrapper for graph operations.\\n\\ngraphs.nebula_graph.NebulaGraph(space[,\\xa0...])\\nNebulaGraph wrapper for graph operations.\\n\\ngraphs.neo4j_graph.Neo4jGraph([url,\\xa0...])\\nNeo4j wrapper for graph operations.\\n\\ngraphs.neptune_graph.NeptuneGraph(host[,\\xa0...])\\nNeptune wrapper for graph operations.\\n\\ngraphs.neptune_graph.NeptuneQueryException(...)\\nA class to handle queries that fail to execute\\n\\ngraphs.networkx_graph.KnowledgeTriple(...)\\nA triple in the graph.\\n\\ngraphs.networkx_graph.NetworkxEntityGraph([graph])\\nNetworkx wrapper for entity graph operations.\\n\\ngraphs.rdf_graph.RdfGraph([source_file,\\xa0...])\\nRDFlib wrapper for graph operations.\\n\\nFunctions¶\\n\\ngraphs.arangodb_graph.get_arangodb_client([...])\\nGet the Arango DB client from credentials.\\n\\ngraphs.networkx_graph.get_entities(entity_str)\\nExtract entities from entity string.\\n\\ngraphs.networkx_graph.parse_triples(...)\\nParse knowledge triples from the knowledge string.\\n\\nlangchain.hub¶\\nInterface with the LangChain Hub.\\n\\nFunctions¶\\n\\nhub.pull(owner_repo_commit,\\xa0*[,\\xa0api_url,\\xa0...])\\nPulls an object from the hub and returns it as a LangChain object.\\n\\nhub.push(repo_full_name,\\xa0object,\\xa0*[,\\xa0...])\\nPushes an object to the hub and returns the URL it can be viewed at in a browser.\\n\\nlangchain.indexes¶\\nCode to support various indexing workflows.\\nProvides code to:\\n\\nCreate knowledge graphs from data.\\nSupport indexing workflows from LangChain data loaders to vectorstores.\\n\\nFor indexing workflows, this code is used to avoid writing duplicated content\\ninto the vectostore and to avoid over-writing content if it’s unchanged.\\nImportantly, this keeps on working even if the content being written is derived\\nvia a set of transformations from some source content (e.g., indexing children\\ndocuments that were derived from parent documents by chunking.)\\n\\nClasses¶\\n\\nindexes.base.RecordManager(namespace)\\nAn abstract base class representing the interface for a record manager.\\n\\nindexes.graph.GraphIndexCreator\\nFunctionality to create graph index.\\n\\nindexes.vectorstore.VectorStoreIndexWrapper\\nWrapper around a vectorstore for easy access.\\n\\nindexes.vectorstore.VectorstoreIndexCreator\\nLogic for creating indexes.\\n\\nFunctions¶\\n\\nlangchain.llms¶\\nLLM classes provide\\naccess to the large language model (LLM) APIs and services.\\nClass hierarchy:\\nBaseLanguageModel --> BaseLLM --> LLM --> <name>  # Examples: AI21, HuggingFaceHub, OpenAI\\n\\nMain helpers:\\nLLMResult, PromptValue,\\nCallbackManagerForLLMRun, AsyncCallbackManagerForLLMRun,\\nCallbackManager, AsyncCallbackManager,\\nAIMessage, BaseMessage\\n\\nClasses¶\\n\\nllms.ai21.AI21\\nAI21 large language models.\\n\\nllms.ai21.AI21PenaltyData\\nParameters for AI21 penalty data.\\n\\nllms.aleph_alpha.AlephAlpha\\nAleph Alpha large language models.\\n\\nllms.amazon_api_gateway.AmazonAPIGateway\\nAmazon API Gateway to access LLM models hosted on AWS.\\n\\nllms.amazon_api_gateway.ContentHandlerAmazonAPIGateway()\\nAdapter to prepare the inputs from Langchain to a format that LLM model expects.\\n\\nllms.anthropic.Anthropic\\nAnthropic large language models.\\n\\nllms.anyscale.Anyscale\\nAnyscale large language models.\\n\\nllms.arcee.Arcee\\nArcee\\'s Domain Adapted Language Models (DALMs).\\n\\nllms.aviary.Aviary\\nAviary hosted models.\\n\\nllms.aviary.AviaryBackend(backend_url,\\xa0bearer)\\nAviary backend.\\n\\nllms.azureml_endpoint.AzureMLEndpointClient(...)\\nAzureML Managed Endpoint client.\\n\\nllms.azureml_endpoint.AzureMLOnlineEndpoint\\nAzure ML Online Endpoint models.\\n\\nllms.azureml_endpoint.ContentFormatterBase()\\nTransform request and response of AzureML endpoint to match with required schema.\\n\\nllms.azureml_endpoint.DollyContentFormatter()\\nContent handler for the Dolly-v2-12b model\\n\\nllms.azureml_endpoint.GPT2ContentFormatter()\\nContent handler for GPT2\\n\\nllms.azureml_endpoint.HFContentFormatter()\\nContent handler for LLMs from the HuggingFace catalog.\\n\\nllms.azureml_endpoint.LlamaContentFormatter()\\nContent formatter for LLaMa\\n\\nllms.azureml_endpoint.OSSContentFormatter()\\nDeprecated: Kept for backwards compatibility\\n\\nllms.baidu_qianfan_endpoint.QianfanLLMEndpoint\\nBaidu Qianfan hosted open source or customized models.\\n\\nllms.bananadev.Banana\\nBanana large language models.\\n\\nllms.baseten.Baseten\\nBaseten models.\\n\\nllms.beam.Beam\\nBeam API for gpt2 large language model.\\n\\nllms.bedrock.Bedrock\\nBedrock models.\\n\\nllms.bedrock.BedrockBase\\nBase class for Bedrock models.\\n\\nllms.bedrock.LLMInputOutputAdapter()\\nAdapter class to prepare the inputs from Langchain to a format that LLM model expects.\\n\\nllms.bittensor.NIBittensorLLM\\nNIBittensor LLMs\\n\\nllms.cerebriumai.CerebriumAI\\nCerebriumAI large language models.\\n\\nllms.chatglm.ChatGLM\\nChatGLM LLM service.\\n\\nllms.clarifai.Clarifai\\nClarifai large language models.\\n\\nllms.cohere.BaseCohere\\nBase class for Cohere models.\\n\\nllms.cohere.Cohere\\nCohere large language models.\\n\\nllms.ctransformers.CTransformers\\nC Transformers LLM models.\\n\\nllms.ctranslate2.CTranslate2\\nCTranslate2 language model.\\n\\nllms.databricks.Databricks\\nDatabricks serving endpoint or a cluster driver proxy app for LLM.\\n\\nllms.deepinfra.DeepInfra\\nDeepInfra models.\\n\\nllms.deepsparse.DeepSparse\\nNeural Magic DeepSparse LLM interface.\\n\\nllms.edenai.EdenAI\\nWrapper around edenai models.\\n\\nllms.fake.FakeListLLM\\nFake LLM for testing purposes.\\n\\nllms.fake.FakeStreamingListLLM\\nFake streaming list LLM for testing purposes.\\n\\nllms.fireworks.Fireworks\\nFireworks models.\\n\\nllms.forefrontai.ForefrontAI\\nForefrontAI large language models.\\n\\nllms.gigachat.GigaChat\\nGigaChat large language models API.\\n\\nllms.google_palm.GooglePalm\\nGoogle PaLM models.\\n\\nllms.gooseai.GooseAI\\nGooseAI large language models.\\n\\nllms.gpt4all.GPT4All\\nGPT4All language models.\\n\\nllms.gradient_ai.GradientLLM\\nGradient.ai LLM Endpoints.\\n\\nllms.gradient_ai.TrainResult\\nTrain result.\\n\\nllms.huggingface_endpoint.HuggingFaceEndpoint\\nHuggingFace Endpoint models.\\n\\nllms.huggingface_hub.HuggingFaceHub\\nHuggingFaceHub  models.\\n\\nllms.huggingface_pipeline.HuggingFacePipeline\\nHuggingFace Pipeline API.\\n\\nllms.huggingface_text_gen_inference.HuggingFaceTextGenInference\\nHuggingFace text generation API.\\n\\nllms.human.HumanInputLLM\\nIt returns user input as the response.\\n\\nllms.javelin_ai_gateway.JavelinAIGateway\\nJavelin AI Gateway LLMs.\\n\\nllms.javelin_ai_gateway.Params\\nParameters for the Javelin AI Gateway LLM.\\n\\nllms.koboldai.KoboldApiLLM\\nKobold API language model.\\n\\nllms.llamacpp.LlamaCpp\\nllama.cpp model.\\n\\nllms.manifest.ManifestWrapper\\nHazyResearch\\'s Manifest library.\\n\\nllms.minimax.Minimax\\nWrapper around Minimax large language models.\\n\\nllms.minimax.MinimaxCommon\\nCommon parameters for Minimax large language models.\\n\\nllms.mlflow_ai_gateway.MlflowAIGateway\\nWrapper around completions LLMs in the MLflow AI Gateway.\\n\\nllms.mlflow_ai_gateway.Params\\nParameters for the MLflow AI Gateway LLM.\\n\\nllms.modal.Modal\\nModal large language models.\\n\\nllms.mosaicml.MosaicML\\nMosaicML LLM service.\\n\\nllms.nlpcloud.NLPCloud\\nNLPCloud large language models.\\n\\nllms.octoai_endpoint.OctoAIEndpoint\\nOctoAI LLM Endpoints.\\n\\nllms.ollama.Ollama\\nOllama locally runs large language models.\\n\\nllms.opaqueprompts.OpaquePrompts\\nAn LLM wrapper that uses OpaquePrompts to sanitize prompts.\\n\\nllms.openai.AzureOpenAI\\nAzure-specific OpenAI large language models.\\n\\nllms.openai.BaseOpenAI\\nBase OpenAI large language model class.\\n\\nllms.openai.OpenAI\\nOpenAI large language models.\\n\\nllms.openai.OpenAIChat\\nOpenAI Chat large language models.\\n\\nllms.openllm.IdentifyingParams\\nParameters for identifying a model as a typed dict.\\n\\nllms.openllm.OpenLLM\\nOpenLLM, supporting both in-process model instance and remote OpenLLM servers.\\n\\nllms.openlm.OpenLM\\nOpenLM models.\\n\\nllms.pai_eas_endpoint.PaiEasEndpoint\\nLangchain LLM class to help to access eass llm service.\\n\\nllms.petals.Petals\\nPetals Bloom models.\\n\\nllms.pipelineai.PipelineAI\\nPipelineAI large language models.\\n\\nllms.predibase.Predibase\\nUse your Predibase models with Langchain.\\n\\nllms.predictionguard.PredictionGuard\\nPrediction Guard large language models.\\n\\nllms.promptlayer_openai.PromptLayerOpenAI\\nPromptLayer OpenAI large language models.\\n\\nllms.promptlayer_openai.PromptLayerOpenAIChat\\nWrapper around OpenAI large language models.\\n\\nllms.replicate.Replicate\\nReplicate models.\\n\\nllms.rwkv.RWKV\\nRWKV language models.\\n\\nllms.sagemaker_endpoint.ContentHandlerBase()\\nA handler class to transform input from LLM to a format that SageMaker endpoint expects.\\n\\nllms.sagemaker_endpoint.LLMContentHandler()\\nContent handler for LLM class.\\n\\nllms.sagemaker_endpoint.LineIterator(stream)\\nA helper class for parsing the byte stream input.\\n\\nllms.sagemaker_endpoint.SagemakerEndpoint\\nSagemaker Inference Endpoint models.\\n\\nllms.self_hosted.SelfHostedPipeline\\nModel inference on self-hosted remote hardware.\\n\\nllms.self_hosted_hugging_face.SelfHostedHuggingFaceLLM\\nHuggingFace Pipeline API to run on self-hosted remote hardware.\\n\\nllms.stochasticai.StochasticAI\\nStochasticAI large language models.\\n\\nllms.symblai_nebula.Nebula\\nNebula Service models.\\n\\nllms.textgen.TextGen\\ntext-generation-webui models.\\n\\nllms.titan_takeoff.TitanTakeoff\\nWrapper around Titan Takeoff APIs.\\n\\nllms.titan_takeoff_pro.TitanTakeoffPro\\nCreate a new model by parsing and validating input data from keyword arguments.\\n\\nllms.together.Together\\nWrapper around Together AI models.\\n\\nllms.tongyi.Tongyi\\nTongyi Qwen large language models.\\n\\nllms.vertexai.VertexAI\\nGoogle Vertex AI large language models.\\n\\nllms.vertexai.VertexAIModelGarden\\nLarge language models served from Vertex AI Model Garden.\\n\\nllms.vllm.VLLM\\nVLLM language model.\\n\\nllms.vllm.VLLMOpenAI\\nvLLM OpenAI-compatible API client\\n\\nllms.writer.Writer\\nWriter large language models.\\n\\nllms.xinference.Xinference\\nWrapper for accessing Xinference\\'s large-scale model inference service.\\n\\nllms.yandex.YandexGPT\\nYandex large language models.\\n\\nFunctions¶\\n\\nllms.anyscale.create_llm_result(choices,\\xa0...)\\nCreate the LLMResult from the choices and prompts.\\n\\nllms.anyscale.update_token_usage(keys,\\xa0...)\\nUpdate token usage.\\n\\nllms.aviary.get_completions(model,\\xa0prompt[,\\xa0...])\\nGet completions from Aviary models.\\n\\nllms.aviary.get_models()\\nList available models\\n\\nllms.cohere.acompletion_with_retry(llm,\\xa0**kwargs)\\nUse tenacity to retry the completion call.\\n\\nllms.cohere.completion_with_retry(llm,\\xa0**kwargs)\\nUse tenacity to retry the completion call.\\n\\nllms.databricks.get_default_api_token()\\nGets the default Databricks personal access token.\\n\\nllms.databricks.get_default_host()\\nGets the default Databricks workspace hostname.\\n\\nllms.databricks.get_repl_context()\\nGets the notebook REPL context if running inside a Databricks notebook.\\n\\nllms.fireworks.acompletion_with_retry(llm,\\xa0...)\\nUse tenacity to retry the completion call.\\n\\nllms.fireworks.acompletion_with_retry_batching(...)\\nUse tenacity to retry the completion call.\\n\\nllms.fireworks.acompletion_with_retry_streaming(...)\\nUse tenacity to retry the completion call for streaming.\\n\\nllms.fireworks.completion_with_retry(llm,\\xa0...)\\nUse tenacity to retry the completion call.\\n\\nllms.fireworks.completion_with_retry_batching(...)\\nUse tenacity to retry the completion call.\\n\\nllms.fireworks.conditional_decorator(...)\\n\\nllms.google_palm.generate_with_retry(llm,\\xa0...)\\nUse tenacity to retry the completion call.\\n\\nllms.koboldai.clean_url(url)\\nRemove trailing slash and /api from url if present.\\n\\nllms.loading.load_llm(file)\\nLoad LLM from file.\\n\\nllms.loading.load_llm_from_config(config)\\nLoad LLM from Config Dict.\\n\\nllms.openai.acompletion_with_retry(llm[,\\xa0...])\\nUse tenacity to retry the async completion call.\\n\\nllms.openai.completion_with_retry(llm[,\\xa0...])\\nUse tenacity to retry the completion call.\\n\\nllms.openai.update_token_usage(keys,\\xa0...)\\nUpdate token usage.\\n\\nllms.symblai_nebula.completion_with_retry(...)\\nUse tenacity to retry the completion call.\\n\\nllms.symblai_nebula.make_request(self,\\xa0...)\\nGenerate text from the model.\\n\\nllms.tongyi.generate_with_retry(llm,\\xa0**kwargs)\\nUse tenacity to retry the completion call.\\n\\nllms.tongyi.stream_generate_with_retry(llm,\\xa0...)\\nUse tenacity to retry the completion call.\\n\\nllms.utils.enforce_stop_tokens(text,\\xa0stop)\\nCut off the text as soon as any stop words occur.\\n\\nllms.vertexai.acompletion_with_retry(llm,\\xa0*args)\\nUse tenacity to retry the completion call.\\n\\nllms.vertexai.completion_with_retry(llm,\\xa0*args)\\nUse tenacity to retry the completion call.\\n\\nllms.vertexai.is_codey_model(model_name)\\nReturns True if the model name is a Codey model.\\n\\nllms.vertexai.stream_completion_with_retry(...)\\nUse tenacity to retry the completion call.\\n\\nlangchain.memory¶\\nMemory maintains Chain state, incorporating context from past runs.\\nClass hierarchy for Memory:\\nBaseMemory --> BaseChatMemory --> <name>Memory  # Examples: ZepMemory, MotorheadMemory\\n\\nMain helpers:\\nBaseChatMessageHistory\\n\\nChat Message History stores the chat message history in different stores.\\nClass hierarchy for ChatMessageHistory:\\nBaseChatMessageHistory --> <name>ChatMessageHistory  # Example: ZepChatMessageHistory\\n\\nMain helpers:\\nAIMessage, BaseMessage, HumanMessage\\n\\nClasses¶\\n\\nmemory.buffer.ConversationBufferMemory\\nBuffer for storing conversation memory.\\n\\nmemory.buffer.ConversationStringBufferMemory\\nBuffer for storing conversation memory.\\n\\nmemory.buffer_window.ConversationBufferWindowMemory\\nBuffer for storing conversation memory inside a limited size window.\\n\\nmemory.chat_memory.BaseChatMemory\\nAbstract base class for chat memory.\\n\\nmemory.chat_message_histories.cassandra.CassandraChatMessageHistory(...)\\nChat message history that stores history in Cassandra.\\n\\nmemory.chat_message_histories.cosmos_db.CosmosDBChatMessageHistory(...)\\nChat message history backed by Azure CosmosDB.\\n\\nmemory.chat_message_histories.dynamodb.DynamoDBChatMessageHistory(...)\\nChat message history that stores history in AWS DynamoDB.\\n\\nmemory.chat_message_histories.elasticsearch.ElasticsearchChatMessageHistory(...)\\nChat message history that stores history in Elasticsearch.\\n\\nmemory.chat_message_histories.file.FileChatMessageHistory(...)\\nChat message history that stores history in a local file.\\n\\nmemory.chat_message_histories.firestore.FirestoreChatMessageHistory(...)\\nChat message history backed by Google Firestore.\\n\\nmemory.chat_message_histories.in_memory.ChatMessageHistory\\nIn memory implementation of chat message history.\\n\\nmemory.chat_message_histories.momento.MomentoChatMessageHistory(...)\\nChat message history cache that uses Momento as a backend.\\n\\nmemory.chat_message_histories.mongodb.MongoDBChatMessageHistory(...)\\nChat message history that stores history in MongoDB.\\n\\nmemory.chat_message_histories.neo4j.Neo4jChatMessageHistory(...)\\nChat message history stored in a Neo4j database.\\n\\nmemory.chat_message_histories.postgres.PostgresChatMessageHistory(...)\\nChat message history stored in a Postgres database.\\n\\nmemory.chat_message_histories.redis.RedisChatMessageHistory(...)\\nChat message history stored in a Redis database.\\n\\nmemory.chat_message_histories.rocksetdb.RocksetChatMessageHistory(...)\\nUses Rockset to store chat messages.\\n\\nmemory.chat_message_histories.singlestoredb.SingleStoreDBChatMessageHistory(...)\\nChat message history stored in a SingleStoreDB database.\\n\\nmemory.chat_message_histories.sql.BaseMessageConverter()\\nThe class responsible for converting BaseMessage to your SQLAlchemy model.\\n\\nmemory.chat_message_histories.sql.DefaultMessageConverter(...)\\nThe default message converter for SQLChatMessageHistory.\\n\\nmemory.chat_message_histories.sql.SQLChatMessageHistory(...)\\nChat message history stored in an SQL database.\\n\\nmemory.chat_message_histories.streamlit.StreamlitChatMessageHistory([key])\\nChat message history that stores messages in Streamlit session state.\\n\\nmemory.chat_message_histories.upstash_redis.UpstashRedisChatMessageHistory(...)\\nChat message history stored in an Upstash Redis database.\\n\\nmemory.chat_message_histories.xata.XataChatMessageHistory(...)\\nChat message history stored in a Xata database.\\n\\nmemory.chat_message_histories.zep.ZepChatMessageHistory(...)\\nChat message history that uses Zep as a backend.\\n\\nmemory.combined.CombinedMemory\\nCombining multiple memories\\' data together.\\n\\nmemory.entity.BaseEntityStore\\nAbstract base class for Entity store.\\n\\nmemory.entity.ConversationEntityMemory\\nEntity extractor & summarizer memory.\\n\\nmemory.entity.InMemoryEntityStore\\nIn-memory Entity store.\\n\\nmemory.entity.RedisEntityStore\\nRedis-backed Entity store.\\n\\nmemory.entity.SQLiteEntityStore\\nSQLite-backed Entity store\\n\\nmemory.entity.UpstashRedisEntityStore\\nUpstash Redis backed Entity store.\\n\\nmemory.kg.ConversationKGMemory\\nKnowledge graph conversation memory.\\n\\nmemory.motorhead_memory.MotorheadMemory\\nChat message memory backed by Motorhead service.\\n\\nmemory.readonly.ReadOnlySharedMemory\\nA memory wrapper that is read-only and cannot be changed.\\n\\nmemory.simple.SimpleMemory\\nSimple memory for storing context or other information that shouldn\\'t ever change between prompts.\\n\\nmemory.summary.ConversationSummaryMemory\\nConversation summarizer to chat memory.\\n\\nmemory.summary.SummarizerMixin\\nMixin for summarizer.\\n\\nmemory.summary_buffer.ConversationSummaryBufferMemory\\nBuffer with summarizer for storing conversation memory.\\n\\nmemory.token_buffer.ConversationTokenBufferMemory\\nConversation chat memory with token limit.\\n\\nmemory.vectorstore.VectorStoreRetrieverMemory\\nVectorStoreRetriever-backed memory.\\n\\nmemory.zep_memory.ZepMemory\\nPersist your chain history to the Zep MemoryStore.\\n\\nFunctions¶\\n\\nmemory.chat_message_histories.sql.create_message_model(...)\\nCreate a message model for a given table name.\\n\\nmemory.utils.get_prompt_input_key(inputs,\\xa0...)\\nGet the prompt input key.\\n\\nlangchain.model_laboratory¶\\nExperiment with different models.\\n\\nClasses¶\\n\\nmodel_laboratory.ModelLaboratory(chains[,\\xa0names])\\nExperiment with different models.\\n\\nlangchain.output_parsers¶\\nOutputParser classes parse the output of an LLM call.\\nClass hierarchy:\\nBaseLLMOutputParser --> BaseOutputParser --> <name>OutputParser  # ListOutputParser, PydanticOutputParser\\n\\nMain helpers:\\nSerializable, Generation, PromptValue\\n\\nClasses¶\\n\\noutput_parsers.boolean.BooleanOutputParser\\nParse the output of an LLM call to a boolean.\\n\\noutput_parsers.combining.CombiningOutputParser\\nCombine multiple output parsers into one.\\n\\noutput_parsers.datetime.DatetimeOutputParser\\nParse the output of an LLM call to a datetime.\\n\\noutput_parsers.enum.EnumOutputParser\\nParse an output that is one of a set of values.\\n\\noutput_parsers.ernie_functions.JsonKeyOutputFunctionsParser\\nParse an output as the element of the Json object.\\n\\noutput_parsers.ernie_functions.JsonOutputFunctionsParser\\nParse an output as the Json object.\\n\\noutput_parsers.ernie_functions.OutputFunctionsParser\\nParse an output that is one of sets of values.\\n\\noutput_parsers.ernie_functions.PydanticAttrOutputFunctionsParser\\nParse an output as an attribute of a pydantic object.\\n\\noutput_parsers.ernie_functions.PydanticOutputFunctionsParser\\nParse an output as a pydantic object.\\n\\noutput_parsers.fix.OutputFixingParser\\nWraps a parser and tries to fix parsing errors.\\n\\noutput_parsers.json.SimpleJsonOutputParser\\nParse the output of an LLM call to a JSON object.\\n\\noutput_parsers.openai_functions.JsonKeyOutputFunctionsParser\\nParse an output as the element of the Json object.\\n\\noutput_parsers.openai_functions.JsonOutputFunctionsParser\\nParse an output as the Json object.\\n\\noutput_parsers.openai_functions.OutputFunctionsParser\\nParse an output that is one of sets of values.\\n\\noutput_parsers.openai_functions.PydanticAttrOutputFunctionsParser\\nParse an output as an attribute of a pydantic object.\\n\\noutput_parsers.openai_functions.PydanticOutputFunctionsParser\\nParse an output as a pydantic object.\\n\\noutput_parsers.openai_tools.JsonOutputKeyToolsParser\\nParse tools from OpenAI response.\\n\\noutput_parsers.openai_tools.JsonOutputToolsParser\\nParse tools from OpenAI response.\\n\\noutput_parsers.openai_tools.PydanticToolsParser\\nParse tools from OpenAI response.\\n\\noutput_parsers.pydantic.PydanticOutputParser\\nParse an output using a pydantic model.\\n\\noutput_parsers.rail_parser.GuardrailsOutputParser\\nParse the output of an LLM call using Guardrails.\\n\\noutput_parsers.regex.RegexParser\\nParse the output of an LLM call using a regex.\\n\\noutput_parsers.regex_dict.RegexDictParser\\nParse the output of an LLM call into a Dictionary using a regex.\\n\\noutput_parsers.retry.RetryOutputParser\\nWraps a parser and tries to fix parsing errors.\\n\\noutput_parsers.retry.RetryWithErrorOutputParser\\nWraps a parser and tries to fix parsing errors.\\n\\noutput_parsers.structured.ResponseSchema\\nA schema for a response from a structured output parser.\\n\\noutput_parsers.structured.StructuredOutputParser\\nParse the output of an LLM call to a structured output.\\n\\noutput_parsers.xml.XMLOutputParser\\nParse an output using xml format.\\n\\nFunctions¶\\n\\noutput_parsers.json.parse_and_check_json_markdown(...)\\nParse a JSON string from a Markdown string and check that it contains the expected keys.\\n\\noutput_parsers.json.parse_json_markdown(...)\\nParse a JSON string from a Markdown string.\\n\\noutput_parsers.json.parse_partial_json(s,\\xa0*)\\nParse a JSON string that may be missing closing braces.\\n\\noutput_parsers.loading.load_output_parser(config)\\nLoad an output parser.\\n\\nlangchain.prompts¶\\nPrompt is the input to the model.\\nPrompt is often constructed\\nfrom multiple components. Prompt classes and functions make constructing\\n\\nand working with prompts easy.\\n\\nClass hierarchy:\\nBasePromptTemplate --> PipelinePromptTemplate\\n                       StringPromptTemplate --> PromptTemplate\\n                                                FewShotPromptTemplate\\n                                                FewShotPromptWithTemplates\\n                       BaseChatPromptTemplate --> AutoGPTPrompt\\n                                                  ChatPromptTemplate --> AgentScratchPadChatPromptTemplate\\n\\nBaseMessagePromptTemplate --> MessagesPlaceholder\\n                              BaseStringMessagePromptTemplate --> ChatMessagePromptTemplate\\n                                                                  HumanMessagePromptTemplate\\n                                                                  AIMessagePromptTemplate\\n                                                                  SystemMessagePromptTemplate\\n\\nPromptValue --> StringPromptValue\\n                ChatPromptValue\\n\\nClasses¶\\n\\nprompts.example_selector.ngram_overlap.NGramOverlapExampleSelector\\nSelect and order examples based on ngram overlap score (sentence_bleu score).\\n\\nFunctions¶\\n\\nprompts.example_selector.ngram_overlap.ngram_overlap_score(...)\\nCompute ngram overlap score of source and example as sentence_bleu score.\\n\\nlangchain.retrievers¶\\nRetriever class returns Documents given a text query.\\nIt is more general than a vector store. A retriever does not need to be able to\\nstore documents, only to return (or retrieve) it. Vector stores can be used as\\nthe backbone of a retriever, but there are other types of retrievers as well.\\nClass hierarchy:\\nBaseRetriever --> <name>Retriever  # Examples: ArxivRetriever, MergerRetriever\\n\\nMain helpers:\\nDocument, Serializable, Callbacks,\\nCallbackManagerForRetrieverRun, AsyncCallbackManagerForRetrieverRun\\n\\nClasses¶\\n\\nretrievers.arcee.ArceeRetriever\\nDocument retriever for Arcee\\'s Domain Adapted Language Models (DALMs).\\n\\nretrievers.arxiv.ArxivRetriever\\nArxiv retriever.\\n\\nretrievers.azure_cognitive_search.AzureCognitiveSearchRetriever\\nAzure Cognitive Search service retriever.\\n\\nretrievers.bm25.BM25Retriever\\nBM25 retriever without Elasticsearch.\\n\\nretrievers.chaindesk.ChaindeskRetriever\\nChaindesk API retriever.\\n\\nretrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever\\nChatGPT plugin retriever.\\n\\nretrievers.cohere_rag_retriever.CohereRagRetriever\\nCohere Chat API with RAG.\\n\\nretrievers.contextual_compression.ContextualCompressionRetriever\\nRetriever that wraps a base retriever and compresses the results.\\n\\nretrievers.databerry.DataberryRetriever\\nDataberry API retriever.\\n\\nretrievers.docarray.DocArrayRetriever\\nDocArray Document Indices retriever.\\n\\nretrievers.docarray.SearchType(value[,\\xa0...])\\nEnumerator of the types of search to perform.\\n\\nretrievers.document_compressors.base.BaseDocumentCompressor\\nBase class for document compressors.\\n\\nretrievers.document_compressors.base.DocumentCompressorPipeline\\nDocument compressor that uses a pipeline of Transformers.\\n\\nretrievers.document_compressors.chain_extract.LLMChainExtractor\\nDocument compressor that uses an LLM chain to extract the relevant parts of documents.\\n\\nretrievers.document_compressors.chain_extract.NoOutputParser\\nParse outputs that could return a null string of some sort.\\n\\nretrievers.document_compressors.chain_filter.LLMChainFilter\\nFilter that drops documents that aren\\'t relevant to the query.\\n\\nretrievers.document_compressors.cohere_rerank.CohereRerank\\nDocument compressor that uses Cohere Rerank API.\\n\\nretrievers.document_compressors.embeddings_filter.EmbeddingsFilter\\nDocument compressor that uses embeddings to drop documents unrelated to the query.\\n\\nretrievers.elastic_search_bm25.ElasticSearchBM25Retriever\\nElasticsearch retriever that uses BM25.\\n\\nretrievers.embedchain.EmbedchainRetriever\\nEmbedchain retriever.\\n\\nretrievers.ensemble.EnsembleRetriever\\nRetriever that ensembles the multiple retrievers.\\n\\nretrievers.google_cloud_documentai_warehouse.GoogleDocumentAIWarehouseRetriever\\nA retriever based on Document AI Warehouse.\\n\\nretrievers.google_vertex_ai_search.GoogleCloudEnterpriseSearchRetriever\\nGoogle Vertex Search API retriever alias for backwards compatibility.\\n\\nretrievers.google_vertex_ai_search.GoogleVertexAIMultiTurnSearchRetriever\\nGoogle Vertex AI Search retriever for multi-turn conversations.\\n\\nretrievers.google_vertex_ai_search.GoogleVertexAISearchRetriever\\nGoogle Vertex AI Search retriever.\\n\\nretrievers.kay.KayAiRetriever\\nRetriever for Kay.ai datasets.\\n\\nretrievers.kendra.AdditionalResultAttribute\\nAdditional result attribute.\\n\\nretrievers.kendra.AdditionalResultAttributeValue\\nValue of an additional result attribute.\\n\\nretrievers.kendra.AmazonKendraRetriever\\nAmazon Kendra Index retriever.\\n\\nretrievers.kendra.DocumentAttribute\\nDocument attribute.\\n\\nretrievers.kendra.DocumentAttributeValue\\nValue of a document attribute.\\n\\nretrievers.kendra.Highlight\\nInformation that highlights the keywords in the excerpt.\\n\\nretrievers.kendra.QueryResult\\nAmazon Kendra Query API search result.\\n\\nretrievers.kendra.QueryResultItem\\nQuery API result item.\\n\\nretrievers.kendra.ResultItem\\nBase class of a result item.\\n\\nretrievers.kendra.RetrieveResult\\nAmazon Kendra Retrieve API search result.\\n\\nretrievers.kendra.RetrieveResultItem\\nRetrieve API result item.\\n\\nretrievers.kendra.TextWithHighLights\\nText with highlights.\\n\\nretrievers.knn.KNNRetriever\\nKNN retriever.\\n\\nretrievers.llama_index.LlamaIndexGraphRetriever\\nLlamaIndex graph data structure retriever.\\n\\nretrievers.llama_index.LlamaIndexRetriever\\nLlamaIndex retriever.\\n\\nretrievers.merger_retriever.MergerRetriever\\nRetriever that merges the results of multiple retrievers.\\n\\nretrievers.metal.MetalRetriever\\nMetal API retriever.\\n\\nretrievers.milvus.MilvusRetriever\\nMilvus API retriever.\\n\\nretrievers.multi_query.LineList\\nList of lines.\\n\\nretrievers.multi_query.LineListOutputParser\\nOutput parser for a list of lines.\\n\\nretrievers.multi_query.MultiQueryRetriever\\nGiven a query, use an LLM to write a set of queries.\\n\\nretrievers.multi_vector.MultiVectorRetriever\\nRetrieve from a set of multiple embeddings for the same document.\\n\\nretrievers.parent_document_retriever.ParentDocumentRetriever\\nRetrieve small chunks then retrieve their parent documents.\\n\\nretrievers.pinecone_hybrid_search.PineconeHybridSearchRetriever\\nPinecone Hybrid Search retriever.\\n\\nretrievers.pubmed.PubMedRetriever\\nPubMed API retriever.\\n\\nretrievers.re_phraser.RePhraseQueryRetriever\\nGiven a query, use an LLM to re-phrase it.\\n\\nretrievers.remote_retriever.RemoteLangChainRetriever\\nLangChain API retriever.\\n\\nretrievers.self_query.base.SelfQueryRetriever\\nRetriever that uses a vector store and an LLM to generate the vector store queries.\\n\\nretrievers.self_query.chroma.ChromaTranslator()\\nTranslate Chroma internal query language elements to valid filters.\\n\\nretrievers.self_query.dashvector.DashvectorTranslator()\\nLogic for converting internal query language elements to valid filters.\\n\\nretrievers.self_query.deeplake.DeepLakeTranslator()\\nTranslate DeepLake internal query language elements to valid filters.\\n\\nretrievers.self_query.elasticsearch.ElasticsearchTranslator()\\nTranslate Elasticsearch internal query language elements to valid filters.\\n\\nretrievers.self_query.milvus.MilvusTranslator()\\nTranslate Milvus internal query language elements to valid filters.\\n\\nretrievers.self_query.myscale.MyScaleTranslator([...])\\nTranslate MyScale internal query language elements to valid filters.\\n\\nretrievers.self_query.opensearch.OpenSearchTranslator()\\nTranslate OpenSearch internal query domain-specific language elements to valid filters.\\n\\nretrievers.self_query.pinecone.PineconeTranslator()\\nTranslate Pinecone internal query language elements to valid filters.\\n\\nretrievers.self_query.qdrant.QdrantTranslator(...)\\nTranslate Qdrant internal query language elements to valid filters.\\n\\nretrievers.self_query.redis.RedisTranslator(schema)\\nTranslate\\n\\nretrievers.self_query.supabase.SupabaseVectorTranslator()\\nTranslate Langchain filters to Supabase PostgREST filters.\\n\\nretrievers.self_query.timescalevector.TimescaleVectorTranslator()\\nTranslate the internal query language elements to valid filters.\\n\\nretrievers.self_query.vectara.VectaraTranslator()\\nTranslate Vectara internal query language elements to valid filters.\\n\\nretrievers.self_query.weaviate.WeaviateTranslator()\\nTranslate Weaviate internal query language elements to valid filters.\\n\\nretrievers.svm.SVMRetriever\\nSVM retriever.\\n\\nretrievers.tavily_search_api.SearchDepth(value)\\nSearch depth as enumerator.\\n\\nretrievers.tavily_search_api.TavilySearchAPIRetriever\\nTavily Search API retriever.\\n\\nretrievers.tfidf.TFIDFRetriever\\nTF-IDF retriever.\\n\\nretrievers.time_weighted_retriever.TimeWeightedVectorStoreRetriever\\nRetriever that combines embedding similarity with recency in retrieving values.\\n\\nretrievers.vespa_retriever.VespaRetriever\\nVespa retriever.\\n\\nretrievers.weaviate_hybrid_search.WeaviateHybridSearchRetriever\\nWeaviate hybrid search retriever.\\n\\nretrievers.web_research.LineList\\nList of questions.\\n\\nretrievers.web_research.QuestionListOutputParser\\nOutput parser for a list of numbered questions.\\n\\nretrievers.web_research.SearchQueries\\nSearch queries to research for the user\\'s goal.\\n\\nretrievers.web_research.WebResearchRetriever\\nGoogle Search API retriever.\\n\\nretrievers.wikipedia.WikipediaRetriever\\nWikipedia API retriever.\\n\\nretrievers.you.YouRetriever\\nYou retriever that uses You.com\\'s search API.\\n\\nretrievers.zep.SearchScope(value[,\\xa0names,\\xa0...])\\nWhich documents to search.\\n\\nretrievers.zep.SearchType(value[,\\xa0names,\\xa0...])\\nEnumerator of the types of search to perform.\\n\\nretrievers.zep.ZepRetriever\\nZep MemoryStore Retriever.\\n\\nretrievers.zilliz.ZillizRetriever\\nZilliz API retriever.\\n\\nFunctions¶\\n\\nretrievers.bm25.default_preprocessing_func(text)\\n\\nretrievers.document_compressors.chain_extract.default_get_input(...)\\nReturn the compression chain input.\\n\\nretrievers.document_compressors.chain_filter.default_get_input(...)\\nReturn the compression chain input.\\n\\nretrievers.kendra.clean_excerpt(excerpt)\\nClean an excerpt from Kendra.\\n\\nretrievers.kendra.combined_text(item)\\nCombine a ResultItem title and excerpt into a single string.\\n\\nretrievers.knn.create_index(contexts,\\xa0embeddings)\\nCreate an index of embeddings for a list of contexts.\\n\\nretrievers.milvus.MilvusRetreiver(*args,\\xa0...)\\nDeprecated MilvusRetreiver.\\n\\nretrievers.pinecone_hybrid_search.create_index(...)\\nCreate an index from a list of contexts.\\n\\nretrievers.pinecone_hybrid_search.hash_text(text)\\nHash a text using SHA256.\\n\\nretrievers.self_query.deeplake.can_cast_to_float(string)\\nCheck if a string can be cast to a float.\\n\\nretrievers.self_query.milvus.process_value(value)\\nConvert a value to a string and add double quotes if it is a string.\\n\\nretrievers.self_query.vectara.process_value(value)\\nConvert a value to a string and add single quotes if it is a string.\\n\\nretrievers.svm.create_index(contexts,\\xa0embeddings)\\nCreate an index of embeddings for a list of contexts.\\n\\nretrievers.zilliz.ZillizRetreiver(*args,\\xa0...)\\nDeprecated ZillizRetreiver.\\n\\nlangchain.runnables¶\\n\\nClasses¶\\n\\nrunnables.hub.HubRunnable\\nAn instance of a runnable stored in the LangChain Hub.\\n\\nrunnables.openai_functions.OpenAIFunction\\nA function description for ChatOpenAI\\n\\nrunnables.openai_functions.OpenAIFunctionsRouter\\nA runnable that routes to the selected function.\\n\\nlangchain.smith¶\\nLangSmith utilities.\\nThis module provides utilities for connecting to LangSmith. For more information on LangSmith, see the LangSmith documentation.\\nEvaluation\\nLangSmith helps you evaluate Chains and other language model application components using a number of LangChain evaluators.\\nAn example of this is shown below, assuming you’ve created a LangSmith dataset called <my_dataset_name>:\\nfrom langsmith import Client\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.chains import LLMChain\\nfrom langchain.smith import RunEvalConfig, run_on_dataset\\n\\n# Chains may have memory. Passing in a constructor function lets the\\n# evaluation framework avoid cross-contamination between runs.\\ndef construct_chain():\\n    llm = ChatOpenAI(temperature=0)\\n    chain = LLMChain.from_string(\\n        llm,\\n        \"What\\'s the answer to {your_input_key}\"\\n    )\\n    return chain\\n\\n# Load off-the-shelf evaluators via config or the EvaluatorType (string or enum)\\nevaluation_config = RunEvalConfig(\\n    evaluators=[\\n        \"qa\",  # \"Correctness\" against a reference answer\\n        \"embedding_distance\",\\n        RunEvalConfig.Criteria(\"helpfulness\"),\\n        RunEvalConfig.Criteria({\\n            \"fifth-grader-score\": \"Do you have to be smarter than a fifth grader to answer this question?\"\\n        }),\\n    ]\\n)\\n\\nclient = Client()\\nrun_on_dataset(\\n    client,\\n    \"<my_dataset_name>\",\\n    construct_chain,\\n    evaluation=evaluation_config,\\n)\\n\\nYou can also create custom evaluators by subclassing the\\nStringEvaluator\\nor LangSmith’s RunEvaluator classes.\\nfrom typing import Optional\\nfrom langchain.evaluation import StringEvaluator\\n\\nclass MyStringEvaluator(StringEvaluator):\\n\\n    @property\\n    def requires_input(self) -> bool:\\n        return False\\n\\n    @property\\n    def requires_reference(self) -> bool:\\n        return True\\n\\n    @property\\n    def evaluation_name(self) -> str:\\n        return \"exact_match\"\\n\\n    def _evaluate_strings(self, prediction, reference=None, input=None, **kwargs) -> dict:\\n        return {\"score\": prediction == reference}\\n\\nevaluation_config = RunEvalConfig(\\n    custom_evaluators = [MyStringEvaluator()],\\n)\\n\\nrun_on_dataset(\\n    client,\\n    \"<my_dataset_name>\",\\n    construct_chain,\\n    evaluation=evaluation_config,\\n)\\n\\nPrimary Functions\\n\\narun_on_dataset: Asynchronous function to evaluate a chain, agent, or other LangChain component over a dataset.\\nrun_on_dataset: Function to evaluate a chain, agent, or other LangChain component over a dataset.\\nRunEvalConfig: Class representing the configuration for running evaluation. You can select evaluators by EvaluatorType or config, or you can pass in custom_evaluators\\n\\nClasses¶\\n\\nsmith.evaluation.config.EvalConfig\\nConfiguration for a given run evaluator.\\n\\nsmith.evaluation.config.RunEvalConfig\\nConfiguration for a run evaluation.\\n\\nsmith.evaluation.config.SingleKeyEvalConfig\\nCreate a new model by parsing and validating input data from keyword arguments.\\n\\nsmith.evaluation.progress.ProgressBarCallback(total)\\nA simple progress bar for the console.\\n\\nsmith.evaluation.runner_utils.EvalError(...)\\nYour architecture raised an error.\\n\\nsmith.evaluation.runner_utils.InputFormatError\\nRaised when the input format is invalid.\\n\\nsmith.evaluation.runner_utils.TestResult\\nA dictionary of the results of a single test run.\\n\\nsmith.evaluation.string_run_evaluator.ChainStringRunMapper\\nExtract items to evaluate from the run object from a chain.\\n\\nsmith.evaluation.string_run_evaluator.LLMStringRunMapper\\nExtract items to evaluate from the run object.\\n\\nsmith.evaluation.string_run_evaluator.StringExampleMapper\\nMap an example, or row in the dataset, to the inputs of an evaluation.\\n\\nsmith.evaluation.string_run_evaluator.StringRunEvaluatorChain\\nEvaluate Run and optional examples.\\n\\nsmith.evaluation.string_run_evaluator.StringRunMapper\\nExtract items to evaluate from the run object.\\n\\nsmith.evaluation.string_run_evaluator.ToolStringRunMapper\\nMap an input to the tool.\\n\\nFunctions¶\\n\\nsmith.evaluation.name_generation.random_name()\\nGenerate a random name.\\n\\nsmith.evaluation.runner_utils.arun_on_dataset(...)\\nRun the Chain or language model on a dataset and store traces to the specified project name.\\n\\nsmith.evaluation.runner_utils.run_on_dataset(...)\\nRun the Chain or language model on a dataset and store traces to the specified project name.\\n\\nlangchain.storage¶\\nImplementations of key-value stores and storage helpers.\\nModule provides implementations of various key-value stores that conform\\nto a simple key-value interface.\\nThe primary goal of these storages is to support implementation of caching.\\n\\nClasses¶\\n\\nstorage.encoder_backed.EncoderBackedStore(...)\\nWraps a store with key and value encoders/decoders.\\n\\nstorage.exceptions.InvalidKeyException\\nRaised when a key is invalid; e.g., uses incorrect characters.\\n\\nstorage.file_system.LocalFileStore(root_path)\\nBaseStore interface that works on the local file system.\\n\\nstorage.in_memory.InMemoryStore()\\nIn-memory implementation of the BaseStore using a dictionary.\\n\\nstorage.redis.RedisStore(*[,\\xa0client,\\xa0...])\\nBaseStore implementation using Redis as the underlying store.\\n\\nstorage.upstash_redis.UpstashRedisStore(*[,\\xa0...])\\nBaseStore implementation using Upstash Redis as the underlying store.\\n\\nlangchain.text_splitter¶\\nText Splitters are classes for splitting text.\\nClass hierarchy:\\nBaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\\n                                             RecursiveCharacterTextSplitter -->  <name>TextSplitter\\n\\nNote: MarkdownHeaderTextSplitter and **HTMLHeaderTextSplitter do not derive from TextSplitter.\\nMain helpers:\\nDocument, Tokenizer, Language, LineType, HeaderType\\n\\nClasses¶\\n\\ntext_splitter.CharacterTextSplitter([...])\\nSplitting text that looks at characters.\\n\\ntext_splitter.ElementType\\nElement type as typed dict.\\n\\ntext_splitter.HTMLHeaderTextSplitter(...[,\\xa0...])\\nSplitting HTML files based on specified headers.\\n\\ntext_splitter.HeaderType\\nHeader type as typed dict.\\n\\ntext_splitter.Language(value[,\\xa0names,\\xa0...])\\nEnum of the programming languages.\\n\\ntext_splitter.LatexTextSplitter(**kwargs)\\nAttempts to split the text along Latex-formatted layout elements.\\n\\ntext_splitter.LineType\\nLine type as typed dict.\\n\\ntext_splitter.MarkdownHeaderTextSplitter(...)\\nSplitting markdown files based on specified headers.\\n\\ntext_splitter.MarkdownTextSplitter(**kwargs)\\nAttempts to split the text along Markdown-formatted headings.\\n\\ntext_splitter.NLTKTextSplitter([separator,\\xa0...])\\nSplitting text using NLTK package.\\n\\ntext_splitter.PythonCodeTextSplitter(**kwargs)\\nAttempts to split the text along Python syntax.\\n\\ntext_splitter.RecursiveCharacterTextSplitter([...])\\nSplitting text by recursively look at characters.\\n\\ntext_splitter.SentenceTransformersTokenTextSplitter([...])\\nSplitting text to tokens using sentence model tokenizer.\\n\\ntext_splitter.SpacyTextSplitter([separator,\\xa0...])\\nSplitting text using Spacy package.\\n\\ntext_splitter.TextSplitter(chunk_size,\\xa0...)\\nInterface for splitting text into chunks.\\n\\ntext_splitter.TokenTextSplitter([...])\\nSplitting text to tokens using model tokenizer.\\n\\ntext_splitter.Tokenizer(chunk_overlap,\\xa0...)\\nTokenizer data class.\\n\\nFunctions¶\\n\\ntext_splitter.split_text_on_tokens(*,\\xa0text,\\xa0...)\\nSplit incoming text and return chunks using tokenizer.\\n\\nlangchain.tools¶\\nTools are classes that an Agent uses to interact with the world.\\nEach tool has a description. Agent uses the description to choose the right\\ntool for the job.\\nClass hierarchy:\\nToolMetaclass --> BaseTool --> <name>Tool  # Examples: AIPluginTool, BaseGraphQLTool\\n                               <name>      # Examples: BraveSearch, HumanInputRun\\n\\nMain helpers:\\nCallbackManagerForToolRun, AsyncCallbackManagerForToolRun\\n\\nClasses¶\\n\\ntools.ainetwork.app.AINAppOps\\nTool for app operations.\\n\\ntools.ainetwork.app.AppOperationType(value)\\nType of app operation as enumerator.\\n\\ntools.ainetwork.app.AppSchema\\nSchema for app operations.\\n\\ntools.ainetwork.base.AINBaseTool\\nBase class for the AINetwork tools.\\n\\ntools.ainetwork.base.OperationType(value[,\\xa0...])\\nType of operation as enumerator.\\n\\ntools.ainetwork.owner.AINOwnerOps\\nTool for owner operations.\\n\\ntools.ainetwork.owner.RuleSchema\\nSchema for owner operations.\\n\\ntools.ainetwork.rule.AINRuleOps\\nTool for owner operations.\\n\\ntools.ainetwork.rule.RuleSchema\\nSchema for owner operations.\\n\\ntools.ainetwork.transfer.AINTransfer\\nTool for transfer operations.\\n\\ntools.ainetwork.transfer.TransferSchema\\nSchema for transfer operations.\\n\\ntools.ainetwork.value.AINValueOps\\nTool for value operations.\\n\\ntools.ainetwork.value.ValueSchema\\nSchema for value operations.\\n\\ntools.amadeus.base.AmadeusBaseTool\\nBase Tool for Amadeus.\\n\\ntools.amadeus.closest_airport.AmadeusClosestAirport\\nTool for finding the closest airport to a particular location.\\n\\ntools.amadeus.closest_airport.ClosestAirportSchema\\nSchema for the AmadeusClosestAirport tool.\\n\\ntools.amadeus.flight_search.AmadeusFlightSearch\\nTool for searching for a single flight between two airports.\\n\\ntools.amadeus.flight_search.FlightSearchSchema\\nSchema for the AmadeusFlightSearch tool.\\n\\ntools.arxiv.tool.ArxivInput\\nCreate a new model by parsing and validating input data from keyword arguments.\\n\\ntools.arxiv.tool.ArxivQueryRun\\nTool that searches the Arxiv API.\\n\\ntools.azure_cognitive_services.form_recognizer.AzureCogsFormRecognizerTool\\nTool that queries the Azure Cognitive Services Form Recognizer API.\\n\\ntools.azure_cognitive_services.image_analysis.AzureCogsImageAnalysisTool\\nTool that queries the Azure Cognitive Services Image Analysis API.\\n\\ntools.azure_cognitive_services.speech2text.AzureCogsSpeech2TextTool\\nTool that queries the Azure Cognitive Services Speech2Text API.\\n\\ntools.azure_cognitive_services.text2speech.AzureCogsText2SpeechTool\\nTool that queries the Azure Cognitive Services Text2Speech API.\\n\\ntools.azure_cognitive_services.text_analytics_health.AzureCogsTextAnalyticsHealthTool\\nTool that queries the Azure Cognitive Services Text Analytics for Health API.\\n\\ntools.bearly.tool.BearlyInterpreterTool(api_key)\\nTool for evaluating python code in a sandbox environment.\\n\\ntools.bearly.tool.BearlyInterpreterToolArguments\\nArguments for the BearlyInterpreterTool.\\n\\ntools.bearly.tool.FileInfo\\nInformation about a file to be uploaded.\\n\\ntools.bing_search.tool.BingSearchResults\\nTool that queries the Bing Search API and gets back json.\\n\\ntools.bing_search.tool.BingSearchRun\\nTool that queries the Bing search API.\\n\\ntools.brave_search.tool.BraveSearch\\nTool that queries the BraveSearch.\\n\\ntools.clickup.tool.ClickupAction\\nTool that queries the  Clickup API.\\n\\ntools.dataforseo_api_search.tool.DataForSeoAPISearchResults\\nTool that queries the DataForSeo Google Search API and get back json.\\n\\ntools.dataforseo_api_search.tool.DataForSeoAPISearchRun\\nTool that queries the DataForSeo Google search API.\\n\\ntools.ddg_search.tool.DDGInput\\nCreate a new model by parsing and validating input data from keyword arguments.\\n\\ntools.ddg_search.tool.DuckDuckGoSearchResults\\nTool that queries the DuckDuckGo search API and gets back json.\\n\\ntools.ddg_search.tool.DuckDuckGoSearchRun\\nTool that queries the DuckDuckGo search API.\\n\\ntools.e2b_data_analysis.tool.E2BDataAnalysisTool\\nTool for running python code in a sandboxed environment for data analysis.\\n\\ntools.e2b_data_analysis.tool.E2BDataAnalysisToolArguments\\nArguments for the E2BDataAnalysisTool.\\n\\ntools.e2b_data_analysis.tool.UploadedFile\\nDescription of the uploaded path with its remote path.\\n\\ntools.e2b_data_analysis.unparse.Unparser(tree)\\nMethods in this class recursively traverse an AST and output source code for the abstract syntax; original formatting is disregarded.\\n\\ntools.edenai.audio_speech_to_text.EdenAiSpeechToTextTool\\nTool that queries the Eden AI Speech To Text API.\\n\\ntools.edenai.audio_text_to_speech.EdenAiTextToSpeechTool\\nTool that queries the Eden AI Text to speech API.\\n\\ntools.edenai.edenai_base_tool.EdenaiTool\\nthe base tool for all the EdenAI Tools .\\n\\ntools.edenai.image_explicitcontent.EdenAiExplicitImageTool\\nTool that queries the Eden AI Explicit image detection.\\n\\ntools.edenai.image_objectdetection.EdenAiObjectDetectionTool\\nTool that queries the Eden AI Object detection API.\\n\\ntools.edenai.ocr_identityparser.EdenAiParsingIDTool\\nTool that queries the Eden AI  Identity parsing API.\\n\\ntools.edenai.ocr_invoiceparser.EdenAiParsingInvoiceTool\\nTool that queries the Eden AI Invoice parsing API.\\n\\ntools.edenai.text_moderation.EdenAiTextModerationTool\\nTool that queries the Eden AI Explicit text detection.\\n\\ntools.eleven_labs.models.ElevenLabsModel(value)\\nModels available for Eleven Labs Text2Speech.\\n\\ntools.eleven_labs.text2speech.ElevenLabsModel(value)\\nModels available for Eleven Labs Text2Speech.\\n\\ntools.eleven_labs.text2speech.ElevenLabsText2SpeechTool\\nTool that queries the Eleven Labs Text2Speech API.\\n\\ntools.file_management.copy.CopyFileTool\\nTool that copies a file.\\n\\ntools.file_management.copy.FileCopyInput\\nInput for CopyFileTool.\\n\\ntools.file_management.delete.DeleteFileTool\\nTool that deletes a file.\\n\\ntools.file_management.delete.FileDeleteInput\\nInput for DeleteFileTool.\\n\\ntools.file_management.file_search.FileSearchInput\\nInput for FileSearchTool.\\n\\ntools.file_management.file_search.FileSearchTool\\nTool that searches for files in a subdirectory that match a regex pattern.\\n\\ntools.file_management.list_dir.DirectoryListingInput\\nInput for ListDirectoryTool.\\n\\ntools.file_management.list_dir.ListDirectoryTool\\nTool that lists files and directories in a specified folder.\\n\\ntools.file_management.move.FileMoveInput\\nInput for MoveFileTool.\\n\\ntools.file_management.move.MoveFileTool\\nTool that moves a file.\\n\\ntools.file_management.read.ReadFileInput\\nInput for ReadFileTool.\\n\\ntools.file_management.read.ReadFileTool\\nTool that reads a file.\\n\\ntools.file_management.utils.BaseFileToolMixin\\nMixin for file system tools.\\n\\ntools.file_management.utils.FileValidationError\\nError for paths outside the root directory.\\n\\ntools.file_management.write.WriteFileInput\\nInput for WriteFileTool.\\n\\ntools.file_management.write.WriteFileTool\\nTool that writes a file to disk.\\n\\ntools.github.tool.GitHubAction\\nTool for interacting with the GitHub API.\\n\\ntools.gitlab.tool.GitLabAction\\nTool for interacting with the GitLab API.\\n\\ntools.gmail.base.GmailBaseTool\\nBase class for Gmail tools.\\n\\ntools.gmail.create_draft.CreateDraftSchema\\nInput for CreateDraftTool.\\n\\ntools.gmail.create_draft.GmailCreateDraft\\nTool that creates a draft email for Gmail.\\n\\ntools.gmail.get_message.GmailGetMessage\\nTool that gets a message by ID from Gmail.\\n\\ntools.gmail.get_message.SearchArgsSchema\\nInput for GetMessageTool.\\n\\ntools.gmail.get_thread.GetThreadSchema\\nInput for GetMessageTool.\\n\\ntools.gmail.get_thread.GmailGetThread\\nTool that gets a thread by ID from Gmail.\\n\\ntools.gmail.search.GmailSearch\\nTool that searches for messages or threads in Gmail.\\n\\ntools.gmail.search.Resource(value[,\\xa0names,\\xa0...])\\nEnumerator of Resources to search.\\n\\ntools.gmail.search.SearchArgsSchema\\nInput for SearchGmailTool.\\n\\ntools.gmail.send_message.GmailSendMessage\\nTool that sends a message to Gmail.\\n\\ntools.gmail.send_message.SendMessageSchema\\nInput for SendMessageTool.\\n\\ntools.golden_query.tool.GoldenQueryRun\\nTool that adds the capability to query using the Golden API and get back JSON.\\n\\ntools.google_cloud.texttospeech.GoogleCloudTextToSpeechTool\\nTool that queries the Google Cloud Text to Speech API.\\n\\ntools.google_places.tool.GooglePlacesSchema\\nInput for GooglePlacesTool.\\n\\ntools.google_places.tool.GooglePlacesTool\\nTool that queries the Google places API.\\n\\ntools.google_scholar.tool.GoogleScholarQueryRun\\nTool that queries the Google search API.\\n\\ntools.google_search.tool.GoogleSearchResults\\nTool that queries the Google Search API and gets back json.\\n\\ntools.google_search.tool.GoogleSearchRun\\nTool that queries the Google search API.\\n\\ntools.google_serper.tool.GoogleSerperResults\\nTool that queries the Serper.dev Google Search API and get back json.\\n\\ntools.google_serper.tool.GoogleSerperRun\\nTool that queries the Serper.dev Google search API.\\n\\ntools.graphql.tool.BaseGraphQLTool\\nBase tool for querying a GraphQL API.\\n\\ntools.human.tool.HumanInputRun\\nTool that asks user for input.\\n\\ntools.ifttt.IFTTTWebhook\\nIFTTT Webhook.\\n\\ntools.jira.tool.JiraAction\\nTool that queries the Atlassian Jira API.\\n\\ntools.json.tool.JsonGetValueTool\\nTool for getting a value in a JSON spec.\\n\\ntools.json.tool.JsonListKeysTool\\nTool for listing keys in a JSON spec.\\n\\ntools.json.tool.JsonSpec\\nBase class for JSON spec.\\n\\ntools.memorize.tool.Memorize\\nCreate a new model by parsing and validating input data from keyword arguments.\\n\\ntools.memorize.tool.TrainableLLM(*args,\\xa0**kwargs)\\n\\ntools.metaphor_search.tool.MetaphorSearchResults\\nTool that queries the Metaphor Search API and gets back json.\\n\\ntools.multion.close_session.CloseSessionSchema\\nInput for UpdateSessionTool.\\n\\ntools.multion.close_session.MultionCloseSession\\nTool that closes an existing Multion Browser Window with provided fields.\\n\\ntools.multion.create_session.CreateSessionSchema\\nInput for CreateSessionTool.\\n\\ntools.multion.create_session.MultionCreateSession\\nTool that creates a new Multion Browser Window with provided fields.\\n\\ntools.multion.update_session.MultionUpdateSession\\nTool that updates an existing Multion Browser Window with provided fields.\\n\\ntools.multion.update_session.UpdateSessionSchema\\nInput for UpdateSessionTool.\\n\\ntools.nuclia.tool.NUASchema\\nInput for Nuclia Understanding API.\\n\\ntools.nuclia.tool.NucliaUnderstandingAPI\\nTool to process files with the Nuclia Understanding API.\\n\\ntools.office365.base.O365BaseTool\\nBase class for the Office 365 tools.\\n\\ntools.office365.create_draft_message.CreateDraftMessageSchema\\nInput for SendMessageTool.\\n\\ntools.office365.create_draft_message.O365CreateDraftMessage\\nTool for creating a draft email in Office 365.\\n\\ntools.office365.events_search.O365SearchEvents\\nClass for searching calendar events in Office 365\\n\\ntools.office365.events_search.SearchEventsInput\\nInput for SearchEmails Tool.\\n\\ntools.office365.messages_search.O365SearchEmails\\nClass for searching email messages in Office 365\\n\\ntools.office365.messages_search.SearchEmailsInput\\nInput for SearchEmails Tool.\\n\\ntools.office365.send_event.O365SendEvent\\nTool for sending calendar events in Office 365.\\n\\ntools.office365.send_event.SendEventSchema\\nInput for CreateEvent Tool.\\n\\ntools.office365.send_message.O365SendMessage\\nTool for sending an email in Office 365.\\n\\ntools.office365.send_message.SendMessageSchema\\nInput for SendMessageTool.\\n\\ntools.openapi.utils.api_models.APIOperation\\nA model for a single API operation.\\n\\ntools.openapi.utils.api_models.APIProperty\\nA model for a property in the query, path, header, or cookie params.\\n\\ntools.openapi.utils.api_models.APIPropertyBase\\nBase model for an API property.\\n\\ntools.openapi.utils.api_models.APIPropertyLocation(value)\\nThe location of the property.\\n\\ntools.openapi.utils.api_models.APIRequestBody\\nA model for a request body.\\n\\ntools.openapi.utils.api_models.APIRequestBodyProperty\\nA model for a request body property.\\n\\ntools.openweathermap.tool.OpenWeatherMapQueryRun\\nTool that queries the OpenWeatherMap API.\\n\\ntools.playwright.base.BaseBrowserTool\\nBase class for browser tools.\\n\\ntools.playwright.click.ClickTool\\nTool for clicking on an element with the given CSS selector.\\n\\ntools.playwright.click.ClickToolInput\\nInput for ClickTool.\\n\\ntools.playwright.current_page.CurrentWebPageTool\\nTool for getting the URL of the current webpage.\\n\\ntools.playwright.extract_hyperlinks.ExtractHyperlinksTool\\nExtract all hyperlinks on the page.\\n\\ntools.playwright.extract_hyperlinks.ExtractHyperlinksToolInput\\nInput for ExtractHyperlinksTool.\\n\\ntools.playwright.extract_text.ExtractTextTool\\nTool for extracting all the text on the current webpage.\\n\\ntools.playwright.get_elements.GetElementsTool\\nTool for getting elements in the current web page matching a CSS selector.\\n\\ntools.playwright.get_elements.GetElementsToolInput\\nInput for GetElementsTool.\\n\\ntools.playwright.navigate.NavigateTool\\nTool for navigating a browser to a URL.\\n\\ntools.playwright.navigate.NavigateToolInput\\nInput for NavigateToolInput.\\n\\ntools.playwright.navigate_back.NavigateBackTool\\nNavigate back to the previous page in the browser history.\\n\\ntools.plugin.AIPlugin\\nAI Plugin Definition.\\n\\ntools.plugin.AIPluginTool\\nTool for getting the OpenAPI spec for an AI Plugin.\\n\\ntools.plugin.AIPluginToolSchema\\nSchema for AIPluginTool.\\n\\ntools.plugin.ApiConfig\\nAPI Configuration.\\n\\ntools.powerbi.tool.InfoPowerBITool\\nTool for getting metadata about a PowerBI Dataset.\\n\\ntools.powerbi.tool.ListPowerBITool\\nTool for getting tables names.\\n\\ntools.powerbi.tool.QueryPowerBITool\\nTool for querying a Power BI Dataset.\\n\\ntools.pubmed.tool.PubmedQueryRun\\nTool that searches the PubMed API.\\n\\ntools.requests.tool.BaseRequestsTool\\nBase class for requests tools.\\n\\ntools.requests.tool.RequestsDeleteTool\\nTool for making a DELETE request to an API endpoint.\\n\\ntools.requests.tool.RequestsGetTool\\nTool for making a GET request to an API endpoint.\\n\\ntools.requests.tool.RequestsPatchTool\\nTool for making a PATCH request to an API endpoint.\\n\\ntools.requests.tool.RequestsPostTool\\nTool for making a POST request to an API endpoint.\\n\\ntools.requests.tool.RequestsPutTool\\nTool for making a PUT request to an API endpoint.\\n\\ntools.retriever.RetrieverInput\\nCreate a new model by parsing and validating input data from keyword arguments.\\n\\ntools.scenexplain.tool.SceneXplainInput\\nInput for SceneXplain.\\n\\ntools.scenexplain.tool.SceneXplainTool\\nTool that explains images.\\n\\ntools.searchapi.tool.SearchAPIResults\\nTool that queries the SearchApi.io search API and returns JSON.\\n\\ntools.searchapi.tool.SearchAPIRun\\nTool that queries the SearchApi.io search API.\\n\\ntools.searx_search.tool.SearxSearchResults\\nTool that queries a Searx instance and gets back json.\\n\\ntools.searx_search.tool.SearxSearchRun\\nTool that queries a Searx instance.\\n\\ntools.shell.tool.ShellInput\\nCommands for the Bash Shell tool.\\n\\ntools.shell.tool.ShellTool\\nTool to run shell commands.\\n\\ntools.sleep.tool.SleepInput\\nInput for CopyFileTool.\\n\\ntools.sleep.tool.SleepTool\\nTool that adds the capability to sleep.\\n\\ntools.spark_sql.tool.BaseSparkSQLTool\\nBase tool for interacting with Spark SQL.\\n\\ntools.spark_sql.tool.InfoSparkSQLTool\\nTool for getting metadata about a Spark SQL.\\n\\ntools.spark_sql.tool.ListSparkSQLTool\\nTool for getting tables names.\\n\\ntools.spark_sql.tool.QueryCheckerTool\\nUse an LLM to check if a query is correct.\\n\\ntools.spark_sql.tool.QuerySparkSQLTool\\nTool for querying a Spark SQL.\\n\\ntools.sql_database.tool.BaseSQLDatabaseTool\\nBase tool for interacting with a SQL database.\\n\\ntools.sql_database.tool.InfoSQLDatabaseTool\\nTool for getting metadata about a SQL database.\\n\\ntools.sql_database.tool.ListSQLDatabaseTool\\nTool for getting tables names.\\n\\ntools.sql_database.tool.QuerySQLCheckerTool\\nUse an LLM to check if a query is correct.\\n\\ntools.sql_database.tool.QuerySQLDataBaseTool\\nTool for querying a SQL database.\\n\\ntools.steamship_image_generation.tool.ModelName(value)\\nSupported Image Models for generation.\\n\\ntools.steamship_image_generation.tool.SteamshipImageGenerationTool\\nTool used to generate images from a text-prompt.\\n\\ntools.tavily_search.tool.TavilyAnswer\\nTool that queries the Tavily Search API and gets back an answer.\\n\\ntools.tavily_search.tool.TavilyInput\\nCreate a new model by parsing and validating input data from keyword arguments.\\n\\ntools.tavily_search.tool.TavilySearchResults\\nTool that queries the Tavily Search API and gets back json.\\n\\ntools.vectorstore.tool.BaseVectorStoreTool\\nBase class for tools that use a VectorStore.\\n\\ntools.vectorstore.tool.VectorStoreQATool\\nTool for the VectorDBQA chain.\\n\\ntools.vectorstore.tool.VectorStoreQAWithSourcesTool\\nTool for the VectorDBQAWithSources chain.\\n\\ntools.wikipedia.tool.WikipediaQueryRun\\nTool that searches the Wikipedia API.\\n\\ntools.wolfram_alpha.tool.WolframAlphaQueryRun\\nTool that queries using the Wolfram Alpha SDK.\\n\\ntools.yahoo_finance_news.YahooFinanceNewsTool\\nTool that searches financial news on Yahoo Finance.\\n\\ntools.youtube.search.YouTubeSearchTool\\nTool that queries YouTube.\\n\\ntools.zapier.tool.ZapierNLAListActions\\nReturns a list of all exposed (enabled) actions associated with\\n\\ntools.zapier.tool.ZapierNLARunAction\\nExecutes an action that is identified by action_id, must be exposed\\n\\nFunctions¶\\n\\ntools.ainetwork.utils.authenticate([network])\\nAuthenticate using the AIN Blockchain\\n\\ntools.amadeus.utils.authenticate()\\nAuthenticate using the Amadeus API\\n\\ntools.azure_cognitive_services.utils.detect_file_src_type(...)\\nDetect if the file is local or remote.\\n\\ntools.azure_cognitive_services.utils.download_audio_from_url(...)\\nDownload audio from url to local.\\n\\ntools.bearly.tool.file_to_base64(path)\\nConvert a file to base64.\\n\\ntools.bearly.tool.head_file(path,\\xa0n)\\nGet the first n lines of a file.\\n\\ntools.bearly.tool.strip_markdown_code(md_string)\\nStrip markdown code from a string.\\n\\ntools.ddg_search.tool.DuckDuckGoSearchTool(...)\\nDeprecated.\\n\\ntools.e2b_data_analysis.tool.add_last_line_print(code)\\nAdd print statement to the last line if it\\'s missing.\\n\\ntools.e2b_data_analysis.unparse.interleave(...)\\nCall f on each item in seq, calling inter() in between.\\n\\ntools.e2b_data_analysis.unparse.roundtrip(...)\\n\\ntools.file_management.utils.get_validated_relative_path(...)\\nResolve a relative path, raising an error if not within the root directory.\\n\\ntools.file_management.utils.is_relative_to(...)\\nCheck if path is relative to root.\\n\\ntools.gmail.utils.build_resource_service([...])\\nBuild a Gmail service.\\n\\ntools.gmail.utils.clean_email_body(body)\\nClean email body.\\n\\ntools.gmail.utils.get_gmail_credentials([...])\\nGet credentials.\\n\\ntools.gmail.utils.import_google()\\nImport google libraries.\\n\\ntools.gmail.utils.import_googleapiclient_resource_builder()\\nImport googleapiclient.discovery.build function.\\n\\ntools.gmail.utils.import_installed_app_flow()\\nImport InstalledAppFlow class.\\n\\ntools.interaction.tool.StdInInquireTool(...)\\nTool for asking the user for input.\\n\\ntools.office365.utils.authenticate()\\nAuthenticate using the Microsoft Grah API\\n\\ntools.office365.utils.clean_body(body)\\nClean body of a message or event.\\n\\ntools.playwright.base.lazy_import_playwright_browsers()\\nLazy import playwright browsers.\\n\\ntools.playwright.utils.aget_current_page(browser)\\nAsynchronously get the current page of the browser.\\n\\ntools.playwright.utils.create_async_playwright_browser([...])\\nCreate an async playwright browser.\\n\\ntools.playwright.utils.create_sync_playwright_browser([...])\\nCreate a playwright browser.\\n\\ntools.playwright.utils.get_current_page(browser)\\nGet the current page of the browser.\\n\\ntools.playwright.utils.run_async(coro)\\nRun an async coroutine.\\n\\ntools.plugin.marshal_spec(txt)\\nConvert the yaml or json serialized spec to a dict.\\n\\ntools.render.format_tool_to_openai_function(tool)\\nFormat tool into the OpenAI function API.\\n\\ntools.render.format_tool_to_openai_tool(tool)\\nFormat tool into the OpenAI function API.\\n\\ntools.render.render_text_description(tools)\\nRender the tool name and description in plain text.\\n\\ntools.render.render_text_description_and_args(tools)\\nRender the tool name, description, and args in plain text.\\n\\ntools.retriever.create_retriever_tool(...)\\nCreate a tool to do retrieval of documents.\\n\\ntools.steamship_image_generation.utils.make_image_public(...)\\nUpload a block to a signed URL and return the public URL.\\n\\nlangchain.tools.render¶\\nDifferent methods for rendering Tools to be passed to LLMs.\\nDepending on the LLM you are using and the prompting strategy you are using,\\nyou may want Tools to be rendered in a different way.\\nThis module contains various ways to render tools.\\n\\nFunctions¶\\n\\ntools.render.format_tool_to_openai_function(tool)\\nFormat tool into the OpenAI function API.\\n\\ntools.render.format_tool_to_openai_tool(tool)\\nFormat tool into the OpenAI function API.\\n\\ntools.render.render_text_description(tools)\\nRender the tool name and description in plain text.\\n\\ntools.render.render_text_description_and_args(tools)\\nRender the tool name, description, and args in plain text.\\n\\nlangchain.utilities¶\\nUtilities are the integrations with third-part systems and packages.\\nOther LangChain classes use Utilities to interact with third-part systems\\nand packages.\\n\\nClasses¶\\n\\nutilities.alpha_vantage.AlphaVantageAPIWrapper\\nWrapper for AlphaVantage API for Currency Exchange Rate.\\n\\nutilities.apify.ApifyWrapper\\nWrapper around Apify.\\n\\nutilities.arcee.ArceeDocument\\nArcee document.\\n\\nutilities.arcee.ArceeDocumentAdapter()\\nAdapter for Arcee documents\\n\\nutilities.arcee.ArceeDocumentSource\\nSource of an Arcee document.\\n\\nutilities.arcee.ArceeRoute(value[,\\xa0names,\\xa0...])\\nRoutes available for the Arcee API as enumerator.\\n\\nutilities.arcee.ArceeWrapper(arcee_api_key,\\xa0...)\\nWrapper for Arcee API.\\n\\nutilities.arcee.DALMFilter\\nFilters available for a DALM retrieval and generation.\\n\\nutilities.arcee.DALMFilterType(value[,\\xa0...])\\nFilter types available for a DALM retrieval as enumerator.\\n\\nutilities.arxiv.ArxivAPIWrapper\\nWrapper around ArxivAPI.\\n\\nutilities.awslambda.LambdaWrapper\\nWrapper for AWS Lambda SDK.\\n\\nutilities.bibtex.BibtexparserWrapper\\nWrapper around bibtexparser.\\n\\nutilities.bing_search.BingSearchAPIWrapper\\nWrapper for Bing Search API.\\n\\nutilities.brave_search.BraveSearchWrapper\\nWrapper around the Brave search engine.\\n\\nutilities.clickup.CUList(folder_id,\\xa0name[,\\xa0...])\\nComponent class for a list.\\n\\nutilities.clickup.ClickupAPIWrapper\\nWrapper for Clickup API.\\n\\nutilities.clickup.Component()\\nBase class for all components.\\n\\nutilities.clickup.Member(id,\\xa0username,\\xa0...)\\nComponent class for a member.\\n\\nutilities.clickup.Space(id,\\xa0name,\\xa0private,\\xa0...)\\nComponent class for a space.\\n\\nutilities.clickup.Task(id,\\xa0name,\\xa0...)\\nClass for a task.\\n\\nutilities.clickup.Team(id,\\xa0name,\\xa0members)\\nComponent class for a team.\\n\\nutilities.dalle_image_generator.DallEAPIWrapper\\nWrapper for OpenAI\\'s DALL-E Image Generator.\\n\\nutilities.dataforseo_api_search.DataForSeoAPIWrapper\\nWrapper around the DataForSeo API.\\n\\nutilities.duckduckgo_search.DuckDuckGoSearchAPIWrapper\\nWrapper for DuckDuckGo Search API.\\n\\nutilities.github.GitHubAPIWrapper\\nWrapper for GitHub API.\\n\\nutilities.gitlab.GitLabAPIWrapper\\nWrapper for GitLab API.\\n\\nutilities.golden_query.GoldenQueryAPIWrapper\\nWrapper for Golden.\\n\\nutilities.google_places_api.GooglePlacesAPIWrapper\\nWrapper around Google Places API.\\n\\nutilities.google_scholar.GoogleScholarAPIWrapper\\nWrapper for Google Scholar API\\n\\nutilities.google_search.GoogleSearchAPIWrapper\\nWrapper for Google Search API.\\n\\nutilities.google_serper.GoogleSerperAPIWrapper\\nWrapper around the Serper.dev Google Search API.\\n\\nutilities.graphql.GraphQLAPIWrapper\\nWrapper around GraphQL API.\\n\\nutilities.jira.JiraAPIWrapper\\nWrapper for Jira API.\\n\\nutilities.max_compute.MaxComputeAPIWrapper(client)\\nInterface for querying Alibaba Cloud MaxCompute tables.\\n\\nutilities.metaphor_search.MetaphorSearchAPIWrapper\\nWrapper for Metaphor Search API.\\n\\nutilities.openapi.HTTPVerb(value[,\\xa0names,\\xa0...])\\nEnumerator of the HTTP verbs.\\n\\nutilities.openapi.OpenAPISpec()\\nOpenAPI Model that removes mis-formatted parts of the spec.\\n\\nutilities.openweathermap.OpenWeatherMapAPIWrapper\\nWrapper for OpenWeatherMap API using PyOWM.\\n\\nutilities.portkey.Portkey()\\nPortkey configuration.\\n\\nutilities.powerbi.PowerBIDataset\\nCreate PowerBI engine from dataset ID and credential or token.\\n\\nutilities.pubmed.PubMedAPIWrapper\\nWrapper around PubMed API.\\n\\nutilities.python.PythonREPL\\nSimulates a standalone Python REPL.\\n\\nutilities.redis.TokenEscaper([escape_chars_re])\\nEscape punctuation within an input string.\\n\\nutilities.requests.Requests\\nWrapper around requests to handle auth and async.\\n\\nutilities.requests.RequestsWrapper\\nalias of TextRequestsWrapper\\n\\nutilities.requests.TextRequestsWrapper\\nLightweight wrapper around requests library.\\n\\nutilities.scenexplain.SceneXplainAPIWrapper\\nWrapper for SceneXplain API.\\n\\nutilities.searchapi.SearchApiAPIWrapper\\nWrapper around SearchApi API.\\n\\nutilities.searx_search.SearxResults(data)\\nDict like wrapper around search api results.\\n\\nutilities.searx_search.SearxSearchWrapper\\nWrapper for Searx API.\\n\\nutilities.serpapi.HiddenPrints()\\nContext manager to hide prints.\\n\\nutilities.serpapi.SerpAPIWrapper\\nWrapper around SerpAPI.\\n\\nutilities.spark_sql.SparkSQL([...])\\nSparkSQL is a utility class for interacting with Spark SQL.\\n\\nutilities.sql_database.SQLDatabase(engine[,\\xa0...])\\nSQLAlchemy wrapper around a database.\\n\\nutilities.tavily_search.TavilySearchAPIWrapper\\nWrapper for Tavily Search API.\\n\\nutilities.tensorflow_datasets.TensorflowDatasets\\nAccess to the TensorFlow Datasets.\\n\\nutilities.twilio.TwilioAPIWrapper\\nMessaging Client using Twilio.\\n\\nutilities.wikipedia.WikipediaAPIWrapper\\nWrapper around WikipediaAPI.\\n\\nutilities.wolfram_alpha.WolframAlphaAPIWrapper\\nWrapper for Wolfram Alpha.\\n\\nutilities.zapier.ZapierNLAWrapper\\nWrapper for Zapier NLA.\\n\\nFunctions¶\\n\\nutilities.anthropic.get_num_tokens_anthropic(text)\\nGet the number of tokens in a string of text.\\n\\nutilities.anthropic.get_token_ids_anthropic(text)\\nGet the token ids for a string of text.\\n\\nutilities.clickup.extract_dict_elements_from_component_fields(...)\\nExtract elements from a dictionary.\\n\\nutilities.clickup.fetch_data(url,\\xa0access_token)\\nFetch data from a URL.\\n\\nutilities.clickup.fetch_first_id(data,\\xa0key)\\nFetch the first id from a dictionary.\\n\\nutilities.clickup.fetch_folder_id(space_id,\\xa0...)\\nFetch the folder id.\\n\\nutilities.clickup.fetch_list_id(space_id,\\xa0...)\\nFetch the list id.\\n\\nutilities.clickup.fetch_space_id(team_id,\\xa0...)\\nFetch the space id.\\n\\nutilities.clickup.fetch_team_id(access_token)\\nFetch the team id.\\n\\nutilities.clickup.load_query(query[,\\xa0...])\\nAttempts to parse a JSON string and return the parsed object.\\n\\nutilities.clickup.parse_dict_through_component(...)\\nParse a dictionary by creating a component and then turning it back into a dictionary.\\n\\nutilities.opaqueprompts.desanitize(...)\\nRestore the original sensitive data from the sanitized text.\\n\\nutilities.opaqueprompts.sanitize(input)\\nSanitize input string or dict of strings by replacing sensitive data with placeholders.\\n\\nutilities.powerbi.fix_table_name(table)\\nAdd single quotes around table names that contain spaces.\\n\\nutilities.powerbi.json_to_md(json_contents)\\nConverts a JSON object to a markdown table.\\n\\nutilities.redis.check_redis_module_exist(...)\\nCheck if the correct Redis modules are installed.\\n\\nutilities.redis.get_client(redis_url,\\xa0**kwargs)\\nGet a redis client from the connection url given.\\n\\nutilities.sql_database.truncate_word(...[,\\xa0...])\\nTruncate a string to a certain number of words, based on the max string length.\\n\\nutilities.vertexai.get_client_info([module])\\nReturns a custom user agent header.\\n\\nutilities.vertexai.init_vertexai([project,\\xa0...])\\nInit vertexai.\\n\\nutilities.vertexai.raise_vertex_import_error([...])\\nRaise ImportError related to Vertex SDK being not available.\\n\\nlangchain.utils¶\\nUtility functions for LangChain.\\nThese functions do not depend on any other LangChain module.\\n\\nClasses¶\\n\\nutils.ernie_functions.FunctionDescription\\nRepresentation of a callable function to the Ernie API.\\n\\nutils.ernie_functions.ToolDescription\\nRepresentation of a callable function to the Ernie API.\\n\\nutils.openai_functions.FunctionDescription\\nRepresentation of a callable function to the OpenAI API.\\n\\nutils.openai_functions.ToolDescription\\nRepresentation of a callable function to the OpenAI API.\\n\\nFunctions¶\\n\\nutils.env.get_from_dict_or_env(data,\\xa0key,\\xa0...)\\nGet a value from a dictionary or an environment variable.\\n\\nutils.env.get_from_env(key,\\xa0env_key[,\\xa0default])\\nGet a value from a dictionary or an environment variable.\\n\\nutils.ernie_functions.convert_pydantic_to_ernie_function(...)\\nConverts a Pydantic model to a function description for the Ernie API.\\n\\nutils.ernie_functions.convert_pydantic_to_ernie_tool(...)\\nConverts a Pydantic model to a function description for the Ernie API.\\n\\nutils.html.extract_sub_links(raw_html,\\xa0url,\\xa0*)\\nExtract all links from a raw html string and convert into absolute paths.\\n\\nutils.html.find_all_links(raw_html,\\xa0*[,\\xa0pattern])\\nExtract all links from a raw html string.\\n\\nutils.json_schema.dereference_refs(schema_obj,\\xa0*)\\nTry to substitute $refs in JSON Schema.\\n\\nutils.math.cosine_similarity(X,\\xa0Y)\\nRow-wise cosine similarity between two equal-width matrices.\\n\\nutils.math.cosine_similarity_top_k(X,\\xa0Y[,\\xa0...])\\nRow-wise cosine similarity with optional top-k and score threshold filtering.\\n\\nutils.openai.is_openai_v1()\\n\\nutils.openai_functions.convert_pydantic_to_openai_function(...)\\nConverts a Pydantic model to a function description for the OpenAI API.\\n\\nutils.openai_functions.convert_pydantic_to_openai_tool(...)\\nConverts a Pydantic model to a function description for the OpenAI API.\\n\\nutils.strings.comma_list(items)\\nConvert a list to a comma-separated string.\\n\\nutils.strings.stringify_dict(data)\\nStringify a dictionary.\\n\\nutils.strings.stringify_value(val)\\nStringify a value.\\n\\nlangchain.vectorstores¶\\nVector store stores embedded data and performs vector search.\\nOne of the most common ways to store and search over unstructured data is to\\nembed it and store the resulting embedding vectors, and then query the store\\nand retrieve the data that are ‘most similar’ to the embedded query.\\nClass hierarchy:\\nVectorStore --> <name>  # Examples: Annoy, FAISS, Milvus\\n\\nBaseRetriever --> VectorStoreRetriever --> <name>Retriever  # Example: VespaRetriever\\n\\nMain helpers:\\nEmbeddings, Document\\n\\nClasses¶\\n\\nvectorstores.alibabacloud_opensearch.AlibabaCloudOpenSearch(...)\\nAlibaba Cloud OpenSearch vector store.\\n\\nvectorstores.alibabacloud_opensearch.AlibabaCloudOpenSearchSettings(...)\\nAlibaba Cloud Opensearch` client configuration.\\n\\nvectorstores.analyticdb.AnalyticDB(...[,\\xa0...])\\nAnalyticDB (distributed PostgreSQL) vector store.\\n\\nvectorstores.annoy.Annoy(embedding_function,\\xa0...)\\nAnnoy vector store.\\n\\nvectorstores.astradb.AstraDB(*,\\xa0embedding,\\xa0...)\\nWrapper around DataStax Astra DB for vector-store workloads.\\n\\nvectorstores.atlas.AtlasDB(name[,\\xa0...])\\nAtlas vector store.\\n\\nvectorstores.awadb.AwaDB([table_name,\\xa0...])\\nAwaDB vector store.\\n\\nvectorstores.azure_cosmos_db.AzureCosmosDBVectorSearch(...)\\nAzure Cosmos DB for MongoDB vCore vector store.\\n\\nvectorstores.azure_cosmos_db.CosmosDBSimilarityType(value)\\nCosmos DB Similarity Type as enumerator.\\n\\nvectorstores.azuresearch.AzureSearch(...[,\\xa0...])\\nAzure Cognitive Search vector store.\\n\\nvectorstores.azuresearch.AzureSearchVectorStoreRetriever\\nRetriever that uses Azure Cognitive Search.\\n\\nvectorstores.bageldb.Bagel([cluster_name,\\xa0...])\\nBagelDB.ai vector store.\\n\\nvectorstores.baiducloud_vector_search.BESVectorStore(...)\\nBaidu Elasticsearch vector store.\\n\\nvectorstores.cassandra.Cassandra(embedding,\\xa0...)\\nWrapper around Apache Cassandra(R) for vector-store workloads.\\n\\nvectorstores.chroma.Chroma([...])\\nChromaDB vector store.\\n\\nvectorstores.clarifai.Clarifai([user_id,\\xa0...])\\nClarifai AI vector store.\\n\\nvectorstores.clickhouse.Clickhouse(embedding)\\nClickHouse VectorSearch vector store.\\n\\nvectorstores.clickhouse.ClickhouseSettings\\nClickHouse client configuration.\\n\\nvectorstores.dashvector.DashVector(...)\\nDashVector vector store.\\n\\nvectorstores.deeplake.DeepLake([...])\\nActiveloop Deep Lake vector store.\\n\\nvectorstores.dingo.Dingo(embedding,\\xa0text_key,\\xa0*)\\nDingo vector store.\\n\\nvectorstores.docarray.base.DocArrayIndex(...)\\nBase class for DocArray based vector stores.\\n\\nvectorstores.docarray.hnsw.DocArrayHnswSearch(...)\\nHnswLib storage using DocArray package.\\n\\nvectorstores.docarray.in_memory.DocArrayInMemorySearch(...)\\nIn-memory DocArray storage for exact search.\\n\\nvectorstores.elastic_vector_search.ElasticKnnSearch(...)\\n[Deprecated]  [DEPRECATED] Elasticsearch with k-nearest neighbor search (k-NN) vector store.\\n\\nvectorstores.elastic_vector_search.ElasticVectorSearch(...)\\nElasticVectorSearch uses the brute force method of searching on vectors.\\n\\nvectorstores.elasticsearch.ApproxRetrievalStrategy([...])\\nApproximate retrieval strategy using the HNSW algorithm.\\n\\nvectorstores.elasticsearch.BaseRetrievalStrategy()\\nBase class for Elasticsearch retrieval strategies.\\n\\nvectorstores.elasticsearch.ElasticsearchStore(...)\\nElasticsearch vector store.\\n\\nvectorstores.elasticsearch.ExactRetrievalStrategy()\\nExact retrieval strategy using the script_score query.\\n\\nvectorstores.elasticsearch.SparseRetrievalStrategy([...])\\nSparse retrieval strategy using the text_expansion processor.\\n\\nvectorstores.epsilla.Epsilla(client,\\xa0embeddings)\\nWrapper around Epsilla vector database.\\n\\nvectorstores.faiss.FAISS(embedding_function,\\xa0...)\\nMeta Faiss vector store.\\n\\nvectorstores.hippo.Hippo(embedding_function)\\nHippo vector store.\\n\\nvectorstores.hologres.Hologres(...[,\\xa0ndims,\\xa0...])\\nHologres API vector store.\\n\\nvectorstores.hologres.HologresWrapper(...)\\nHologres API wrapper.\\n\\nvectorstores.lancedb.LanceDB(connection,\\xa0...)\\nLanceDB vector store.\\n\\nvectorstores.llm_rails.LLMRails([...])\\nImplementation of Vector Store using LLMRails.\\n\\nvectorstores.llm_rails.LLMRailsRetriever\\nRetriever for LLMRails.\\n\\nvectorstores.marqo.Marqo(client,\\xa0index_name)\\nMarqo vector store.\\n\\nvectorstores.matching_engine.MatchingEngine(...)\\nGoogle Vertex AI Matching Engine vector store.\\n\\nvectorstores.meilisearch.Meilisearch(embedding)\\nMeilisearch vector store.\\n\\nvectorstores.milvus.Milvus(embedding_function)\\nMilvus vector store.\\n\\nvectorstores.momento_vector_index.MomentoVectorIndex(...)\\nMomento Vector Index (MVI) vector store.\\n\\nvectorstores.mongodb_atlas.MongoDBAtlasVectorSearch(...)\\nMongoDB Atlas Vector Search vector store.\\n\\nvectorstores.myscale.MyScale(embedding[,\\xa0config])\\nMyScale vector store.\\n\\nvectorstores.myscale.MyScaleSettings\\nMyScale client configuration.\\n\\nvectorstores.myscale.MyScaleWithoutJSON(...)\\nMyScale vector store without metadata column\\n\\nvectorstores.neo4j_vector.Neo4jVector(...[,\\xa0...])\\nNeo4j vector index.\\n\\nvectorstores.neo4j_vector.SearchType(value)\\nEnumerator of the Distance strategies.\\n\\nvectorstores.nucliadb.NucliaDB(...[,\\xa0...])\\nNucliaDB vector store.\\n\\nvectorstores.opensearch_vector_search.OpenSearchVectorSearch(...)\\nAmazon OpenSearch Vector Engine vector store.\\n\\nvectorstores.pgembedding.BaseModel(**kwargs)\\nBase model for all SQL stores.\\n\\nvectorstores.pgembedding.CollectionStore(...)\\nCollection store.\\n\\nvectorstores.pgembedding.EmbeddingStore(**kwargs)\\nEmbedding store.\\n\\nvectorstores.pgembedding.PGEmbedding(...[,\\xa0...])\\nPostgres with the pg_embedding extension as a vector store.\\n\\nvectorstores.pgembedding.QueryResult()\\nResult from a query.\\n\\nvectorstores.pgvecto_rs.PGVecto_rs(...[,\\xa0...])\\n\\nvectorstores.pgvector.BaseModel(**kwargs)\\nBase model for the SQL stores.\\n\\nvectorstores.pgvector.DistanceStrategy(value)\\nEnumerator of the Distance strategies.\\n\\nvectorstores.pgvector.PGVector(...[,\\xa0...])\\nPostgres/PGVector vector store.\\n\\nvectorstores.pinecone.Pinecone(index,\\xa0...[,\\xa0...])\\nPinecone vector store.\\n\\nvectorstores.qdrant.Qdrant(client,\\xa0...[,\\xa0...])\\nQdrant vector store.\\n\\nvectorstores.qdrant.QdrantException\\nQdrant related exceptions.\\n\\nvectorstores.redis.base.Redis(redis_url,\\xa0...)\\nRedis vector database.\\n\\nvectorstores.redis.base.RedisVectorStoreRetriever\\nRetriever for Redis VectorStore.\\n\\nvectorstores.redis.filters.RedisFilter()\\nCollection of RedisFilterFields.\\n\\nvectorstores.redis.filters.RedisFilterExpression([...])\\nA logical expression of RedisFilterFields.\\n\\nvectorstores.redis.filters.RedisFilterField(field)\\nBase class for RedisFilterFields.\\n\\nvectorstores.redis.filters.RedisFilterOperator(value)\\nRedisFilterOperator enumerator is used to create RedisFilterExpressions.\\n\\nvectorstores.redis.filters.RedisNum(field)\\nA RedisFilterField representing a numeric field in a Redis index.\\n\\nvectorstores.redis.filters.RedisTag(field)\\nA RedisFilterField representing a tag in a Redis index.\\n\\nvectorstores.redis.filters.RedisText(field)\\nA RedisFilterField representing a text field in a Redis index.\\n\\nvectorstores.redis.schema.FlatVectorField\\nSchema for flat vector fields in Redis.\\n\\nvectorstores.redis.schema.HNSWVectorField\\nSchema for HNSW vector fields in Redis.\\n\\nvectorstores.redis.schema.NumericFieldSchema\\nSchema for numeric fields in Redis.\\n\\nvectorstores.redis.schema.RedisDistanceMetric(value)\\nDistance metrics for Redis vector fields.\\n\\nvectorstores.redis.schema.RedisField\\nBase class for Redis fields.\\n\\nvectorstores.redis.schema.RedisModel\\nSchema for Redis index.\\n\\nvectorstores.redis.schema.RedisVectorField\\nBase class for Redis vector fields.\\n\\nvectorstores.redis.schema.TagFieldSchema\\nSchema for tag fields in Redis.\\n\\nvectorstores.redis.schema.TextFieldSchema\\nSchema for text fields in Redis.\\n\\nvectorstores.rocksetdb.Rockset(client,\\xa0...)\\nRockset vector store.\\n\\nvectorstores.scann.ScaNN(embedding,\\xa0index,\\xa0...)\\nScaNN vector store.\\n\\nvectorstores.semadb.SemaDB(collection_name,\\xa0...)\\nSemaDB vector store.\\n\\nvectorstores.singlestoredb.SingleStoreDB(...)\\nSingleStore DB vector store.\\n\\nvectorstores.sklearn.BaseSerializer(persist_path)\\nBase class for serializing data.\\n\\nvectorstores.sklearn.BsonSerializer(persist_path)\\nSerializes data in binary json using the bson python package.\\n\\nvectorstores.sklearn.JsonSerializer(persist_path)\\nSerializes data in json using the json package from python standard library.\\n\\nvectorstores.sklearn.ParquetSerializer(...)\\nSerializes data in Apache Parquet format using the pyarrow package.\\n\\nvectorstores.sklearn.SKLearnVectorStore(...)\\nSimple in-memory vector store based on the scikit-learn library NearestNeighbors implementation.\\n\\nvectorstores.sklearn.SKLearnVectorStoreException\\nException raised by SKLearnVectorStore.\\n\\nvectorstores.sqlitevss.SQLiteVSS(table,\\xa0...)\\nWrapper around SQLite with vss extension as a vector database.\\n\\nvectorstores.starrocks.StarRocks(embedding)\\nStarRocks vector store.\\n\\nvectorstores.starrocks.StarRocksSettings\\nStarRocks client configuration.\\n\\nvectorstores.supabase.SupabaseVectorStore(...)\\nSupabase Postgres vector store.\\n\\nvectorstores.tair.Tair(embedding_function,\\xa0...)\\nTair vector store.\\n\\nvectorstores.tencentvectordb.ConnectionParams(...)\\nTencent vector DB Connection params.\\n\\nvectorstores.tencentvectordb.IndexParams(...)\\nTencent vector DB Index params.\\n\\nvectorstores.tencentvectordb.TencentVectorDB(...)\\nInitialize wrapper around the tencent vector database.\\n\\nvectorstores.tigris.Tigris(client,\\xa0...)\\nTigris vector store.\\n\\nvectorstores.tiledb.TileDB(embedding,\\xa0...[,\\xa0...])\\nWrapper around TileDB vector database.\\n\\nvectorstores.timescalevector.TimescaleVector(...)\\nVectorStore implementation using the timescale vector client to store vectors in Postgres.\\n\\nvectorstores.typesense.Typesense(...[,\\xa0...])\\nTypesense vector store.\\n\\nvectorstores.usearch.USearch(embedding,\\xa0...)\\nUSearch vector store.\\n\\nvectorstores.utils.DistanceStrategy(value[,\\xa0...])\\nEnumerator of the Distance strategies for calculating distances between vectors.\\n\\nvectorstores.vald.Vald(embedding[,\\xa0host,\\xa0...])\\nWrapper around Vald vector database.\\n\\nvectorstores.vearch.Vearch(embedding_function)\\nInitialize vearch vector store flag 1 for cluster,0 for standalone\\n\\nvectorstores.vectara.Vectara([...])\\nVectara API vector store.\\n\\nvectorstores.vectara.VectaraRetriever\\nRetriever class for Vectara.\\n\\nvectorstores.vespa.VespaStore(app[,\\xa0...])\\nVespa vector store.\\n\\nvectorstores.weaviate.Weaviate(client,\\xa0...)\\nWeaviate vector store.\\n\\nvectorstores.xata.XataVectorStore(api_key,\\xa0...)\\nXata vector store.\\n\\nvectorstores.zep.CollectionConfig(name,\\xa0...)\\nConfiguration for a Zep Collection.\\n\\nvectorstores.zep.ZepVectorStore(...[,\\xa0...])\\nZep vector store.\\n\\nvectorstores.zilliz.Zilliz(embedding_function)\\nZilliz vector store.\\n\\nFunctions¶\\n\\nvectorstores.alibabacloud_opensearch.create_metadata(fields)\\nCreate metadata from fields.\\n\\nvectorstores.annoy.dependable_annoy_import()\\nImport annoy if available, otherwise raise error.\\n\\nvectorstores.clickhouse.has_mul_sub_str(s,\\xa0*args)\\nCheck if a string contains multiple substrings.\\n\\nvectorstores.faiss.dependable_faiss_import([...])\\nImport faiss if available, otherwise raise error.\\n\\nvectorstores.myscale.has_mul_sub_str(s,\\xa0*args)\\nCheck if a string contains multiple substrings.\\n\\nvectorstores.neo4j_vector.check_if_not_null(...)\\nCheck if the values are not None or empty string\\n\\nvectorstores.neo4j_vector.sort_by_index_name(...)\\nSort first element to match the index_name if exists\\n\\nvectorstores.qdrant.sync_call_fallback(method)\\nDecorator to call the synchronous method of the class if the async method is not implemented.\\n\\nvectorstores.redis.base.check_index_exists(...)\\nCheck if Redis index exists.\\n\\nvectorstores.redis.filters.check_operator_misuse(func)\\nDecorator to check for misuse of equality operators.\\n\\nvectorstores.redis.schema.read_schema(...)\\nReads in the index schema from a dict or yaml file.\\n\\nvectorstores.scann.dependable_scann_import()\\nImport scann if available, otherwise raise error.\\n\\nvectorstores.scann.normalize(x)\\nNormalize vectors to unit length.\\n\\nvectorstores.starrocks.debug_output(s)\\nPrint a debug message if DEBUG is True.\\n\\nvectorstores.starrocks.get_named_result(...)\\nGet a named result from a query.\\n\\nvectorstores.starrocks.has_mul_sub_str(s,\\xa0*args)\\nCheck if a string has multiple substrings.\\n\\nvectorstores.tiledb.dependable_tiledb_import()\\nImport tiledb-vector-search if available, otherwise raise error.\\n\\nvectorstores.tiledb.get_documents_array_uri(uri)\\n\\nvectorstores.tiledb.get_documents_array_uri_from_group(group)\\n\\nvectorstores.tiledb.get_vector_index_uri(uri)\\n\\nvectorstores.tiledb.get_vector_index_uri_from_group(group)\\n\\nvectorstores.usearch.dependable_usearch_import()\\nImport usearch if available, otherwise raise error.\\n\\nvectorstores.utils.filter_complex_metadata(...)\\nFilter out metadata types that are not supported for a vector store.\\n\\nvectorstores.utils.maximal_marginal_relevance(...)\\nCalculate maximal marginal relevance.\\n\\n            © 2023, Harrison Chase.\\n          Last updated on Nov 23, 2023.\\n          Show this page source' metadata={'source': 'https://api.python.langchain.com/en/latest/api_reference.html', 'title': 'langchain API Reference — 🦜🔗 LangChain 0.0.339rc1', 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "print(docs_from_api[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb2c61de-5117-460a-8e66-cab8b4929eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain API Reference — 🦜🔗 LangChain 0.0.339rc1\n",
      "\n",
      "API\n",
      "\n",
      "Core\n",
      "\n",
      "Experimental\n",
      "\n",
      "Python Docs\n",
      "\n",
      "Toggle Menu\n",
      "\n",
      "Prev\n",
      "Up\n",
      "Next\n",
      "\n",
      "LangChain 0.0.339rc1\n",
      "\n",
      "langchain API Reference\n",
      "langchain.adapters\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.agents\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.agents.format_scratchpad\n",
      "Functions\n",
      "\n",
      "langchain.agents.output_parsers\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.cache\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.callbacks\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.chains\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.chat_loaders\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.chat_models\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.docstore\n",
      "Classes\n",
      "\n",
      "langchain.document_loaders\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.document_transformers\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.embeddings\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.evaluation\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.graphs\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.hub\n",
      "Functions\n",
      "\n",
      "langchain.indexes\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.llms\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.memory\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.model_laboratory\n",
      "Classes\n",
      "\n",
      "langchain.output_parsers\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.prompts\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.retrievers\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.runnables\n",
      "Classes\n",
      "\n",
      "langchain.smith\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.storage\n",
      "Classes\n",
      "\n",
      "langchain.text_splitter\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.tools\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.tools.render\n",
      "Functions\n",
      "\n",
      "langchain.utilities\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.utils\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain.vectorstores\n",
      "Classes\n",
      "Functions\n",
      "\n",
      "langchain API Reference¶\n",
      "\n",
      "langchain.adapters¶\n",
      "\n",
      "Classes¶\n",
      "\n",
      "adapters.openai.ChatCompletion()\n",
      "Chat completion.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "adapters.openai.aenumerate(iterable[, start])\n",
      "Async version of enumerate function.\n",
      "\n",
      "adapters.openai.convert_dict_to_message(_dict)\n",
      "Convert a dictionary to a LangChain message.\n",
      "\n",
      "adapters.openai.convert_message_to_dict(message)\n",
      "Convert a LangChain message to a dictionary.\n",
      "\n",
      "adapters.openai.convert_messages_for_finetuning(...)\n",
      "Convert messages to a list of lists of dictionaries for fine-tuning.\n",
      "\n",
      "adapters.openai.convert_openai_messages(messages)\n",
      "Convert dictionaries representing OpenAI messages to LangChain format.\n",
      "\n",
      "langchain.agents¶\n",
      "Agent is a class that uses an LLM to choose a sequence of actions to take.\n",
      "In Chains, a sequence of actions is hardcoded. In Agents,\n",
      "a language model is used as a reasoning engine to determine which actions\n",
      "to take and in which order.\n",
      "Agents select and use Tools and Toolkits for actions.\n",
      "Class hierarchy:\n",
      "BaseSingleActionAgent --> LLMSingleActionAgent\n",
      "                          OpenAIFunctionsAgent\n",
      "                          XMLAgent\n",
      "                          Agent --> <name>Agent  # Examples: ZeroShotAgent, ChatAgent\n",
      "\n",
      "BaseMultiActionAgent  --> OpenAIMultiFunctionsAgent\n",
      "\n",
      "Main helpers:\n",
      "AgentType, AgentExecutor, AgentOutputParser, AgentExecutorIterator,\n",
      "AgentAction, AgentFinish\n",
      "\n",
      "Classes¶\n",
      "\n",
      "agents.agent.Agent\n",
      "Agent that calls the language model and deciding the action.\n",
      "\n",
      "agents.agent.AgentExecutor\n",
      "Agent that is using tools.\n",
      "\n",
      "agents.agent.AgentOutputParser\n",
      "Base class for parsing agent output into agent action/finish.\n",
      "\n",
      "agents.agent.BaseMultiActionAgent\n",
      "Base Multi Action Agent class.\n",
      "\n",
      "agents.agent.BaseSingleActionAgent\n",
      "Base Single Action Agent class.\n",
      "\n",
      "agents.agent.ExceptionTool\n",
      "Tool that just returns the query.\n",
      "\n",
      "agents.agent.LLMSingleActionAgent\n",
      "Base class for single action agents.\n",
      "\n",
      "agents.agent.MultiActionAgentOutputParser\n",
      "Base class for parsing agent output into agent actions/finish.\n",
      "\n",
      "agents.agent.RunnableAgent\n",
      "Agent powered by runnables.\n",
      "\n",
      "agents.agent.RunnableMultiActionAgent\n",
      "Agent powered by runnables.\n",
      "\n",
      "agents.agent_iterator.AgentExecutorIterator(...)\n",
      "Iterator for AgentExecutor.\n",
      "\n",
      "agents.agent_iterator.BaseAgentExecutorIterator()\n",
      "Base class for AgentExecutorIterator.\n",
      "\n",
      "agents.agent_toolkits.ainetwork.toolkit.AINetworkToolkit\n",
      "Toolkit for interacting with AINetwork Blockchain.\n",
      "\n",
      "agents.agent_toolkits.amadeus.toolkit.AmadeusToolkit\n",
      "Toolkit for interacting with Amadeus which offers APIs for travel search.\n",
      "\n",
      "agents.agent_toolkits.azure_cognitive_services.AzureCognitiveServicesToolkit\n",
      "Toolkit for Azure Cognitive Services.\n",
      "\n",
      "agents.agent_toolkits.base.BaseToolkit\n",
      "Base Toolkit representing a collection of related tools.\n",
      "\n",
      "agents.agent_toolkits.clickup.toolkit.ClickupToolkit\n",
      "Clickup Toolkit.\n",
      "\n",
      "agents.agent_toolkits.file_management.toolkit.FileManagementToolkit\n",
      "Toolkit for interacting with local files.\n",
      "\n",
      "agents.agent_toolkits.github.toolkit.GitHubToolkit\n",
      "GitHub Toolkit.\n",
      "\n",
      "agents.agent_toolkits.gitlab.toolkit.GitLabToolkit\n",
      "GitLab Toolkit.\n",
      "\n",
      "agents.agent_toolkits.gmail.toolkit.GmailToolkit\n",
      "Toolkit for interacting with Gmail.\n",
      "\n",
      "agents.agent_toolkits.jira.toolkit.JiraToolkit\n",
      "Jira Toolkit.\n",
      "\n",
      "agents.agent_toolkits.json.toolkit.JsonToolkit\n",
      "Toolkit for interacting with a JSON spec.\n",
      "\n",
      "agents.agent_toolkits.multion.toolkit.MultionToolkit\n",
      "Toolkit for interacting with the Browser Agent.\n",
      "\n",
      "agents.agent_toolkits.nla.tool.NLATool\n",
      "Natural Language API Tool.\n",
      "\n",
      "agents.agent_toolkits.nla.toolkit.NLAToolkit\n",
      "Natural Language API Toolkit.\n",
      "\n",
      "agents.agent_toolkits.office365.toolkit.O365Toolkit\n",
      "Toolkit for interacting with Office 365.\n",
      "\n",
      "agents.agent_toolkits.openapi.planner.RequestsDeleteToolWithParsing\n",
      "A tool that sends a DELETE request and parses the response.\n",
      "\n",
      "agents.agent_toolkits.openapi.planner.RequestsGetToolWithParsing\n",
      "Requests GET tool with LLM-instructed extraction of truncated responses.\n",
      "\n",
      "agents.agent_toolkits.openapi.planner.RequestsPatchToolWithParsing\n",
      "Requests PATCH tool with LLM-instructed extraction of truncated responses.\n",
      "\n",
      "agents.agent_toolkits.openapi.planner.RequestsPostToolWithParsing\n",
      "Requests POST tool with LLM-instructed extraction of truncated responses.\n",
      "\n",
      "agents.agent_toolkits.openapi.planner.RequestsPutToolWithParsing\n",
      "Requests PUT tool with LLM-instructed extraction of truncated responses.\n",
      "\n",
      "agents.agent_toolkits.openapi.spec.ReducedOpenAPISpec(...)\n",
      "A reduced OpenAPI spec.\n",
      "\n",
      "agents.agent_toolkits.openapi.toolkit.OpenAPIToolkit\n",
      "Toolkit for interacting with an OpenAPI API.\n",
      "\n",
      "agents.agent_toolkits.openapi.toolkit.RequestsToolkit\n",
      "Toolkit for making REST requests.\n",
      "\n",
      "agents.agent_toolkits.playwright.toolkit.PlayWrightBrowserToolkit\n",
      "Toolkit for PlayWright browser tools.\n",
      "\n",
      "agents.agent_toolkits.powerbi.toolkit.PowerBIToolkit\n",
      "Toolkit for interacting with Power BI dataset.\n",
      "\n",
      "agents.agent_toolkits.spark_sql.toolkit.SparkSQLToolkit\n",
      "Toolkit for interacting with Spark SQL.\n",
      "\n",
      "agents.agent_toolkits.sql.toolkit.SQLDatabaseToolkit\n",
      "Toolkit for interacting with SQL databases.\n",
      "\n",
      "agents.agent_toolkits.vectorstore.toolkit.VectorStoreInfo\n",
      "Information about a VectorStore.\n",
      "\n",
      "agents.agent_toolkits.vectorstore.toolkit.VectorStoreRouterToolkit\n",
      "Toolkit for routing between Vector Stores.\n",
      "\n",
      "agents.agent_toolkits.vectorstore.toolkit.VectorStoreToolkit\n",
      "Toolkit for interacting with a Vector Store.\n",
      "\n",
      "agents.agent_toolkits.zapier.toolkit.ZapierToolkit\n",
      "Zapier Toolkit.\n",
      "\n",
      "agents.agent_types.AgentType(value[, names, ...])\n",
      "An enum for agent types.\n",
      "\n",
      "agents.chat.base.ChatAgent\n",
      "Chat Agent.\n",
      "\n",
      "agents.chat.output_parser.ChatOutputParser\n",
      "Output parser for the chat agent.\n",
      "\n",
      "agents.conversational.base.ConversationalAgent\n",
      "An agent that holds a conversation in addition to using tools.\n",
      "\n",
      "agents.conversational.output_parser.ConvoOutputParser\n",
      "Output parser for the conversational agent.\n",
      "\n",
      "agents.conversational_chat.base.ConversationalChatAgent\n",
      "An agent designed to hold a conversation in addition to using tools.\n",
      "\n",
      "agents.conversational_chat.output_parser.ConvoOutputParser\n",
      "Output parser for the conversational agent.\n",
      "\n",
      "agents.mrkl.base.ChainConfig(action_name, ...)\n",
      "Configuration for chain to use in MRKL system.\n",
      "\n",
      "agents.mrkl.base.MRKLChain\n",
      "[Deprecated] Chain that implements the MRKL system.\n",
      "\n",
      "agents.mrkl.base.ZeroShotAgent\n",
      "Agent for the MRKL chain.\n",
      "\n",
      "agents.mrkl.output_parser.MRKLOutputParser\n",
      "MRKL Output parser for the chat agent.\n",
      "\n",
      "agents.openai_assistant.base.OpenAIAssistantAction\n",
      "AgentAction with info needed to submit custom tool output to existing run.\n",
      "\n",
      "agents.openai_assistant.base.OpenAIAssistantFinish\n",
      "AgentFinish with run and thread metadata.\n",
      "\n",
      "agents.openai_assistant.base.OpenAIAssistantRunnable\n",
      "Run an OpenAI Assistant.\n",
      "\n",
      "agents.openai_functions_agent.agent_token_buffer_memory.AgentTokenBufferMemory\n",
      "Memory used to save agent output AND intermediate steps.\n",
      "\n",
      "agents.openai_functions_agent.base.OpenAIFunctionsAgent\n",
      "An Agent driven by OpenAIs function powered API.\n",
      "\n",
      "agents.openai_functions_multi_agent.base.OpenAIMultiFunctionsAgent\n",
      "An Agent driven by OpenAIs function powered API.\n",
      "\n",
      "agents.output_parsers.json.JSONAgentOutputParser\n",
      "Parses tool invocations and final answers in JSON format.\n",
      "\n",
      "agents.output_parsers.openai_functions.OpenAIFunctionsAgentOutputParser\n",
      "Parses a message into agent action/finish.\n",
      "\n",
      "agents.output_parsers.openai_tools.OpenAIToolAgentAction\n",
      "Override init to support instantiation by position for backward compat.\n",
      "\n",
      "agents.output_parsers.openai_tools.OpenAIToolsAgentOutputParser\n",
      "Parses a message into agent actions/finish.\n",
      "\n",
      "agents.output_parsers.react_json_single_input.ReActJsonSingleInputOutputParser\n",
      "Parses ReAct-style LLM calls that have a single tool input in json format.\n",
      "\n",
      "agents.output_parsers.react_single_input.ReActSingleInputOutputParser\n",
      "Parses ReAct-style LLM calls that have a single tool input.\n",
      "\n",
      "agents.output_parsers.self_ask.SelfAskOutputParser\n",
      "Parses self-ask style LLM calls.\n",
      "\n",
      "agents.output_parsers.xml.XMLAgentOutputParser\n",
      "Parses tool invocations and final answers in XML format.\n",
      "\n",
      "agents.react.base.DocstoreExplorer(docstore)\n",
      "Class to assist with exploration of a document store.\n",
      "\n",
      "agents.react.base.ReActChain\n",
      "[Deprecated] Chain that implements the ReAct paper.\n",
      "\n",
      "agents.react.base.ReActDocstoreAgent\n",
      "Agent for the ReAct chain.\n",
      "\n",
      "agents.react.base.ReActTextWorldAgent\n",
      "Agent for the ReAct TextWorld chain.\n",
      "\n",
      "agents.react.output_parser.ReActOutputParser\n",
      "Output parser for the ReAct agent.\n",
      "\n",
      "agents.schema.AgentScratchPadChatPromptTemplate\n",
      "Chat prompt template for the agent scratchpad.\n",
      "\n",
      "agents.self_ask_with_search.base.SelfAskWithSearchAgent\n",
      "Agent for the self-ask-with-search paper.\n",
      "\n",
      "agents.self_ask_with_search.base.SelfAskWithSearchChain\n",
      "[Deprecated] Chain that does self-ask with search.\n",
      "\n",
      "agents.structured_chat.base.StructuredChatAgent\n",
      "Structured Chat Agent.\n",
      "\n",
      "agents.structured_chat.output_parser.StructuredChatOutputParser\n",
      "Output parser for the structured chat agent.\n",
      "\n",
      "agents.structured_chat.output_parser.StructuredChatOutputParserWithRetries\n",
      "Output parser with retries for the structured chat agent.\n",
      "\n",
      "agents.tools.InvalidTool\n",
      "Tool that is run when invalid tool name is encountered by agent.\n",
      "\n",
      "agents.xml.base.XMLAgent\n",
      "Agent that uses XML tags.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "agents.agent_iterator.rebuild_callback_manager_on_set(...)\n",
      "Decorator to force setters to rebuild callback mgr\n",
      "\n",
      "agents.agent_toolkits.conversational_retrieval.openai_functions.create_conversational_retrieval_agent(...)\n",
      "A convenience method for creating a conversational retrieval agent.\n",
      "\n",
      "agents.agent_toolkits.json.base.create_json_agent(...)\n",
      "Construct a json agent from an LLM and tools.\n",
      "\n",
      "agents.agent_toolkits.openapi.base.create_openapi_agent(...)\n",
      "Construct an OpenAPI agent from an LLM and tools.\n",
      "\n",
      "agents.agent_toolkits.openapi.planner.create_openapi_agent(...)\n",
      "Instantiate OpenAI API planner and controller for a given spec.\n",
      "\n",
      "agents.agent_toolkits.openapi.spec.reduce_openapi_spec(spec)\n",
      "Simplify/distill/minify a spec somehow.\n",
      "\n",
      "agents.agent_toolkits.powerbi.base.create_pbi_agent(llm)\n",
      "Construct a Power BI agent from an LLM and tools.\n",
      "\n",
      "agents.agent_toolkits.powerbi.chat_base.create_pbi_chat_agent(llm)\n",
      "Construct a Power BI agent from a Chat LLM and tools.\n",
      "\n",
      "agents.agent_toolkits.spark_sql.base.create_spark_sql_agent(...)\n",
      "Construct a Spark SQL agent from an LLM and tools.\n",
      "\n",
      "agents.agent_toolkits.sql.base.create_sql_agent(...)\n",
      "Construct an SQL agent from an LLM and tools.\n",
      "\n",
      "agents.agent_toolkits.vectorstore.base.create_vectorstore_agent(...)\n",
      "Construct a VectorStore agent from an LLM and tools.\n",
      "\n",
      "agents.agent_toolkits.vectorstore.base.create_vectorstore_router_agent(...)\n",
      "Construct a VectorStore router agent from an LLM and tools.\n",
      "\n",
      "agents.format_scratchpad.log.format_log_to_str(...)\n",
      "Construct the scratchpad that lets the agent continue its thought process.\n",
      "\n",
      "agents.format_scratchpad.log_to_messages.format_log_to_messages(...)\n",
      "Construct the scratchpad that lets the agent continue its thought process.\n",
      "\n",
      "agents.format_scratchpad.openai_functions.format_to_openai_function_messages(...)\n",
      "Convert (AgentAction, tool output) tuples into FunctionMessages.\n",
      "\n",
      "agents.format_scratchpad.openai_functions.format_to_openai_functions(...)\n",
      "Convert (AgentAction, tool output) tuples into FunctionMessages.\n",
      "\n",
      "agents.format_scratchpad.openai_tools.format_to_openai_tool_messages(...)\n",
      "Convert (AgentAction, tool output) tuples into FunctionMessages.\n",
      "\n",
      "agents.format_scratchpad.xml.format_xml(...)\n",
      "Format the intermediate steps as XML.\n",
      "\n",
      "agents.initialize.initialize_agent(tools, llm)\n",
      "Load an agent executor given tools and LLM.\n",
      "\n",
      "agents.load_tools.get_all_tool_names()\n",
      "Get a list of all possible tool names.\n",
      "\n",
      "agents.load_tools.load_huggingface_tool(...)\n",
      "Loads a tool from the HuggingFace Hub.\n",
      "\n",
      "agents.load_tools.load_tools(tool_names[, ...])\n",
      "Load tools based on their name.\n",
      "\n",
      "agents.loading.load_agent(path, **kwargs)\n",
      "Unified method for loading an agent from LangChainHub or local fs.\n",
      "\n",
      "agents.loading.load_agent_from_config(config)\n",
      "Load agent from Config Dict.\n",
      "\n",
      "agents.output_parsers.openai_tools.parse_ai_message_to_openai_tool_action(message)\n",
      "Parse an AI message potentially containing tool_calls.\n",
      "\n",
      "agents.utils.validate_tools_single_input(...)\n",
      "Validate tools for single input.\n",
      "\n",
      "langchain.agents.format_scratchpad¶\n",
      "Logic for formatting intermediate steps into an agent scratchpad.\n",
      "Intermediate steps refers to the list of (AgentAction, observation) tuples\n",
      "that result from previous iterations of the agent.\n",
      "Depending on the prompting strategy you are using, you may want to format these\n",
      "differently before passing them into the LLM.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "agents.format_scratchpad.log.format_log_to_str(...)\n",
      "Construct the scratchpad that lets the agent continue its thought process.\n",
      "\n",
      "agents.format_scratchpad.log_to_messages.format_log_to_messages(...)\n",
      "Construct the scratchpad that lets the agent continue its thought process.\n",
      "\n",
      "agents.format_scratchpad.openai_functions.format_to_openai_function_messages(...)\n",
      "Convert (AgentAction, tool output) tuples into FunctionMessages.\n",
      "\n",
      "agents.format_scratchpad.openai_functions.format_to_openai_functions(...)\n",
      "Convert (AgentAction, tool output) tuples into FunctionMessages.\n",
      "\n",
      "agents.format_scratchpad.openai_tools.format_to_openai_tool_messages(...)\n",
      "Convert (AgentAction, tool output) tuples into FunctionMessages.\n",
      "\n",
      "agents.format_scratchpad.xml.format_xml(...)\n",
      "Format the intermediate steps as XML.\n",
      "\n",
      "langchain.agents.output_parsers¶\n",
      "Parsing utils to go from string to AgentAction or Agent Finish.\n",
      "AgentAction means that an action should be taken.\n",
      "This contains the name of the tool to use, the input to pass to that tool,\n",
      "and a log variable (which contains a log of the agent’s thinking).\n",
      "AgentFinish means that a response should be given.\n",
      "This contains a return_values dictionary. This usually contains a\n",
      "single output key, but can be extended to contain more.\n",
      "This also contains a log variable (which contains a log of the agent’s thinking).\n",
      "\n",
      "Classes¶\n",
      "\n",
      "agents.output_parsers.json.JSONAgentOutputParser\n",
      "Parses tool invocations and final answers in JSON format.\n",
      "\n",
      "agents.output_parsers.openai_functions.OpenAIFunctionsAgentOutputParser\n",
      "Parses a message into agent action/finish.\n",
      "\n",
      "agents.output_parsers.openai_tools.OpenAIToolAgentAction\n",
      "Override init to support instantiation by position for backward compat.\n",
      "\n",
      "agents.output_parsers.openai_tools.OpenAIToolsAgentOutputParser\n",
      "Parses a message into agent actions/finish.\n",
      "\n",
      "agents.output_parsers.react_json_single_input.ReActJsonSingleInputOutputParser\n",
      "Parses ReAct-style LLM calls that have a single tool input in json format.\n",
      "\n",
      "agents.output_parsers.react_single_input.ReActSingleInputOutputParser\n",
      "Parses ReAct-style LLM calls that have a single tool input.\n",
      "\n",
      "agents.output_parsers.self_ask.SelfAskOutputParser\n",
      "Parses self-ask style LLM calls.\n",
      "\n",
      "agents.output_parsers.xml.XMLAgentOutputParser\n",
      "Parses tool invocations and final answers in XML format.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "agents.output_parsers.openai_tools.parse_ai_message_to_openai_tool_action(message)\n",
      "Parse an AI message potentially containing tool_calls.\n",
      "\n",
      "langchain.cache¶\n",
      "\n",
      "Warning\n",
      "Beta Feature!\n",
      "\n",
      "Cache provides an optional caching layer for LLMs.\n",
      "Cache is useful for two reasons:\n",
      "\n",
      "It can save you money by reducing the number of API calls you make to the LLM\n",
      "provider if you’re often requesting the same completion multiple times.\n",
      "It can speed up your application by reducing the number of API calls you make\n",
      "to the LLM provider.\n",
      "\n",
      "Cache directly competes with Memory. See documentation for Pros and Cons.\n",
      "Class hierarchy:\n",
      "BaseCache --> <name>Cache  # Examples: InMemoryCache, RedisCache, GPTCache\n",
      "\n",
      "Classes¶\n",
      "\n",
      "cache.CassandraCache([session, keyspace, ...])\n",
      "Cache that uses Cassandra / Astra DB as a backend.\n",
      "\n",
      "cache.CassandraSemanticCache(session, ...[, ...])\n",
      "Cache that uses Cassandra as a vector-store backend for semantic (i.e.\n",
      "\n",
      "cache.FullLLMCache(**kwargs)\n",
      "SQLite table for full LLM Cache (all generations).\n",
      "\n",
      "cache.FullMd5LLMCache(**kwargs)\n",
      "SQLite table for full LLM Cache (all generations).\n",
      "\n",
      "cache.GPTCache([init_func])\n",
      "Cache that uses GPTCache as a backend.\n",
      "\n",
      "cache.InMemoryCache()\n",
      "Cache that stores things in memory.\n",
      "\n",
      "cache.MomentoCache(cache_client, cache_name, *)\n",
      "Cache that uses Momento as a backend.\n",
      "\n",
      "cache.RedisCache(redis_, *[, ttl])\n",
      "Cache that uses Redis as a backend.\n",
      "\n",
      "cache.RedisSemanticCache(redis_url, embedding)\n",
      "Cache that uses Redis as a vector-store backend.\n",
      "\n",
      "cache.SQLAlchemyCache(engine, cache_schema)\n",
      "Cache that uses SQAlchemy as a backend.\n",
      "\n",
      "cache.SQLAlchemyMd5Cache(engine, cache_schema)\n",
      "Cache that uses SQAlchemy as a backend.\n",
      "\n",
      "cache.SQLiteCache([database_path])\n",
      "Cache that uses SQLite as a backend.\n",
      "\n",
      "cache.UpstashRedisCache(redis_, *[, ttl])\n",
      "Cache that uses Upstash Redis as a backend.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "langchain.callbacks¶\n",
      "Callback handlers allow listening to events in LangChain.\n",
      "Class hierarchy:\n",
      "BaseCallbackHandler --> <name>CallbackHandler  # Example: AimCallbackHandler\n",
      "\n",
      "Classes¶\n",
      "\n",
      "callbacks.aim_callback.AimCallbackHandler([...])\n",
      "Callback Handler that logs to Aim.\n",
      "\n",
      "callbacks.aim_callback.BaseMetadataCallbackHandler()\n",
      "This class handles the metadata and associated function states for callbacks.\n",
      "\n",
      "callbacks.argilla_callback.ArgillaCallbackHandler(...)\n",
      "Callback Handler that logs into Argilla.\n",
      "\n",
      "callbacks.arize_callback.ArizeCallbackHandler([...])\n",
      "Callback Handler that logs to Arize.\n",
      "\n",
      "callbacks.arthur_callback.ArthurCallbackHandler(...)\n",
      "Callback Handler that logs to Arthur platform.\n",
      "\n",
      "callbacks.clearml_callback.ClearMLCallbackHandler([...])\n",
      "Callback Handler that logs to ClearML.\n",
      "\n",
      "callbacks.comet_ml_callback.CometCallbackHandler([...])\n",
      "Callback Handler that logs to Comet.\n",
      "\n",
      "callbacks.confident_callback.DeepEvalCallbackHandler(metrics)\n",
      "Callback Handler that logs into deepeval.\n",
      "\n",
      "callbacks.context_callback.ContextCallbackHandler([...])\n",
      "Callback Handler that records transcripts to the Context service.\n",
      "\n",
      "callbacks.file.FileCallbackHandler(filename)\n",
      "Callback Handler that writes to a file.\n",
      "\n",
      "callbacks.flyte_callback.FlyteCallbackHandler()\n",
      "This callback handler that is used within a Flyte task.\n",
      "\n",
      "callbacks.human.HumanApprovalCallbackHandler(...)\n",
      "Callback for manually validating values.\n",
      "\n",
      "callbacks.human.HumanRejectedException\n",
      "Exception to raise when a person manually review and rejects a value.\n",
      "\n",
      "callbacks.infino_callback.InfinoCallbackHandler([...])\n",
      "Callback Handler that logs to Infino.\n",
      "\n",
      "callbacks.labelstudio_callback.LabelStudioCallbackHandler([...])\n",
      "Label Studio callback handler.\n",
      "\n",
      "callbacks.labelstudio_callback.LabelStudioMode(value)\n",
      "Label Studio mode enumerator.\n",
      "\n",
      "callbacks.llmonitor_callback.LLMonitorCallbackHandler([...])\n",
      "Callback Handler for LLMonitor`.\n",
      "\n",
      "callbacks.llmonitor_callback.UserContextManager(user_id)\n",
      "Context manager for LLMonitor user context.\n",
      "\n",
      "callbacks.mlflow_callback.MlflowCallbackHandler([...])\n",
      "Callback Handler that logs metrics and artifacts to mlflow server.\n",
      "\n",
      "callbacks.mlflow_callback.MlflowLogger(**kwargs)\n",
      "Callback Handler that logs metrics and artifacts to mlflow server.\n",
      "\n",
      "callbacks.openai_info.OpenAICallbackHandler()\n",
      "Callback Handler that tracks OpenAI info.\n",
      "\n",
      "callbacks.promptlayer_callback.PromptLayerCallbackHandler([...])\n",
      "Callback handler for promptlayer.\n",
      "\n",
      "callbacks.sagemaker_callback.SageMakerCallbackHandler(run)\n",
      "Callback Handler that logs prompt artifacts and metrics to SageMaker Experiments.\n",
      "\n",
      "callbacks.streaming_aiter.AsyncIteratorCallbackHandler()\n",
      "Callback handler that returns an async iterator.\n",
      "\n",
      "callbacks.streaming_aiter_final_only.AsyncFinalIteratorCallbackHandler(*)\n",
      "Callback handler that returns an async iterator.\n",
      "\n",
      "callbacks.streaming_stdout_final_only.FinalStreamingStdOutCallbackHandler(*)\n",
      "Callback handler for streaming in agents.\n",
      "\n",
      "callbacks.streamlit.mutable_expander.ChildRecord(...)\n",
      "The child record as a NamedTuple.\n",
      "\n",
      "callbacks.streamlit.mutable_expander.ChildType(value)\n",
      "The enumerator of the child type.\n",
      "\n",
      "callbacks.streamlit.mutable_expander.MutableExpander(...)\n",
      "A Streamlit expander that can be renamed and dynamically expanded/collapsed.\n",
      "\n",
      "callbacks.streamlit.streamlit_callback_handler.LLMThought(...)\n",
      "A thought in the LLM's thought stream.\n",
      "\n",
      "callbacks.streamlit.streamlit_callback_handler.LLMThoughtLabeler()\n",
      "Generates markdown labels for LLMThought containers.\n",
      "\n",
      "callbacks.streamlit.streamlit_callback_handler.LLMThoughtState(value)\n",
      "Enumerator of the LLMThought state.\n",
      "\n",
      "callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler(...)\n",
      "A callback handler that writes to a Streamlit app.\n",
      "\n",
      "callbacks.streamlit.streamlit_callback_handler.ToolRecord(...)\n",
      "The tool record as a NamedTuple.\n",
      "\n",
      "callbacks.tracers.wandb.RunProcessor(...)\n",
      "Handles the conversion of a LangChain Runs into a WBTraceTree.\n",
      "\n",
      "callbacks.tracers.wandb.WandbRunArgs\n",
      "Arguments for the WandbTracer.\n",
      "\n",
      "callbacks.tracers.wandb.WandbTracer([run_args])\n",
      "Callback Handler that logs to Weights and Biases.\n",
      "\n",
      "callbacks.trubrics_callback.TrubricsCallbackHandler([...])\n",
      "Callback handler for Trubrics.\n",
      "\n",
      "callbacks.utils.BaseMetadataCallbackHandler()\n",
      "This class handles the metadata and associated function states for callbacks.\n",
      "\n",
      "callbacks.wandb_callback.WandbCallbackHandler([...])\n",
      "Callback Handler that logs to Weights and Biases.\n",
      "\n",
      "callbacks.whylabs_callback.WhyLabsCallbackHandler(...)\n",
      "Callback Handler for logging to WhyLabs.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "callbacks.aim_callback.import_aim()\n",
      "Import the aim python package and raise an error if it is not installed.\n",
      "\n",
      "callbacks.clearml_callback.import_clearml()\n",
      "Import the clearml python package and raise an error if it is not installed.\n",
      "\n",
      "callbacks.comet_ml_callback.import_comet_ml()\n",
      "Import comet_ml and raise an error if it is not installed.\n",
      "\n",
      "callbacks.context_callback.import_context()\n",
      "Import the getcontext package.\n",
      "\n",
      "callbacks.flyte_callback.analyze_text(text)\n",
      "Analyze text using textstat and spacy.\n",
      "\n",
      "callbacks.flyte_callback.import_flytekit()\n",
      "Import flytekit and flytekitplugins-deck-standard.\n",
      "\n",
      "callbacks.infino_callback.get_num_tokens(...)\n",
      "Calculate num tokens for OpenAI with tiktoken package.\n",
      "\n",
      "callbacks.infino_callback.import_infino()\n",
      "Import the infino client.\n",
      "\n",
      "callbacks.infino_callback.import_tiktoken()\n",
      "Import tiktoken for counting tokens for OpenAI models.\n",
      "\n",
      "callbacks.labelstudio_callback.get_default_label_configs(mode)\n",
      "Get default Label Studio configs for the given mode.\n",
      "\n",
      "callbacks.llmonitor_callback.identify(user_id)\n",
      "Builds an LLMonitor UserContextManager\n",
      "\n",
      "callbacks.manager.get_openai_callback()\n",
      "Get the OpenAI callback handler in a context manager.\n",
      "\n",
      "callbacks.manager.wandb_tracing_enabled([...])\n",
      "Get the WandbTracer in a context manager.\n",
      "\n",
      "callbacks.mlflow_callback.analyze_text(text)\n",
      "Analyze text using textstat and spacy.\n",
      "\n",
      "callbacks.mlflow_callback.construct_html_from_prompt_and_generation(...)\n",
      "Construct an html element from a prompt and a generation.\n",
      "\n",
      "callbacks.mlflow_callback.import_mlflow()\n",
      "Import the mlflow python package and raise an error if it is not installed.\n",
      "\n",
      "callbacks.openai_info.get_openai_token_cost_for_model(...)\n",
      "Get the cost in USD for a given model and number of tokens.\n",
      "\n",
      "callbacks.openai_info.standardize_model_name(...)\n",
      "Standardize the model name to a format that can be used in the OpenAI API.\n",
      "\n",
      "callbacks.sagemaker_callback.save_json(data, ...)\n",
      "Save dict to local file path.\n",
      "\n",
      "callbacks.utils.flatten_dict(nested_dict[, ...])\n",
      "Flattens a nested dictionary into a flat dictionary.\n",
      "\n",
      "callbacks.utils.hash_string(s)\n",
      "Hash a string using sha1.\n",
      "\n",
      "callbacks.utils.import_pandas()\n",
      "Import the pandas python package and raise an error if it is not installed.\n",
      "\n",
      "callbacks.utils.import_spacy()\n",
      "Import the spacy python package and raise an error if it is not installed.\n",
      "\n",
      "callbacks.utils.import_textstat()\n",
      "Import the textstat python package and raise an error if it is not installed.\n",
      "\n",
      "callbacks.utils.load_json(json_path)\n",
      "Load json file to a string.\n",
      "\n",
      "callbacks.wandb_callback.analyze_text(text)\n",
      "Analyze text using textstat and spacy.\n",
      "\n",
      "callbacks.wandb_callback.construct_html_from_prompt_and_generation(...)\n",
      "Construct an html element from a prompt and a generation.\n",
      "\n",
      "callbacks.wandb_callback.import_wandb()\n",
      "Import the wandb python package and raise an error if it is not installed.\n",
      "\n",
      "callbacks.wandb_callback.load_json_to_dict(...)\n",
      "Load json file to a dictionary.\n",
      "\n",
      "callbacks.whylabs_callback.import_langkit([...])\n",
      "Import the langkit python package and raise an error if it is not installed.\n",
      "\n",
      "langchain.chains¶\n",
      "Chains are easily reusable components linked together.\n",
      "Chains encode a sequence of calls to components like models, document retrievers,\n",
      "other Chains, etc., and provide a simple interface to this sequence.\n",
      "The Chain interface makes it easy to create apps that are:\n",
      "\n",
      "Stateful: add Memory to any Chain to give it state,\n",
      "Observable: pass Callbacks to a Chain to execute additional functionality,\n",
      "like logging, outside the main sequence of component calls,\n",
      "Composable: combine Chains with other components, including other Chains.\n",
      "\n",
      "Class hierarchy:\n",
      "Chain --> <name>Chain  # Examples: LLMChain, MapReduceChain, RouterChain\n",
      "\n",
      "Classes¶\n",
      "\n",
      "chains.api.base.APIChain\n",
      "Chain that makes API calls and summarizes the responses to answer a question.\n",
      "\n",
      "chains.api.openapi.chain.OpenAPIEndpointChain\n",
      "Chain interacts with an OpenAPI endpoint using natural language.\n",
      "\n",
      "chains.api.openapi.requests_chain.APIRequesterChain\n",
      "Get the request parser.\n",
      "\n",
      "chains.api.openapi.requests_chain.APIRequesterOutputParser\n",
      "Parse the request and error tags.\n",
      "\n",
      "chains.api.openapi.response_chain.APIResponderChain\n",
      "Get the response parser.\n",
      "\n",
      "chains.api.openapi.response_chain.APIResponderOutputParser\n",
      "Parse the response and error tags.\n",
      "\n",
      "chains.base.Chain\n",
      "Abstract base class for creating structured sequences of calls to components.\n",
      "\n",
      "chains.combine_documents.base.AnalyzeDocumentChain\n",
      "Chain that splits documents, then analyzes it in pieces.\n",
      "\n",
      "chains.combine_documents.base.BaseCombineDocumentsChain\n",
      "Base interface for chains combining documents.\n",
      "\n",
      "chains.combine_documents.map_reduce.MapReduceDocumentsChain\n",
      "Combining documents by mapping a chain over them, then combining results.\n",
      "\n",
      "chains.combine_documents.map_rerank.MapRerankDocumentsChain\n",
      "Combining documents by mapping a chain over them, then reranking results.\n",
      "\n",
      "chains.combine_documents.reduce.AsyncCombineDocsProtocol(...)\n",
      "Interface for the combine_docs method.\n",
      "\n",
      "chains.combine_documents.reduce.CombineDocsProtocol(...)\n",
      "Interface for the combine_docs method.\n",
      "\n",
      "chains.combine_documents.reduce.ReduceDocumentsChain\n",
      "Combine documents by recursively reducing them.\n",
      "\n",
      "chains.combine_documents.refine.RefineDocumentsChain\n",
      "Combine documents by doing a first pass and then refining on more documents.\n",
      "\n",
      "chains.combine_documents.stuff.StuffDocumentsChain\n",
      "Chain that combines documents by stuffing into context.\n",
      "\n",
      "chains.constitutional_ai.base.ConstitutionalChain\n",
      "Chain for applying constitutional principles.\n",
      "\n",
      "chains.constitutional_ai.models.ConstitutionalPrinciple\n",
      "Class for a constitutional principle.\n",
      "\n",
      "chains.conversation.base.ConversationChain\n",
      "Chain to have a conversation and load context from memory.\n",
      "\n",
      "chains.conversational_retrieval.base.BaseConversationalRetrievalChain\n",
      "Chain for chatting with an index.\n",
      "\n",
      "chains.conversational_retrieval.base.ChatVectorDBChain\n",
      "Chain for chatting with a vector database.\n",
      "\n",
      "chains.conversational_retrieval.base.ConversationalRetrievalChain\n",
      "Chain for having a conversation based on retrieved documents.\n",
      "\n",
      "chains.conversational_retrieval.base.InputType\n",
      "Create a new model by parsing and validating input data from keyword arguments.\n",
      "\n",
      "chains.elasticsearch_database.base.ElasticsearchDatabaseChain\n",
      "Chain for interacting with Elasticsearch Database.\n",
      "\n",
      "chains.flare.base.FlareChain\n",
      "Chain that combines a retriever, a question generator, and a response generator.\n",
      "\n",
      "chains.flare.base.QuestionGeneratorChain\n",
      "Chain that generates questions from uncertain spans.\n",
      "\n",
      "chains.flare.prompts.FinishedOutputParser\n",
      "Output parser that checks if the output is finished.\n",
      "\n",
      "chains.graph_qa.arangodb.ArangoGraphQAChain\n",
      "Chain for question-answering against a graph by generating AQL statements.\n",
      "\n",
      "chains.graph_qa.base.GraphQAChain\n",
      "Chain for question-answering against a graph.\n",
      "\n",
      "chains.graph_qa.cypher.GraphCypherQAChain\n",
      "Chain for question-answering against a graph by generating Cypher statements.\n",
      "\n",
      "chains.graph_qa.cypher_utils.CypherQueryCorrector(schemas)\n",
      "Used to correct relationship direction in generated Cypher statements.\n",
      "\n",
      "chains.graph_qa.cypher_utils.Schema(...)\n",
      "Create new instance of Schema(left_node, relation, right_node)\n",
      "\n",
      "chains.graph_qa.falkordb.FalkorDBQAChain\n",
      "Chain for question-answering against a graph by generating Cypher statements.\n",
      "\n",
      "chains.graph_qa.hugegraph.HugeGraphQAChain\n",
      "Chain for question-answering against a graph by generating gremlin statements.\n",
      "\n",
      "chains.graph_qa.kuzu.KuzuQAChain\n",
      "Question-answering against a graph by generating Cypher statements for Kùzu.\n",
      "\n",
      "chains.graph_qa.nebulagraph.NebulaGraphQAChain\n",
      "Chain for question-answering against a graph by generating nGQL statements.\n",
      "\n",
      "chains.graph_qa.neptune_cypher.NeptuneOpenCypherQAChain\n",
      "Chain for question-answering against a Neptune graph by generating openCypher statements.\n",
      "\n",
      "chains.graph_qa.sparql.GraphSparqlQAChain\n",
      "Question-answering against an RDF or OWL graph by generating SPARQL statements.\n",
      "\n",
      "chains.hyde.base.HypotheticalDocumentEmbedder\n",
      "Generate hypothetical document for query, and then embed that.\n",
      "\n",
      "chains.llm.LLMChain\n",
      "Chain to run queries against LLMs.\n",
      "\n",
      "chains.llm_checker.base.LLMCheckerChain\n",
      "Chain for question-answering with self-verification.\n",
      "\n",
      "chains.llm_math.base.LLMMathChain\n",
      "Chain that interprets a prompt and executes python code to do math.\n",
      "\n",
      "chains.llm_requests.LLMRequestsChain\n",
      "Chain that requests a URL and then uses an LLM to parse results.\n",
      "\n",
      "chains.llm_summarization_checker.base.LLMSummarizationCheckerChain\n",
      "Chain for question-answering with self-verification.\n",
      "\n",
      "chains.mapreduce.MapReduceChain\n",
      "Map-reduce chain.\n",
      "\n",
      "chains.moderation.OpenAIModerationChain\n",
      "Pass input through a moderation endpoint.\n",
      "\n",
      "chains.natbot.base.NatBotChain\n",
      "Implement an LLM driven browser.\n",
      "\n",
      "chains.natbot.crawler.Crawler()\n",
      "A crawler for web pages.\n",
      "\n",
      "chains.natbot.crawler.ElementInViewPort\n",
      "A typed dictionary containing information about elements in the viewport.\n",
      "\n",
      "chains.openai_functions.citation_fuzzy_match.FactWithEvidence\n",
      "Class representing a single statement.\n",
      "\n",
      "chains.openai_functions.citation_fuzzy_match.QuestionAnswer\n",
      "A question and its answer as a list of facts each one should have a source.\n",
      "\n",
      "chains.openai_functions.openapi.SimpleRequestChain\n",
      "Chain for making a simple request to an API endpoint.\n",
      "\n",
      "chains.openai_functions.qa_with_structure.AnswerWithSources\n",
      "An answer to the question, with sources.\n",
      "\n",
      "chains.prompt_selector.BasePromptSelector\n",
      "Base class for prompt selectors.\n",
      "\n",
      "chains.prompt_selector.ConditionalPromptSelector\n",
      "Prompt collection that goes through conditionals.\n",
      "\n",
      "chains.qa_generation.base.QAGenerationChain\n",
      "Base class for question-answer generation chains.\n",
      "\n",
      "chains.qa_with_sources.base.BaseQAWithSourcesChain\n",
      "Question answering chain with sources over documents.\n",
      "\n",
      "chains.qa_with_sources.base.QAWithSourcesChain\n",
      "Question answering with sources over documents.\n",
      "\n",
      "chains.qa_with_sources.loading.LoadingCallable(...)\n",
      "Interface for loading the combine documents chain.\n",
      "\n",
      "chains.qa_with_sources.retrieval.RetrievalQAWithSourcesChain\n",
      "Question-answering with sources over an index.\n",
      "\n",
      "chains.qa_with_sources.vector_db.VectorDBQAWithSourcesChain\n",
      "Question-answering with sources over a vector database.\n",
      "\n",
      "chains.query_constructor.base.StructuredQueryOutputParser\n",
      "Output parser that parses a structured query.\n",
      "\n",
      "chains.query_constructor.ir.Comparator(value)\n",
      "Enumerator of the comparison operators.\n",
      "\n",
      "chains.query_constructor.ir.Comparison\n",
      "A comparison to a value.\n",
      "\n",
      "chains.query_constructor.ir.Expr\n",
      "Base class for all expressions.\n",
      "\n",
      "chains.query_constructor.ir.FilterDirective\n",
      "A filtering expression.\n",
      "\n",
      "chains.query_constructor.ir.Operation\n",
      "A logical operation over other directives.\n",
      "\n",
      "chains.query_constructor.ir.Operator(value)\n",
      "Enumerator of the operations.\n",
      "\n",
      "chains.query_constructor.ir.StructuredQuery\n",
      "A structured query.\n",
      "\n",
      "chains.query_constructor.ir.Visitor()\n",
      "Defines interface for IR translation using visitor pattern.\n",
      "\n",
      "chains.query_constructor.parser.ISO8601Date\n",
      "A date in ISO 8601 format (YYYY-MM-DD).\n",
      "\n",
      "chains.query_constructor.schema.AttributeInfo\n",
      "Information about a data source attribute.\n",
      "\n",
      "chains.retrieval_qa.base.BaseRetrievalQA\n",
      "Base class for question-answering chains.\n",
      "\n",
      "chains.retrieval_qa.base.RetrievalQA\n",
      "Chain for question-answering against an index.\n",
      "\n",
      "chains.retrieval_qa.base.VectorDBQA\n",
      "Chain for question-answering against a vector database.\n",
      "\n",
      "chains.router.base.MultiRouteChain\n",
      "Use a single chain to route an input to one of multiple candidate chains.\n",
      "\n",
      "chains.router.base.Route(destination, ...)\n",
      "Create new instance of Route(destination, next_inputs)\n",
      "\n",
      "chains.router.base.RouterChain\n",
      "Chain that outputs the name of a destination chain and the inputs to it.\n",
      "\n",
      "chains.router.embedding_router.EmbeddingRouterChain\n",
      "Chain that uses embeddings to route between options.\n",
      "\n",
      "chains.router.llm_router.LLMRouterChain\n",
      "A router chain that uses an LLM chain to perform routing.\n",
      "\n",
      "chains.router.llm_router.RouterOutputParser\n",
      "Parser for output of router chain in the multi-prompt chain.\n",
      "\n",
      "chains.router.multi_prompt.MultiPromptChain\n",
      "A multi-route chain that uses an LLM router chain to choose amongst prompts.\n",
      "\n",
      "chains.router.multi_retrieval_qa.MultiRetrievalQAChain\n",
      "A multi-route chain that uses an LLM router chain to choose amongst retrieval qa chains.\n",
      "\n",
      "chains.sequential.SequentialChain\n",
      "Chain where the outputs of one chain feed directly into next.\n",
      "\n",
      "chains.sequential.SimpleSequentialChain\n",
      "Simple chain where the outputs of one step feed directly into next.\n",
      "\n",
      "chains.sql_database.query.SQLInput\n",
      "Input for a SQL Chain.\n",
      "\n",
      "chains.sql_database.query.SQLInputWithTables\n",
      "Input for a SQL Chain.\n",
      "\n",
      "chains.transform.TransformChain\n",
      "Chain that transforms the chain output.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "chains.combine_documents.reduce.acollapse_docs(...)\n",
      "Execute a collapse function on a set of documents and merge their metadatas.\n",
      "\n",
      "chains.combine_documents.reduce.collapse_docs(...)\n",
      "Execute a collapse function on a set of documents and merge their metadatas.\n",
      "\n",
      "chains.combine_documents.reduce.split_list_of_docs(...)\n",
      "Split Documents into subsets that each meet a cumulative length constraint.\n",
      "\n",
      "chains.ernie_functions.base.convert_python_function_to_ernie_function(...)\n",
      "Convert a Python function to an Ernie function-calling API compatible dict.\n",
      "\n",
      "chains.ernie_functions.base.convert_to_ernie_function(...)\n",
      "Convert a raw function/class to an Ernie function.\n",
      "\n",
      "chains.ernie_functions.base.create_ernie_fn_chain(...)\n",
      "[Legacy] Create an LLM chain that uses Ernie functions.\n",
      "\n",
      "chains.ernie_functions.base.create_ernie_fn_runnable(...)\n",
      "Create a runnable sequence that uses Ernie functions.\n",
      "\n",
      "chains.ernie_functions.base.create_structured_output_chain(...)\n",
      "[Legacy] Create an LLMChain that uses an Ernie function to get a structured output.\n",
      "\n",
      "chains.ernie_functions.base.create_structured_output_runnable(...)\n",
      "Create a runnable that uses an Ernie function to get a structured output.\n",
      "\n",
      "chains.ernie_functions.base.get_ernie_output_parser(...)\n",
      "Get the appropriate function output parser given the user functions.\n",
      "\n",
      "chains.example_generator.generate_example(...)\n",
      "Return another example given a list of examples for a prompt.\n",
      "\n",
      "chains.graph_qa.cypher.construct_schema(...)\n",
      "Filter the schema based on included or excluded types\n",
      "\n",
      "chains.graph_qa.cypher.extract_cypher(text)\n",
      "Extract Cypher code from a text.\n",
      "\n",
      "chains.graph_qa.falkordb.extract_cypher(text)\n",
      "Extract Cypher code from a text.\n",
      "\n",
      "chains.graph_qa.neptune_cypher.extract_cypher(text)\n",
      "Extract Cypher code from text using Regex.\n",
      "\n",
      "chains.graph_qa.neptune_cypher.trim_query(query)\n",
      "Trim the query to only include Cypher keywords.\n",
      "\n",
      "chains.graph_qa.neptune_cypher.use_simple_prompt(llm)\n",
      "Decides whether to use the simple prompt\n",
      "\n",
      "chains.loading.load_chain(path, **kwargs)\n",
      "Unified method for loading a chain from LangChainHub or local fs.\n",
      "\n",
      "chains.loading.load_chain_from_config(...)\n",
      "Load chain from Config Dict.\n",
      "\n",
      "chains.openai_functions.base.convert_python_function_to_openai_function(...)\n",
      "Convert a Python function to an OpenAI function-calling API compatible dict.\n",
      "\n",
      "chains.openai_functions.base.convert_to_openai_function(...)\n",
      "Convert a raw function/class to an OpenAI function.\n",
      "\n",
      "chains.openai_functions.base.create_openai_fn_chain(...)\n",
      "[Legacy] Create an LLM chain that uses OpenAI functions.\n",
      "\n",
      "chains.openai_functions.base.create_openai_fn_runnable(...)\n",
      "Create a runnable sequence that uses OpenAI functions.\n",
      "\n",
      "chains.openai_functions.base.create_structured_output_chain(...)\n",
      "[Legacy] Create an LLMChain that uses an OpenAI function to get a structured output.\n",
      "\n",
      "chains.openai_functions.base.create_structured_output_runnable(...)\n",
      "Create a runnable that uses an OpenAI function to get a structured output.\n",
      "\n",
      "chains.openai_functions.base.get_openai_output_parser(...)\n",
      "Get the appropriate function output parser given the user functions.\n",
      "\n",
      "chains.openai_functions.citation_fuzzy_match.create_citation_fuzzy_match_chain(llm)\n",
      "Create a citation fuzzy match chain.\n",
      "\n",
      "chains.openai_functions.extraction.create_extraction_chain(...)\n",
      "Creates a chain that extracts information from a passage.\n",
      "\n",
      "chains.openai_functions.extraction.create_extraction_chain_pydantic(...)\n",
      "Creates a chain that extracts information from a passage using pydantic schema.\n",
      "\n",
      "chains.openai_functions.openapi.get_openapi_chain(spec)\n",
      "Create a chain for querying an API from a OpenAPI spec.\n",
      "\n",
      "chains.openai_functions.openapi.openapi_spec_to_openai_fn(spec)\n",
      "Convert a valid OpenAPI spec to the JSON Schema format expected for OpenAI\n",
      "\n",
      "chains.openai_functions.qa_with_structure.create_qa_with_sources_chain(llm)\n",
      "Create a question answering chain that returns an answer with sources.\n",
      "\n",
      "chains.openai_functions.qa_with_structure.create_qa_with_structure_chain(...)\n",
      "Create a question answering chain that returns an answer with sources\n",
      "\n",
      "chains.openai_functions.tagging.create_tagging_chain(...)\n",
      "Creates a chain that extracts information from a passage\n",
      "\n",
      "chains.openai_functions.tagging.create_tagging_chain_pydantic(...)\n",
      "Creates a chain that extracts information from a passage\n",
      "\n",
      "chains.openai_functions.utils.get_llm_kwargs(...)\n",
      "Returns the kwargs for the LLMChain constructor.\n",
      "\n",
      "chains.openai_tools.extraction.create_extraction_chain_pydantic(...)\n",
      "\n",
      "chains.prompt_selector.is_chat_model(llm)\n",
      "Check if the language model is a chat model.\n",
      "\n",
      "chains.prompt_selector.is_llm(llm)\n",
      "Check if the language model is a LLM.\n",
      "\n",
      "chains.qa_with_sources.loading.load_qa_with_sources_chain(llm)\n",
      "Load a question answering with sources chain.\n",
      "\n",
      "chains.query_constructor.base.construct_examples(...)\n",
      "Construct examples from input-output pairs.\n",
      "\n",
      "chains.query_constructor.base.fix_filter_directive(...)\n",
      "Fix invalid filter directive.\n",
      "\n",
      "chains.query_constructor.base.get_query_constructor_prompt(...)\n",
      "Create query construction prompt.\n",
      "\n",
      "chains.query_constructor.base.load_query_constructor_chain(...)\n",
      "Load a query constructor chain.\n",
      "\n",
      "chains.query_constructor.base.load_query_constructor_runnable(...)\n",
      "Load a query constructor runnable chain.\n",
      "\n",
      "chains.query_constructor.parser.get_parser([...])\n",
      "Returns a parser for the query language.\n",
      "\n",
      "chains.query_constructor.parser.v_args(...)\n",
      "Dummy decorator for when lark is not installed.\n",
      "\n",
      "chains.sql_database.query.create_sql_query_chain(llm, db)\n",
      "Create a chain that generates SQL queries.\n",
      "\n",
      "langchain.chat_loaders¶\n",
      "Chat Loaders load chat messages from common communications platforms.\n",
      "Load chat messages from various\n",
      "communications platforms such as Facebook Messenger, Telegram, and\n",
      "WhatsApp. The loaded chat messages can be used for fine-tuning models.\n",
      "Class hierarchy:\n",
      "BaseChatLoader --> <name>ChatLoader  # Examples: WhatsAppChatLoader, IMessageChatLoader\n",
      "\n",
      "Main helpers:\n",
      "ChatSession\n",
      "\n",
      "Classes¶\n",
      "\n",
      "chat_loaders.base.BaseChatLoader()\n",
      "Base class for chat loaders.\n",
      "\n",
      "chat_loaders.facebook_messenger.FolderFacebookMessengerChatLoader(path)\n",
      "Load Facebook Messenger chat data from a folder.\n",
      "\n",
      "chat_loaders.facebook_messenger.SingleFileFacebookMessengerChatLoader(path)\n",
      "Load Facebook Messenger chat data from a single file.\n",
      "\n",
      "chat_loaders.gmail.GMailLoader(creds[, n, ...])\n",
      "Load data from GMail.\n",
      "\n",
      "chat_loaders.imessage.IMessageChatLoader([path])\n",
      "Load chat sessions from the iMessage chat.db SQLite file.\n",
      "\n",
      "chat_loaders.langsmith.LangSmithDatasetChatLoader(*, ...)\n",
      "Load chat sessions from a LangSmith dataset with the \"chat\" data type.\n",
      "\n",
      "chat_loaders.langsmith.LangSmithRunChatLoader(runs)\n",
      "Load chat sessions from a list of LangSmith \"llm\" runs.\n",
      "\n",
      "chat_loaders.slack.SlackChatLoader(path)\n",
      "Load Slack conversations from a dump zip file.\n",
      "\n",
      "chat_loaders.telegram.TelegramChatLoader(path)\n",
      "Load telegram conversations to LangChain chat messages.\n",
      "\n",
      "chat_loaders.whatsapp.WhatsAppChatLoader(path)\n",
      "Load WhatsApp conversations from a dump zip file or directory.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "chat_loaders.utils.map_ai_messages(...)\n",
      "Convert messages from the specified 'sender' to AI messages.\n",
      "\n",
      "chat_loaders.utils.map_ai_messages_in_session(...)\n",
      "Convert messages from the specified 'sender' to AI messages.\n",
      "\n",
      "chat_loaders.utils.merge_chat_runs(chat_sessions)\n",
      "Merge chat runs together.\n",
      "\n",
      "chat_loaders.utils.merge_chat_runs_in_session(...)\n",
      "Merge chat runs together in a chat session.\n",
      "\n",
      "langchain.chat_models¶\n",
      "Chat Models are a variation on language models.\n",
      "While Chat Models use language models under the hood, the interface they expose\n",
      "is a bit different. Rather than expose a “text in, text out” API, they expose\n",
      "an interface where “chat messages” are the inputs and outputs.\n",
      "Class hierarchy:\n",
      "BaseLanguageModel --> BaseChatModel --> <name>  # Examples: ChatOpenAI, ChatGooglePalm\n",
      "\n",
      "Main helpers:\n",
      "AIMessage, BaseMessage, HumanMessage\n",
      "\n",
      "Classes¶\n",
      "\n",
      "chat_models.anthropic.ChatAnthropic\n",
      "Anthropic chat large language models.\n",
      "\n",
      "chat_models.anyscale.ChatAnyscale\n",
      "Anyscale Chat large language models.\n",
      "\n",
      "chat_models.azure_openai.AzureChatOpenAI\n",
      "Azure OpenAI Chat Completion API.\n",
      "\n",
      "chat_models.azureml_endpoint.AzureMLChatOnlineEndpoint\n",
      "AzureML Chat models API.\n",
      "\n",
      "chat_models.azureml_endpoint.LlamaContentFormatter()\n",
      "Content formatter for LLaMA.\n",
      "\n",
      "chat_models.baichuan.ChatBaichuan\n",
      "Baichuan chat models API by Baichuan Intelligent Technology.\n",
      "\n",
      "chat_models.baidu_qianfan_endpoint.QianfanChatEndpoint\n",
      "Baidu Qianfan chat models.\n",
      "\n",
      "chat_models.bedrock.BedrockChat\n",
      "A chat model that uses the Bedrock API.\n",
      "\n",
      "chat_models.bedrock.ChatPromptAdapter()\n",
      "Adapter class to prepare the inputs from Langchain to prompt format that Chat model expects.\n",
      "\n",
      "chat_models.cohere.ChatCohere\n",
      "Cohere chat large language models.\n",
      "\n",
      "chat_models.ernie.ErnieBotChat\n",
      "ERNIE-Bot large language model.\n",
      "\n",
      "chat_models.everlyai.ChatEverlyAI\n",
      "EverlyAI Chat large language models.\n",
      "\n",
      "chat_models.fake.FakeListChatModel\n",
      "Fake ChatModel for testing purposes.\n",
      "\n",
      "chat_models.fake.FakeMessagesListChatModel\n",
      "Fake ChatModel for testing purposes.\n",
      "\n",
      "chat_models.fireworks.ChatFireworks\n",
      "Fireworks Chat models.\n",
      "\n",
      "chat_models.gigachat.GigaChat\n",
      "GigaChat large language models API.\n",
      "\n",
      "chat_models.google_palm.ChatGooglePalm\n",
      "Google PaLM Chat models API.\n",
      "\n",
      "chat_models.google_palm.ChatGooglePalmError\n",
      "Error with the Google PaLM API.\n",
      "\n",
      "chat_models.human.HumanInputChatModel\n",
      "ChatModel which returns user input as the response.\n",
      "\n",
      "chat_models.hunyuan.ChatHunyuan\n",
      "Tencent Hunyuan chat models API by Tencent.\n",
      "\n",
      "chat_models.javelin_ai_gateway.ChatJavelinAIGateway\n",
      "Javelin AI Gateway chat models API.\n",
      "\n",
      "chat_models.javelin_ai_gateway.ChatParams\n",
      "Parameters for the Javelin AI Gateway LLM.\n",
      "\n",
      "chat_models.jinachat.JinaChat\n",
      "Jina AI Chat models API.\n",
      "\n",
      "chat_models.konko.ChatKonko\n",
      "ChatKonko Chat large language models API.\n",
      "\n",
      "chat_models.litellm.ChatLiteLLM\n",
      "A chat model that uses the LiteLLM API.\n",
      "\n",
      "chat_models.litellm.ChatLiteLLMException\n",
      "Error with the LiteLLM I/O library\n",
      "\n",
      "chat_models.minimax.MiniMaxChat\n",
      "Wrapper around Minimax large language models.\n",
      "\n",
      "chat_models.mlflow_ai_gateway.ChatMLflowAIGateway\n",
      "MLflow AI Gateway chat models API.\n",
      "\n",
      "chat_models.mlflow_ai_gateway.ChatParams\n",
      "Parameters for the MLflow AI Gateway LLM.\n",
      "\n",
      "chat_models.ollama.ChatOllama\n",
      "Ollama locally runs large language models.\n",
      "\n",
      "chat_models.openai.ChatOpenAI\n",
      "OpenAI Chat large language models API.\n",
      "\n",
      "chat_models.pai_eas_endpoint.PaiEasChatEndpoint\n",
      "Eas LLM Service chat model API.\n",
      "\n",
      "chat_models.promptlayer_openai.PromptLayerChatOpenAI\n",
      "PromptLayer and OpenAI Chat large language models API.\n",
      "\n",
      "chat_models.tongyi.ChatTongyi\n",
      "Alibaba Tongyi Qwen chat models API.\n",
      "\n",
      "chat_models.vertexai.ChatVertexAI\n",
      "Vertex AI Chat large language models API.\n",
      "\n",
      "chat_models.yandex.ChatYandexGPT\n",
      "Wrapper around YandexGPT large language models.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "chat_models.anthropic.convert_messages_to_prompt_anthropic(...)\n",
      "Format a list of messages into a full prompt for the Anthropic model\n",
      "\n",
      "chat_models.baidu_qianfan_endpoint.convert_message_to_dict(message)\n",
      "Convert a message to a dictionary that can be passed to the API.\n",
      "\n",
      "chat_models.cohere.get_cohere_chat_request(...)\n",
      "Get the request for the Cohere chat API.\n",
      "\n",
      "chat_models.cohere.get_role(message)\n",
      "Get the role of the message.\n",
      "\n",
      "chat_models.fireworks.acompletion_with_retry(...)\n",
      "Use tenacity to retry the async completion call.\n",
      "\n",
      "chat_models.fireworks.acompletion_with_retry_streaming(...)\n",
      "Use tenacity to retry the completion call for streaming.\n",
      "\n",
      "chat_models.fireworks.completion_with_retry(...)\n",
      "Use tenacity to retry the completion call.\n",
      "\n",
      "chat_models.fireworks.conditional_decorator(...)\n",
      "\n",
      "chat_models.fireworks.convert_dict_to_message(_dict)\n",
      "Convert a dict response to a message.\n",
      "\n",
      "chat_models.google_palm.achat_with_retry(...)\n",
      "Use tenacity to retry the async completion call.\n",
      "\n",
      "chat_models.google_palm.chat_with_retry(llm, ...)\n",
      "Use tenacity to retry the completion call.\n",
      "\n",
      "chat_models.jinachat.acompletion_with_retry(...)\n",
      "Use tenacity to retry the async completion call.\n",
      "\n",
      "chat_models.litellm.acompletion_with_retry(llm)\n",
      "Use tenacity to retry the async completion call.\n",
      "\n",
      "chat_models.meta.convert_messages_to_prompt_llama(...)\n",
      "\n",
      "chat_models.openai.acompletion_with_retry(llm)\n",
      "Use tenacity to retry the async completion call.\n",
      "\n",
      "chat_models.tongyi.convert_dict_to_message(_dict)\n",
      "\n",
      "chat_models.tongyi.convert_message_to_dict(message)\n",
      "\n",
      "langchain.docstore¶\n",
      "Docstores are classes to store and load Documents.\n",
      "The Docstore is a simplified version of the Document Loader.\n",
      "Class hierarchy:\n",
      "Docstore --> <name> # Examples: InMemoryDocstore, Wikipedia\n",
      "\n",
      "Main helpers:\n",
      "Document, AddableMixin\n",
      "\n",
      "Classes¶\n",
      "\n",
      "docstore.arbitrary_fn.DocstoreFn(lookup_fn)\n",
      "Langchain Docstore via arbitrary lookup function.\n",
      "\n",
      "docstore.base.AddableMixin()\n",
      "Mixin class that supports adding texts.\n",
      "\n",
      "docstore.base.Docstore()\n",
      "Interface to access to place that stores documents.\n",
      "\n",
      "docstore.in_memory.InMemoryDocstore([_dict])\n",
      "Simple in memory docstore in the form of a dict.\n",
      "\n",
      "docstore.wikipedia.Wikipedia()\n",
      "Wrapper around wikipedia API.\n",
      "\n",
      "langchain.document_loaders¶\n",
      "Document Loaders  are classes to load Documents.\n",
      "Document Loaders are usually used to load a lot of Documents in a single run.\n",
      "Class hierarchy:\n",
      "BaseLoader --> <name>Loader  # Examples: TextLoader, UnstructuredFileLoader\n",
      "\n",
      "Main helpers:\n",
      "Document, <name>TextSplitter\n",
      "\n",
      "Classes¶\n",
      "\n",
      "document_loaders.acreom.AcreomLoader(path[, ...])\n",
      "Load acreom vault from a directory.\n",
      "\n",
      "document_loaders.airbyte.AirbyteCDKLoader(...)\n",
      "Load with an Airbyte source connector implemented using the CDK.\n",
      "\n",
      "document_loaders.airbyte.AirbyteGongLoader(...)\n",
      "Load from Gong using an Airbyte source connector.\n",
      "\n",
      "document_loaders.airbyte.AirbyteHubspotLoader(...)\n",
      "Load from Hubspot using an Airbyte source connector.\n",
      "\n",
      "document_loaders.airbyte.AirbyteSalesforceLoader(...)\n",
      "Load from Salesforce using an Airbyte source connector.\n",
      "\n",
      "document_loaders.airbyte.AirbyteShopifyLoader(...)\n",
      "Load from Shopify using an Airbyte source connector.\n",
      "\n",
      "document_loaders.airbyte.AirbyteStripeLoader(...)\n",
      "Load from Stripe using an Airbyte source connector.\n",
      "\n",
      "document_loaders.airbyte.AirbyteTypeformLoader(...)\n",
      "Load from Typeform using an Airbyte source connector.\n",
      "\n",
      "document_loaders.airbyte.AirbyteZendeskSupportLoader(...)\n",
      "Load from Zendesk Support using an Airbyte source connector.\n",
      "\n",
      "document_loaders.airbyte_json.AirbyteJSONLoader(...)\n",
      "Load local Airbyte json files.\n",
      "\n",
      "document_loaders.airtable.AirtableLoader(...)\n",
      "Load the Airtable tables.\n",
      "\n",
      "document_loaders.apify_dataset.ApifyDatasetLoader\n",
      "Load datasets from Apify web scraping, crawling, and data extraction platform.\n",
      "\n",
      "document_loaders.arcgis_loader.ArcGISLoader(layer)\n",
      "Load records from an ArcGIS FeatureLayer.\n",
      "\n",
      "document_loaders.arxiv.ArxivLoader(query[, ...])\n",
      "Load a query result from Arxiv.\n",
      "\n",
      "document_loaders.assemblyai.AssemblyAIAudioTranscriptLoader(...)\n",
      "Loader for AssemblyAI audio transcripts.\n",
      "\n",
      "document_loaders.assemblyai.TranscriptFormat(value)\n",
      "Transcript format to use for the document loader.\n",
      "\n",
      "document_loaders.async_html.AsyncHtmlLoader(...)\n",
      "Load HTML asynchronously.\n",
      "\n",
      "document_loaders.azlyrics.AZLyricsLoader([...])\n",
      "Load AZLyrics webpages.\n",
      "\n",
      "document_loaders.azure_blob_storage_container.AzureBlobStorageContainerLoader(...)\n",
      "Load from Azure Blob Storage container.\n",
      "\n",
      "document_loaders.azure_blob_storage_file.AzureBlobStorageFileLoader(...)\n",
      "Load from Azure Blob Storage files.\n",
      "\n",
      "document_loaders.baiducloud_bos_directory.BaiduBOSDirectoryLoader(...)\n",
      "Load from Baidu BOS directory.\n",
      "\n",
      "document_loaders.baiducloud_bos_file.BaiduBOSFileLoader(...)\n",
      "Load from Baidu Cloud BOS file.\n",
      "\n",
      "document_loaders.base.BaseBlobParser()\n",
      "Abstract interface for blob parsers.\n",
      "\n",
      "document_loaders.base.BaseLoader()\n",
      "Interface for Document Loader.\n",
      "\n",
      "document_loaders.base_o365.O365BaseLoader\n",
      "Base class for all loaders that uses O365 Package\n",
      "\n",
      "document_loaders.bibtex.BibtexLoader(...[, ...])\n",
      "Load a bibtex file.\n",
      "\n",
      "document_loaders.bigquery.BigQueryLoader(query)\n",
      "Load from the Google Cloud Platform BigQuery.\n",
      "\n",
      "document_loaders.bilibili.BiliBiliLoader(...)\n",
      "Load BiliBili video transcripts.\n",
      "\n",
      "document_loaders.blackboard.BlackboardLoader(...)\n",
      "Load a Blackboard course.\n",
      "\n",
      "document_loaders.blob_loaders.file_system.FileSystemBlobLoader(path, *)\n",
      "Load blobs in the local file system.\n",
      "\n",
      "document_loaders.blob_loaders.schema.Blob\n",
      "Blob represents raw data by either reference or value.\n",
      "\n",
      "document_loaders.blob_loaders.schema.BlobLoader()\n",
      "Abstract interface for blob loaders implementation.\n",
      "\n",
      "document_loaders.blob_loaders.youtube_audio.YoutubeAudioLoader(...)\n",
      "Load YouTube urls as audio file(s).\n",
      "\n",
      "document_loaders.blockchain.BlockchainDocumentLoader(...)\n",
      "Load elements from a blockchain smart contract.\n",
      "\n",
      "document_loaders.blockchain.BlockchainType(value)\n",
      "Enumerator of the supported blockchains.\n",
      "\n",
      "document_loaders.brave_search.BraveSearchLoader(...)\n",
      "Load with Brave Search engine.\n",
      "\n",
      "document_loaders.browserless.BrowserlessLoader(...)\n",
      "Load webpages with Browserless /content endpoint.\n",
      "\n",
      "document_loaders.chatgpt.ChatGPTLoader(log_file)\n",
      "Load conversations from exported ChatGPT data.\n",
      "\n",
      "document_loaders.chromium.AsyncChromiumLoader(urls)\n",
      "Scrape HTML pages from URLs using a headless instance of the Chromium.\n",
      "\n",
      "document_loaders.college_confidential.CollegeConfidentialLoader([...])\n",
      "Load College Confidential webpages.\n",
      "\n",
      "document_loaders.concurrent.ConcurrentLoader(...)\n",
      "Load and pars Documents concurrently.\n",
      "\n",
      "document_loaders.confluence.ConfluenceLoader(url)\n",
      "Load Confluence pages.\n",
      "\n",
      "document_loaders.confluence.ContentFormat(value)\n",
      "Enumerator of the content formats of Confluence page.\n",
      "\n",
      "document_loaders.conllu.CoNLLULoader(file_path)\n",
      "Load CoNLL-U files.\n",
      "\n",
      "document_loaders.csv_loader.CSVLoader(file_path)\n",
      "Load a CSV file into a list of Documents.\n",
      "\n",
      "document_loaders.csv_loader.UnstructuredCSVLoader(...)\n",
      "Load CSV files using Unstructured.\n",
      "\n",
      "document_loaders.cube_semantic.CubeSemanticLoader(...)\n",
      "Load Cube semantic layer metadata.\n",
      "\n",
      "document_loaders.datadog_logs.DatadogLogsLoader(...)\n",
      "Load Datadog logs.\n",
      "\n",
      "document_loaders.dataframe.BaseDataFrameLoader(...)\n",
      "Initialize with dataframe object.\n",
      "\n",
      "document_loaders.dataframe.DataFrameLoader(...)\n",
      "Load Pandas DataFrame.\n",
      "\n",
      "document_loaders.diffbot.DiffbotLoader(...)\n",
      "Load Diffbot json file.\n",
      "\n",
      "document_loaders.directory.DirectoryLoader(...)\n",
      "Load from a directory.\n",
      "\n",
      "document_loaders.discord.DiscordChatLoader(...)\n",
      "Load Discord chat logs.\n",
      "\n",
      "document_loaders.docugami.DocugamiLoader\n",
      "Load from Docugami.\n",
      "\n",
      "document_loaders.docusaurus.DocusaurusLoader(url)\n",
      "Loader that leverages the SitemapLoader to loop through the generated pages of a Docusaurus Documentation website and extracts the content by looking for specific HTML tags.\n",
      "\n",
      "document_loaders.dropbox.DropboxLoader\n",
      "Load files from Dropbox.\n",
      "\n",
      "document_loaders.duckdb_loader.DuckDBLoader(query)\n",
      "Load from DuckDB.\n",
      "\n",
      "document_loaders.email.OutlookMessageLoader(...)\n",
      "Loads Outlook Message files using extract_msg.\n",
      "\n",
      "document_loaders.email.UnstructuredEmailLoader(...)\n",
      "Load email files using Unstructured.\n",
      "\n",
      "document_loaders.embaas.BaseEmbaasLoader\n",
      "Base loader for Embaas document extraction API.\n",
      "\n",
      "document_loaders.embaas.EmbaasBlobLoader\n",
      "Load Embaas blob.\n",
      "\n",
      "document_loaders.embaas.EmbaasDocumentExtractionParameters\n",
      "Parameters for the embaas document extraction API.\n",
      "\n",
      "document_loaders.embaas.EmbaasDocumentExtractionPayload\n",
      "Payload for the Embaas document extraction API.\n",
      "\n",
      "document_loaders.embaas.EmbaasLoader\n",
      "Load from Embaas.\n",
      "\n",
      "document_loaders.epub.UnstructuredEPubLoader(...)\n",
      "Load EPub files using Unstructured.\n",
      "\n",
      "document_loaders.etherscan.EtherscanLoader(...)\n",
      "Load transactions from Ethereum mainnet.\n",
      "\n",
      "document_loaders.evernote.EverNoteLoader(...)\n",
      "Load from EverNote.\n",
      "\n",
      "document_loaders.excel.UnstructuredExcelLoader(...)\n",
      "Load Microsoft Excel files using Unstructured.\n",
      "\n",
      "document_loaders.facebook_chat.FacebookChatLoader(path)\n",
      "Load Facebook Chat messages directory dump.\n",
      "\n",
      "document_loaders.fauna.FaunaLoader(query, ...)\n",
      "Load from FaunaDB.\n",
      "\n",
      "document_loaders.figma.FigmaFileLoader(...)\n",
      "Load Figma file.\n",
      "\n",
      "document_loaders.gcs_directory.GCSDirectoryLoader(...)\n",
      "Load from GCS directory.\n",
      "\n",
      "document_loaders.gcs_file.GCSFileLoader(...)\n",
      "Load from GCS file.\n",
      "\n",
      "document_loaders.generic.GenericLoader(...)\n",
      "Generic Document Loader.\n",
      "\n",
      "document_loaders.geodataframe.GeoDataFrameLoader(...)\n",
      "Load geopandas Dataframe.\n",
      "\n",
      "document_loaders.git.GitLoader(repo_path[, ...])\n",
      "Load Git repository files.\n",
      "\n",
      "document_loaders.gitbook.GitbookLoader(web_page)\n",
      "Load GitBook data.\n",
      "\n",
      "document_loaders.github.BaseGitHubLoader\n",
      "Load GitHub repository Issues.\n",
      "\n",
      "document_loaders.github.GitHubIssuesLoader\n",
      "Load issues of a GitHub repository.\n",
      "\n",
      "document_loaders.google_speech_to_text.GoogleSpeechToTextLoader(...)\n",
      "Loader for Google Cloud Speech-to-Text audio transcripts.\n",
      "\n",
      "document_loaders.googledrive.GoogleDriveLoader\n",
      "Load Google Docs from Google Drive.\n",
      "\n",
      "document_loaders.gutenberg.GutenbergLoader(...)\n",
      "Load from Gutenberg.org.\n",
      "\n",
      "document_loaders.helpers.FileEncoding(...)\n",
      "File encoding as the NamedTuple.\n",
      "\n",
      "document_loaders.hn.HNLoader([web_path, ...])\n",
      "Load Hacker News data.\n",
      "\n",
      "document_loaders.html.UnstructuredHTMLLoader(...)\n",
      "Load HTML files using Unstructured.\n",
      "\n",
      "document_loaders.html_bs.BSHTMLLoader(file_path)\n",
      "Load HTML files and parse them with beautiful soup.\n",
      "\n",
      "document_loaders.hugging_face_dataset.HuggingFaceDatasetLoader(path)\n",
      "Load from Hugging Face Hub datasets.\n",
      "\n",
      "document_loaders.ifixit.IFixitLoader(web_path)\n",
      "Load iFixit repair guides, device wikis and answers.\n",
      "\n",
      "document_loaders.image.UnstructuredImageLoader(...)\n",
      "Load PNG and JPG files using Unstructured.\n",
      "\n",
      "document_loaders.image_captions.ImageCaptionLoader(images)\n",
      "Load image captions.\n",
      "\n",
      "document_loaders.imsdb.IMSDbLoader([...])\n",
      "Load IMSDb webpages.\n",
      "\n",
      "document_loaders.iugu.IuguLoader(resource[, ...])\n",
      "Load from IUGU.\n",
      "\n",
      "document_loaders.joplin.JoplinLoader([...])\n",
      "Load notes from Joplin.\n",
      "\n",
      "document_loaders.json_loader.JSONLoader(...)\n",
      "Load a JSON file using a jq schema.\n",
      "\n",
      "document_loaders.lakefs.LakeFSClient(...)\n",
      "\n",
      "document_loaders.lakefs.LakeFSLoader(...[, ...])\n",
      "Load from lakeFS.\n",
      "\n",
      "document_loaders.lakefs.UnstructuredLakeFSLoader(...)\n",
      "Args:\n",
      "\n",
      "document_loaders.larksuite.LarkSuiteDocLoader(...)\n",
      "Load from LarkSuite (FeiShu).\n",
      "\n",
      "document_loaders.markdown.UnstructuredMarkdownLoader(...)\n",
      "Load Markdown files using Unstructured.\n",
      "\n",
      "document_loaders.mastodon.MastodonTootsLoader(...)\n",
      "Load the Mastodon 'toots'.\n",
      "\n",
      "document_loaders.max_compute.MaxComputeLoader(...)\n",
      "Load from Alibaba Cloud MaxCompute table.\n",
      "\n",
      "document_loaders.mediawikidump.MWDumpLoader(...)\n",
      "Load MediaWiki dump from an XML file.\n",
      "\n",
      "document_loaders.merge.MergedDataLoader(loaders)\n",
      "Merge documents from a list of loaders\n",
      "\n",
      "document_loaders.mhtml.MHTMLLoader(file_path)\n",
      "Parse MHTML files with BeautifulSoup.\n",
      "\n",
      "document_loaders.modern_treasury.ModernTreasuryLoader(...)\n",
      "Load from Modern Treasury.\n",
      "\n",
      "document_loaders.mongodb.MongodbLoader(...)\n",
      "Load MongoDB documents.\n",
      "\n",
      "document_loaders.news.NewsURLLoader(urls[, ...])\n",
      "Load news articles from URLs using Unstructured.\n",
      "\n",
      "document_loaders.notebook.NotebookLoader(path)\n",
      "Load Jupyter notebook (.ipynb) files.\n",
      "\n",
      "document_loaders.notion.NotionDirectoryLoader(path, *)\n",
      "Load Notion directory dump.\n",
      "\n",
      "document_loaders.notiondb.NotionDBLoader(...)\n",
      "Load from Notion DB.\n",
      "\n",
      "document_loaders.nuclia.NucliaLoader(path, ...)\n",
      "Load from any file type using Nuclia Understanding API.\n",
      "\n",
      "document_loaders.obs_directory.OBSDirectoryLoader(...)\n",
      "Load from Huawei OBS directory.\n",
      "\n",
      "document_loaders.obs_file.OBSFileLoader(...)\n",
      "Load from the Huawei OBS file.\n",
      "\n",
      "document_loaders.obsidian.ObsidianLoader(path)\n",
      "Load Obsidian files from directory.\n",
      "\n",
      "document_loaders.odt.UnstructuredODTLoader(...)\n",
      "Load OpenOffice ODT files using Unstructured.\n",
      "\n",
      "document_loaders.onedrive.OneDriveLoader\n",
      "Load from Microsoft OneDrive.\n",
      "\n",
      "document_loaders.onedrive_file.OneDriveFileLoader\n",
      "Load a file from Microsoft OneDrive.\n",
      "\n",
      "document_loaders.open_city_data.OpenCityDataLoader(...)\n",
      "Load from Open City.\n",
      "\n",
      "document_loaders.org_mode.UnstructuredOrgModeLoader(...)\n",
      "Load Org-Mode files using Unstructured.\n",
      "\n",
      "document_loaders.parsers.audio.OpenAIWhisperParser([...])\n",
      "Transcribe and parse audio files.\n",
      "\n",
      "document_loaders.parsers.audio.OpenAIWhisperParserLocal([...])\n",
      "Transcribe and parse audio files with OpenAI Whisper model.\n",
      "\n",
      "document_loaders.parsers.audio.YandexSTTParser(*)\n",
      "Transcribe and parse audio files.\n",
      "\n",
      "document_loaders.parsers.docai.DocAIParser(*)\n",
      "Google Cloud Document AI parser.\n",
      "\n",
      "document_loaders.parsers.docai.DocAIParsingResults(...)\n",
      "A dataclass to store Document AI parsing results.\n",
      "\n",
      "document_loaders.parsers.generic.MimeTypeBasedParser(...)\n",
      "Parser that uses mime-types to parse a blob.\n",
      "\n",
      "document_loaders.parsers.grobid.GrobidParser(...)\n",
      "Load  article PDF files using Grobid.\n",
      "\n",
      "document_loaders.parsers.grobid.ServerUnavailableException\n",
      "Exception raised when the Grobid server is unavailable.\n",
      "\n",
      "document_loaders.parsers.html.bs4.BS4HTMLParser(*)\n",
      "Pparse HTML files using Beautiful Soup.\n",
      "\n",
      "document_loaders.parsers.language.cobol.CobolSegmenter(code)\n",
      "Code segmenter for COBOL.\n",
      "\n",
      "document_loaders.parsers.language.code_segmenter.CodeSegmenter(code)\n",
      "Abstract class for the code segmenter.\n",
      "\n",
      "document_loaders.parsers.language.javascript.JavaScriptSegmenter(code)\n",
      "Code segmenter for JavaScript.\n",
      "\n",
      "document_loaders.parsers.language.language_parser.LanguageParser([...])\n",
      "Parse using the respective programming language syntax.\n",
      "\n",
      "document_loaders.parsers.language.python.PythonSegmenter(code)\n",
      "Code segmenter for Python.\n",
      "\n",
      "document_loaders.parsers.msword.MsWordParser()\n",
      "Parse the Microsoft Word documents from a blob.\n",
      "\n",
      "document_loaders.parsers.pdf.AmazonTextractPDFParser([...])\n",
      "Send PDF files to Amazon Textract and parse them.\n",
      "\n",
      "document_loaders.parsers.pdf.DocumentIntelligenceParser(...)\n",
      "Loads a PDF with Azure Document Intelligence (formerly Forms Recognizer) and chunks at character level.\n",
      "\n",
      "document_loaders.parsers.pdf.PDFMinerParser([...])\n",
      "Parse PDF using PDFMiner.\n",
      "\n",
      "document_loaders.parsers.pdf.PDFPlumberParser([...])\n",
      "Parse PDF with PDFPlumber.\n",
      "\n",
      "document_loaders.parsers.pdf.PyMuPDFParser([...])\n",
      "Parse PDF using PyMuPDF.\n",
      "\n",
      "document_loaders.parsers.pdf.PyPDFParser([...])\n",
      "Load PDF using pypdf\n",
      "\n",
      "document_loaders.parsers.pdf.PyPDFium2Parser([...])\n",
      "Parse PDF with PyPDFium2.\n",
      "\n",
      "document_loaders.parsers.txt.TextParser()\n",
      "Parser for text blobs.\n",
      "\n",
      "document_loaders.pdf.AmazonTextractPDFLoader(...)\n",
      "Load PDF files from a local file system, HTTP or S3.\n",
      "\n",
      "document_loaders.pdf.BasePDFLoader(file_path, *)\n",
      "Base Loader class for PDF files.\n",
      "\n",
      "document_loaders.pdf.DocumentIntelligenceLoader(...)\n",
      "Loads a PDF with Azure Document Intelligence\n",
      "\n",
      "document_loaders.pdf.MathpixPDFLoader(file_path)\n",
      "Load PDF files using Mathpix service.\n",
      "\n",
      "document_loaders.pdf.OnlinePDFLoader(...[, ...])\n",
      "Load online PDF.\n",
      "\n",
      "document_loaders.pdf.PDFMinerLoader(file_path, *)\n",
      "Load PDF files using PDFMiner.\n",
      "\n",
      "document_loaders.pdf.PDFMinerPDFasHTMLLoader(...)\n",
      "Load PDF files as HTML content using PDFMiner.\n",
      "\n",
      "document_loaders.pdf.PDFPlumberLoader(file_path)\n",
      "Load PDF files using pdfplumber.\n",
      "\n",
      "document_loaders.pdf.PyMuPDFLoader(file_path, *)\n",
      "Load PDF files using PyMuPDF.\n",
      "\n",
      "document_loaders.pdf.PyPDFDirectoryLoader(path)\n",
      "Load a directory with PDF files using pypdf and chunks at character level.\n",
      "\n",
      "document_loaders.pdf.PyPDFLoader(file_path)\n",
      "Load PDF using pypdf into list of documents.\n",
      "\n",
      "document_loaders.pdf.PyPDFium2Loader(...[, ...])\n",
      "Load PDF using pypdfium2 and chunks at character level.\n",
      "\n",
      "document_loaders.pdf.UnstructuredPDFLoader(...)\n",
      "Load PDF files using Unstructured.\n",
      "\n",
      "document_loaders.polars_dataframe.PolarsDataFrameLoader(...)\n",
      "Load Polars DataFrame.\n",
      "\n",
      "document_loaders.powerpoint.UnstructuredPowerPointLoader(...)\n",
      "Load Microsoft PowerPoint files using Unstructured.\n",
      "\n",
      "document_loaders.psychic.PsychicLoader(...)\n",
      "Load from Psychic.dev.\n",
      "\n",
      "document_loaders.pubmed.PubMedLoader(query)\n",
      "Load from the PubMed biomedical library.\n",
      "\n",
      "document_loaders.pyspark_dataframe.PySparkDataFrameLoader([...])\n",
      "Load PySpark DataFrames.\n",
      "\n",
      "document_loaders.python.PythonLoader(file_path)\n",
      "Load Python files, respecting any non-default encoding if specified.\n",
      "\n",
      "document_loaders.quip.QuipLoader(api_url, ...)\n",
      "Load Quip pages.\n",
      "\n",
      "document_loaders.readthedocs.ReadTheDocsLoader(path)\n",
      "Load ReadTheDocs documentation directory.\n",
      "\n",
      "document_loaders.recursive_url_loader.RecursiveUrlLoader(url)\n",
      "Load all child links from a URL page.\n",
      "\n",
      "document_loaders.reddit.RedditPostsLoader(...)\n",
      "Load Reddit posts.\n",
      "\n",
      "document_loaders.roam.RoamLoader(path)\n",
      "Load Roam files from a directory.\n",
      "\n",
      "document_loaders.rocksetdb.ColumnNotFoundError(...)\n",
      "Column not found error.\n",
      "\n",
      "document_loaders.rocksetdb.RocksetLoader(...)\n",
      "Load from a Rockset database.\n",
      "\n",
      "document_loaders.rspace.RSpaceLoader(global_id)\n",
      "Loads  content from RSpace notebooks, folders, documents or PDF Gallery files into Langchain documents.\n",
      "\n",
      "document_loaders.rss.RSSFeedLoader([urls, ...])\n",
      "Load news articles from RSS feeds using Unstructured.\n",
      "\n",
      "document_loaders.rst.UnstructuredRSTLoader(...)\n",
      "Load RST files using Unstructured.\n",
      "\n",
      "document_loaders.rtf.UnstructuredRTFLoader(...)\n",
      "Load RTF files using Unstructured.\n",
      "\n",
      "document_loaders.s3_directory.S3DirectoryLoader(bucket)\n",
      "Load from Amazon AWS S3 directory.\n",
      "\n",
      "document_loaders.s3_file.S3FileLoader(...[, ...])\n",
      "Load from Amazon AWS S3 file.\n",
      "\n",
      "document_loaders.sharepoint.SharePointLoader\n",
      "Load  from SharePoint.\n",
      "\n",
      "document_loaders.sitemap.SitemapLoader(web_path)\n",
      "Load a sitemap and its URLs.\n",
      "\n",
      "document_loaders.slack_directory.SlackDirectoryLoader(...)\n",
      "Load from a Slack directory dump.\n",
      "\n",
      "document_loaders.snowflake_loader.SnowflakeLoader(...)\n",
      "Load from Snowflake API.\n",
      "\n",
      "document_loaders.spreedly.SpreedlyLoader(...)\n",
      "Load from Spreedly API.\n",
      "\n",
      "document_loaders.srt.SRTLoader(file_path)\n",
      "Load .srt (subtitle) files.\n",
      "\n",
      "document_loaders.stripe.StripeLoader(resource)\n",
      "Load from Stripe API.\n",
      "\n",
      "document_loaders.telegram.TelegramChatApiLoader([...])\n",
      "Load Telegram chat json directory dump.\n",
      "\n",
      "document_loaders.telegram.TelegramChatFileLoader(path)\n",
      "Load from Telegram chat dump.\n",
      "\n",
      "document_loaders.tencent_cos_directory.TencentCOSDirectoryLoader(...)\n",
      "Load from Tencent Cloud COS directory.\n",
      "\n",
      "document_loaders.tencent_cos_file.TencentCOSFileLoader(...)\n",
      "Load from Tencent Cloud COS file.\n",
      "\n",
      "document_loaders.tensorflow_datasets.TensorflowDatasetLoader(...)\n",
      "Load from TensorFlow Dataset.\n",
      "\n",
      "document_loaders.text.TextLoader(file_path)\n",
      "Load text file.\n",
      "\n",
      "document_loaders.tomarkdown.ToMarkdownLoader(...)\n",
      "Load HTML using 2markdown API.\n",
      "\n",
      "document_loaders.toml.TomlLoader(source)\n",
      "Load TOML files.\n",
      "\n",
      "document_loaders.trello.TrelloLoader(client, ...)\n",
      "Load cards from a Trello board.\n",
      "\n",
      "document_loaders.tsv.UnstructuredTSVLoader(...)\n",
      "Load TSV files using Unstructured.\n",
      "\n",
      "document_loaders.twitter.TwitterTweetLoader(...)\n",
      "Load Twitter tweets.\n",
      "\n",
      "document_loaders.unstructured.UnstructuredAPIFileIOLoader(file)\n",
      "Load files using Unstructured API.\n",
      "\n",
      "document_loaders.unstructured.UnstructuredAPIFileLoader([...])\n",
      "Load files using Unstructured API.\n",
      "\n",
      "document_loaders.unstructured.UnstructuredBaseLoader([...])\n",
      "Base Loader that uses Unstructured.\n",
      "\n",
      "document_loaders.unstructured.UnstructuredFileIOLoader(file)\n",
      "Load files using Unstructured.\n",
      "\n",
      "document_loaders.unstructured.UnstructuredFileLoader(...)\n",
      "Load files using Unstructured.\n",
      "\n",
      "document_loaders.url.UnstructuredURLLoader(urls)\n",
      "Load files from remote URLs using Unstructured.\n",
      "\n",
      "document_loaders.url_playwright.PlaywrightEvaluator()\n",
      "Abstract base class for all evaluators.\n",
      "\n",
      "document_loaders.url_playwright.PlaywrightURLLoader(urls)\n",
      "Load HTML pages with Playwright and parse with Unstructured.\n",
      "\n",
      "document_loaders.url_playwright.UnstructuredHtmlEvaluator([...])\n",
      "Evaluates the page HTML content using the unstructured library.\n",
      "\n",
      "document_loaders.url_selenium.SeleniumURLLoader(urls)\n",
      "Load HTML pages with Selenium and parse with Unstructured.\n",
      "\n",
      "document_loaders.weather.WeatherDataLoader(...)\n",
      "Load weather data with Open Weather Map API.\n",
      "\n",
      "document_loaders.web_base.WebBaseLoader([...])\n",
      "Load HTML pages using urllib and parse them with `BeautifulSoup'.\n",
      "\n",
      "document_loaders.whatsapp_chat.WhatsAppChatLoader(path)\n",
      "Load WhatsApp messages text file.\n",
      "\n",
      "document_loaders.wikipedia.WikipediaLoader(query)\n",
      "Load from Wikipedia.\n",
      "\n",
      "document_loaders.word_document.Docx2txtLoader(...)\n",
      "Load DOCX file using docx2txt and chunks at character level.\n",
      "\n",
      "document_loaders.word_document.UnstructuredWordDocumentLoader(...)\n",
      "Load Microsoft Word file using Unstructured.\n",
      "\n",
      "document_loaders.xml.UnstructuredXMLLoader(...)\n",
      "Load XML file using Unstructured.\n",
      "\n",
      "document_loaders.xorbits.XorbitsLoader(...)\n",
      "Load Xorbits DataFrame.\n",
      "\n",
      "document_loaders.youtube.GoogleApiClient([...])\n",
      "Generic Google API Client.\n",
      "\n",
      "document_loaders.youtube.GoogleApiYoutubeLoader(...)\n",
      "Load all Videos from a YouTube Channel.\n",
      "\n",
      "document_loaders.youtube.YoutubeLoader(video_id)\n",
      "Load YouTube transcripts.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "document_loaders.base_o365.fetch_mime_types(...)\n",
      "Fetch the mime types for the specified file types.\n",
      "\n",
      "document_loaders.chatgpt.concatenate_rows(...)\n",
      "Combine message information in a readable format ready to be used.\n",
      "\n",
      "document_loaders.facebook_chat.concatenate_rows(row)\n",
      "Combine message information in a readable format ready to be used.\n",
      "\n",
      "document_loaders.helpers.detect_file_encodings(...)\n",
      "Try to detect the file encoding.\n",
      "\n",
      "document_loaders.notebook.concatenate_cells(...)\n",
      "Combine cells information in a readable format ready to be used.\n",
      "\n",
      "document_loaders.notebook.remove_newlines(x)\n",
      "Recursively remove newlines, no matter the data structure they are stored in.\n",
      "\n",
      "document_loaders.parsers.pdf.extract_from_images_with_rapidocr(images)\n",
      "Extract text from images with RapidOCR.\n",
      "\n",
      "document_loaders.parsers.registry.get_parser(...)\n",
      "Get a parser by parser name.\n",
      "\n",
      "document_loaders.rocksetdb.default_joiner(docs)\n",
      "Default joiner for content columns.\n",
      "\n",
      "document_loaders.telegram.concatenate_rows(row)\n",
      "Combine message information in a readable format ready to be used.\n",
      "\n",
      "document_loaders.telegram.text_to_docs(text)\n",
      "Convert a string or list of strings to a list of Documents with metadata.\n",
      "\n",
      "document_loaders.unstructured.get_elements_from_api([...])\n",
      "Retrieve a list of elements from the Unstructured API.\n",
      "\n",
      "document_loaders.unstructured.satisfies_min_unstructured_version(...)\n",
      "Check if the installed Unstructured version exceeds the minimum version for the feature in question.\n",
      "\n",
      "document_loaders.unstructured.validate_unstructured_version(...)\n",
      "Raise an error if the Unstructured version does not exceed the specified minimum.\n",
      "\n",
      "document_loaders.whatsapp_chat.concatenate_rows(...)\n",
      "Combine message information in a readable format ready to be used.\n",
      "\n",
      "langchain.document_transformers¶\n",
      "Document Transformers are classes to transform Documents.\n",
      "Document Transformers usually used to transform a lot of Documents in a single run.\n",
      "Class hierarchy:\n",
      "BaseDocumentTransformer --> <name>  # Examples: DoctranQATransformer, DoctranTextTranslator\n",
      "\n",
      "Main helpers:\n",
      "Document\n",
      "\n",
      "Classes¶\n",
      "\n",
      "document_transformers.beautiful_soup_transformer.BeautifulSoupTransformer()\n",
      "Transform HTML content by extracting specific tags and removing unwanted ones.\n",
      "\n",
      "document_transformers.doctran_text_extract.DoctranPropertyExtractor(...)\n",
      "Extract properties from text documents using doctran.\n",
      "\n",
      "document_transformers.doctran_text_qa.DoctranQATransformer([...])\n",
      "Extract QA from text documents using doctran.\n",
      "\n",
      "document_transformers.doctran_text_translate.DoctranTextTranslator([...])\n",
      "Translate text documents using doctran.\n",
      "\n",
      "document_transformers.embeddings_redundant_filter.EmbeddingsClusteringFilter\n",
      "Perform K-means clustering on document vectors.\n",
      "\n",
      "document_transformers.embeddings_redundant_filter.EmbeddingsRedundantFilter\n",
      "Filter that drops redundant documents by comparing their embeddings.\n",
      "\n",
      "document_transformers.google_translate.GoogleTranslateTransformer(...)\n",
      "Translate text documents using Google Cloud Translation.\n",
      "\n",
      "document_transformers.html2text.Html2TextTransformer([...])\n",
      "Replace occurrences of a particular search pattern with a replacement string\n",
      "\n",
      "document_transformers.long_context_reorder.LongContextReorder\n",
      "Lost in the middle: Performance degrades when models must access relevant information in the middle of long contexts.\n",
      "\n",
      "document_transformers.nuclia_text_transform.NucliaTextTransformer(nua)\n",
      "The Nuclia Understanding API splits into paragraphs and sentences, identifies entities, provides a summary of the text and generates embeddings for all sentences.\n",
      "\n",
      "document_transformers.openai_functions.OpenAIMetadataTagger\n",
      "Extract metadata tags from document contents using OpenAI functions.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "document_transformers.beautiful_soup_transformer.get_navigable_strings(element)\n",
      "\n",
      "document_transformers.embeddings_redundant_filter.get_stateful_documents(...)\n",
      "Convert a list of documents to a list of documents with state.\n",
      "\n",
      "document_transformers.openai_functions.create_metadata_tagger(...)\n",
      "Create a DocumentTransformer that uses an OpenAI function chain to automatically\n",
      "\n",
      "langchain.embeddings¶\n",
      "Embedding models  are wrappers around embedding models\n",
      "from different APIs and services.\n",
      "Embedding models can be LLMs or not.\n",
      "Class hierarchy:\n",
      "Embeddings --> <name>Embeddings  # Examples: OpenAIEmbeddings, HuggingFaceEmbeddings\n",
      "\n",
      "Classes¶\n",
      "\n",
      "embeddings.aleph_alpha.AlephAlphaAsymmetricSemanticEmbedding\n",
      "Aleph Alpha's asymmetric semantic embedding.\n",
      "\n",
      "embeddings.aleph_alpha.AlephAlphaSymmetricSemanticEmbedding\n",
      "The symmetric version of the Aleph Alpha's semantic embeddings.\n",
      "\n",
      "embeddings.awa.AwaEmbeddings\n",
      "Embedding documents and queries with Awa DB.\n",
      "\n",
      "embeddings.azure_openai.AzureOpenAIEmbeddings\n",
      "Azure OpenAI Embeddings API.\n",
      "\n",
      "embeddings.baidu_qianfan_endpoint.QianfanEmbeddingsEndpoint\n",
      "Baidu Qianfan Embeddings embedding models.\n",
      "\n",
      "embeddings.bedrock.BedrockEmbeddings\n",
      "Bedrock embedding models.\n",
      "\n",
      "embeddings.cache.CacheBackedEmbeddings(...)\n",
      "Interface for caching results from embedding models.\n",
      "\n",
      "embeddings.clarifai.ClarifaiEmbeddings\n",
      "Clarifai embedding models.\n",
      "\n",
      "embeddings.cohere.CohereEmbeddings\n",
      "Cohere embedding models.\n",
      "\n",
      "embeddings.dashscope.DashScopeEmbeddings\n",
      "DashScope embedding models.\n",
      "\n",
      "embeddings.deepinfra.DeepInfraEmbeddings\n",
      "Deep Infra's embedding inference service.\n",
      "\n",
      "embeddings.edenai.EdenAiEmbeddings\n",
      "EdenAI embedding.\n",
      "\n",
      "embeddings.elasticsearch.ElasticsearchEmbeddings(...)\n",
      "Elasticsearch embedding models.\n",
      "\n",
      "embeddings.embaas.EmbaasEmbeddings\n",
      "Embaas's embedding service.\n",
      "\n",
      "embeddings.embaas.EmbaasEmbeddingsPayload\n",
      "Payload for the Embaas embeddings API.\n",
      "\n",
      "embeddings.ernie.ErnieEmbeddings\n",
      "Ernie Embeddings V1 embedding models.\n",
      "\n",
      "embeddings.fake.DeterministicFakeEmbedding\n",
      "Fake embedding model that always returns the same embedding vector for the same text.\n",
      "\n",
      "embeddings.fake.FakeEmbeddings\n",
      "Fake embedding model.\n",
      "\n",
      "embeddings.fastembed.FastEmbedEmbeddings\n",
      "Qdrant FastEmbedding models.\n",
      "\n",
      "embeddings.google_palm.GooglePalmEmbeddings\n",
      "Google's PaLM Embeddings APIs.\n",
      "\n",
      "embeddings.gpt4all.GPT4AllEmbeddings\n",
      "GPT4All embedding models.\n",
      "\n",
      "embeddings.gradient_ai.GradientEmbeddings\n",
      "Gradient.ai Embedding models.\n",
      "\n",
      "embeddings.gradient_ai.TinyAsyncGradientEmbeddingClient([...])\n",
      "A helper tool to embed Gradient.\n",
      "\n",
      "embeddings.huggingface.HuggingFaceBgeEmbeddings\n",
      "HuggingFace BGE sentence_transformers embedding models.\n",
      "\n",
      "embeddings.huggingface.HuggingFaceEmbeddings\n",
      "HuggingFace sentence_transformers embedding models.\n",
      "\n",
      "embeddings.huggingface.HuggingFaceInferenceAPIEmbeddings\n",
      "Embed texts using the HuggingFace API.\n",
      "\n",
      "embeddings.huggingface.HuggingFaceInstructEmbeddings\n",
      "Wrapper around sentence_transformers embedding models.\n",
      "\n",
      "embeddings.huggingface_hub.HuggingFaceHubEmbeddings\n",
      "HuggingFaceHub embedding models.\n",
      "\n",
      "embeddings.javelin_ai_gateway.JavelinAIGatewayEmbeddings\n",
      "Wrapper around embeddings LLMs in the Javelin AI Gateway.\n",
      "\n",
      "embeddings.jina.JinaEmbeddings\n",
      "Jina embedding models.\n",
      "\n",
      "embeddings.johnsnowlabs.JohnSnowLabsEmbeddings\n",
      "JohnSnowLabs embedding models\n",
      "\n",
      "embeddings.llamacpp.LlamaCppEmbeddings\n",
      "llama.cpp embedding models.\n",
      "\n",
      "embeddings.llm_rails.LLMRailsEmbeddings\n",
      "LLMRails embedding models.\n",
      "\n",
      "embeddings.localai.LocalAIEmbeddings\n",
      "LocalAI embedding models.\n",
      "\n",
      "embeddings.minimax.MiniMaxEmbeddings\n",
      "MiniMax's embedding service.\n",
      "\n",
      "embeddings.mlflow_gateway.MlflowAIGatewayEmbeddings\n",
      "Wrapper around embeddings LLMs in the MLflow AI Gateway.\n",
      "\n",
      "embeddings.modelscope_hub.ModelScopeEmbeddings\n",
      "ModelScopeHub embedding models.\n",
      "\n",
      "embeddings.mosaicml.MosaicMLInstructorEmbeddings\n",
      "MosaicML embedding service.\n",
      "\n",
      "embeddings.nlpcloud.NLPCloudEmbeddings\n",
      "NLP Cloud embedding models.\n",
      "\n",
      "embeddings.octoai_embeddings.OctoAIEmbeddings\n",
      "OctoAI Compute Service embedding models.\n",
      "\n",
      "embeddings.ollama.OllamaEmbeddings\n",
      "Ollama locally runs large language models.\n",
      "\n",
      "embeddings.openai.OpenAIEmbeddings\n",
      "OpenAI embedding models.\n",
      "\n",
      "embeddings.sagemaker_endpoint.EmbeddingsContentHandler()\n",
      "Content handler for LLM class.\n",
      "\n",
      "embeddings.sagemaker_endpoint.SagemakerEndpointEmbeddings\n",
      "Custom Sagemaker Inference Endpoints.\n",
      "\n",
      "embeddings.self_hosted.SelfHostedEmbeddings\n",
      "Custom embedding models on self-hosted remote hardware.\n",
      "\n",
      "embeddings.self_hosted_hugging_face.SelfHostedHuggingFaceEmbeddings\n",
      "HuggingFace embedding models on self-hosted remote hardware.\n",
      "\n",
      "embeddings.self_hosted_hugging_face.SelfHostedHuggingFaceInstructEmbeddings\n",
      "HuggingFace InstructEmbedding models on self-hosted remote hardware.\n",
      "\n",
      "embeddings.spacy_embeddings.SpacyEmbeddings\n",
      "Embeddings by SpaCy models.\n",
      "\n",
      "embeddings.tensorflow_hub.TensorflowHubEmbeddings\n",
      "TensorflowHub embedding models.\n",
      "\n",
      "embeddings.vertexai.VertexAIEmbeddings\n",
      "Google Cloud VertexAI embedding models.\n",
      "\n",
      "embeddings.voyageai.VoyageEmbeddings\n",
      "Voyage embedding models.\n",
      "\n",
      "embeddings.xinference.XinferenceEmbeddings([...])\n",
      "Xinference embedding models.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "embeddings.dashscope.embed_with_retry(...)\n",
      "Use tenacity to retry the embedding call.\n",
      "\n",
      "embeddings.google_palm.embed_with_retry(...)\n",
      "Use tenacity to retry the completion call.\n",
      "\n",
      "embeddings.localai.async_embed_with_retry(...)\n",
      "Use tenacity to retry the embedding call.\n",
      "\n",
      "embeddings.localai.embed_with_retry(...)\n",
      "Use tenacity to retry the embedding call.\n",
      "\n",
      "embeddings.minimax.embed_with_retry(...)\n",
      "Use tenacity to retry the completion call.\n",
      "\n",
      "embeddings.openai.async_embed_with_retry(...)\n",
      "Use tenacity to retry the embedding call.\n",
      "\n",
      "embeddings.openai.embed_with_retry(...)\n",
      "Use tenacity to retry the embedding call.\n",
      "\n",
      "embeddings.self_hosted_hugging_face.load_embedding_model(...)\n",
      "Load the embedding model.\n",
      "\n",
      "embeddings.voyageai.embed_with_retry(...)\n",
      "Use tenacity to retry the embedding call.\n",
      "\n",
      "langchain.evaluation¶\n",
      "Evaluation chains for grading LLM and Chain outputs.\n",
      "This module contains off-the-shelf evaluation chains for grading the output of\n",
      "LangChain primitives such as language models and chains.\n",
      "Loading an evaluator\n",
      "To load an evaluator, you can use the load_evaluators or\n",
      "load_evaluator functions with the\n",
      "names of the evaluators to load.\n",
      "from langchain.evaluation import load_evaluator\n",
      "\n",
      "evaluator = load_evaluator(\"qa\")\n",
      "evaluator.evaluate_strings(\n",
      "    prediction=\"We sold more than 40,000 units last week\",\n",
      "    input=\"How many units did we sell last week?\",\n",
      "    reference=\"We sold 32,378 units\",\n",
      ")\n",
      "\n",
      "The evaluator must be one of EvaluatorType.\n",
      "Datasets\n",
      "To load one of the LangChain HuggingFace datasets, you can use the load_dataset function with the\n",
      "name of the dataset to load.\n",
      "from langchain.evaluation import load_dataset\n",
      "ds = load_dataset(\"llm-math\")\n",
      "\n",
      "Some common use cases for evaluation include:\n",
      "\n",
      "Grading the accuracy of a response against ground truth answers: QAEvalChain\n",
      "Comparing the output of two models: PairwiseStringEvalChain or LabeledPairwiseStringEvalChain when there is additionally a reference label.\n",
      "Judging the efficacy of an agent’s tool usage: TrajectoryEvalChain\n",
      "Checking whether an output complies with a set of criteria: CriteriaEvalChain or LabeledCriteriaEvalChain when there is additionally a reference label.\n",
      "Computing semantic difference between a prediction and reference: EmbeddingDistanceEvalChain or between two predictions: PairwiseEmbeddingDistanceEvalChain\n",
      "Measuring the string distance between a prediction and reference StringDistanceEvalChain or between two predictions PairwiseStringDistanceEvalChain\n",
      "\n",
      "Low-level API\n",
      "These evaluators implement one of the following interfaces:\n",
      "\n",
      "StringEvaluator: Evaluate a prediction string against a reference label and/or input context.\n",
      "PairwiseStringEvaluator: Evaluate two prediction strings against each other. Useful for scoring preferences, measuring similarity between two chain or llm agents, or comparing outputs on similar inputs.\n",
      "AgentTrajectoryEvaluator Evaluate the full sequence of actions taken by an agent.\n",
      "\n",
      "These interfaces enable easier composability and usage within a higher level evaluation framework.\n",
      "\n",
      "Classes¶\n",
      "\n",
      "evaluation.agents.trajectory_eval_chain.TrajectoryEval\n",
      "A named tuple containing the score and reasoning for a trajectory.\n",
      "\n",
      "evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain\n",
      "A chain for evaluating ReAct style agents.\n",
      "\n",
      "evaluation.agents.trajectory_eval_chain.TrajectoryOutputParser\n",
      "Trajectory output parser.\n",
      "\n",
      "evaluation.comparison.eval_chain.LabeledPairwiseStringEvalChain\n",
      "A chain for comparing two outputs, such as the outputs\n",
      "\n",
      "evaluation.comparison.eval_chain.PairwiseStringEvalChain\n",
      "A chain for comparing two outputs, such as the outputs\n",
      "\n",
      "evaluation.comparison.eval_chain.PairwiseStringResultOutputParser\n",
      "A parser for the output of the PairwiseStringEvalChain.\n",
      "\n",
      "evaluation.criteria.eval_chain.Criteria(value)\n",
      "A Criteria to evaluate.\n",
      "\n",
      "evaluation.criteria.eval_chain.CriteriaEvalChain\n",
      "LLM Chain for evaluating runs against criteria.\n",
      "\n",
      "evaluation.criteria.eval_chain.CriteriaResultOutputParser\n",
      "A parser for the output of the CriteriaEvalChain.\n",
      "\n",
      "evaluation.criteria.eval_chain.LabeledCriteriaEvalChain\n",
      "Criteria evaluation chain that requires references.\n",
      "\n",
      "evaluation.embedding_distance.base.EmbeddingDistance(value)\n",
      "Embedding Distance Metric.\n",
      "\n",
      "evaluation.embedding_distance.base.EmbeddingDistanceEvalChain\n",
      "Use embedding distances to score semantic difference between a prediction and reference.\n",
      "\n",
      "evaluation.embedding_distance.base.PairwiseEmbeddingDistanceEvalChain\n",
      "Use embedding distances to score semantic difference between two predictions.\n",
      "\n",
      "evaluation.exact_match.base.ExactMatchStringEvaluator(*)\n",
      "Compute an exact match between the prediction and the reference.\n",
      "\n",
      "evaluation.parsing.base.JsonEqualityEvaluator([...])\n",
      "Evaluates whether the prediction is equal to the reference after\n",
      "\n",
      "evaluation.parsing.base.JsonValidityEvaluator(...)\n",
      "Evaluates whether the prediction is valid JSON.\n",
      "\n",
      "evaluation.parsing.json_distance.JsonEditDistanceEvaluator([...])\n",
      "An evaluator that calculates the edit distance between JSON strings.\n",
      "\n",
      "evaluation.parsing.json_schema.JsonSchemaEvaluator(...)\n",
      "An evaluator that validates a JSON prediction against a JSON schema reference.\n",
      "\n",
      "evaluation.qa.eval_chain.ContextQAEvalChain\n",
      "LLM Chain for evaluating QA w/o GT based on context\n",
      "\n",
      "evaluation.qa.eval_chain.CotQAEvalChain\n",
      "LLM Chain for evaluating QA using chain of thought reasoning.\n",
      "\n",
      "evaluation.qa.eval_chain.QAEvalChain\n",
      "LLM Chain for evaluating question answering.\n",
      "\n",
      "evaluation.qa.generate_chain.QAGenerateChain\n",
      "LLM Chain for generating examples for question answering.\n",
      "\n",
      "evaluation.regex_match.base.RegexMatchStringEvaluator(*)\n",
      "Compute a regex match between the prediction and the reference.\n",
      "\n",
      "evaluation.schema.AgentTrajectoryEvaluator()\n",
      "Interface for evaluating agent trajectories.\n",
      "\n",
      "evaluation.schema.EvaluatorType(value[, ...])\n",
      "The types of the evaluators.\n",
      "\n",
      "evaluation.schema.LLMEvalChain\n",
      "A base class for evaluators that use an LLM.\n",
      "\n",
      "evaluation.schema.PairwiseStringEvaluator()\n",
      "Compare the output of two models (or two outputs of the same model).\n",
      "\n",
      "evaluation.schema.StringEvaluator()\n",
      "Grade, tag, or otherwise evaluate predictions relative to their inputs and/or reference labels.\n",
      "\n",
      "evaluation.scoring.eval_chain.LabeledScoreStringEvalChain\n",
      "A chain for scoring the output of a model on a scale of 1-10.\n",
      "\n",
      "evaluation.scoring.eval_chain.ScoreStringEvalChain\n",
      "A chain for scoring on a scale of 1-10 the output of a model.\n",
      "\n",
      "evaluation.scoring.eval_chain.ScoreStringResultOutputParser\n",
      "A parser for the output of the ScoreStringEvalChain.\n",
      "\n",
      "evaluation.string_distance.base.PairwiseStringDistanceEvalChain\n",
      "Compute string edit distances between two predictions.\n",
      "\n",
      "evaluation.string_distance.base.StringDistance(value)\n",
      "Distance metric to use.\n",
      "\n",
      "evaluation.string_distance.base.StringDistanceEvalChain\n",
      "Compute string distances between the prediction and the reference.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "evaluation.comparison.eval_chain.resolve_pairwise_criteria(...)\n",
      "Resolve the criteria for the pairwise evaluator.\n",
      "\n",
      "evaluation.criteria.eval_chain.resolve_criteria(...)\n",
      "Resolve the criteria to evaluate.\n",
      "\n",
      "evaluation.loading.load_dataset(uri)\n",
      "Load a dataset from the LangChainDatasets on HuggingFace.\n",
      "\n",
      "evaluation.loading.load_evaluator(evaluator, *)\n",
      "Load the requested evaluation chain specified by a string.\n",
      "\n",
      "evaluation.loading.load_evaluators(evaluators, *)\n",
      "Load evaluators specified by a list of evaluator types.\n",
      "\n",
      "evaluation.scoring.eval_chain.resolve_criteria(...)\n",
      "Resolve the criteria for the pairwise evaluator.\n",
      "\n",
      "langchain.graphs¶\n",
      "Graphs provide a natural language interface to graph databases.\n",
      "\n",
      "Classes¶\n",
      "\n",
      "graphs.arangodb_graph.ArangoGraph(db)\n",
      "ArangoDB wrapper for graph operations.\n",
      "\n",
      "graphs.falkordb_graph.FalkorDBGraph(database)\n",
      "FalkorDB wrapper for graph operations.\n",
      "\n",
      "graphs.graph_document.GraphDocument\n",
      "Represents a graph document consisting of nodes and relationships.\n",
      "\n",
      "graphs.graph_document.Node\n",
      "Represents a node in a graph with associated properties.\n",
      "\n",
      "graphs.graph_document.Relationship\n",
      "Represents a directed relationship between two nodes in a graph.\n",
      "\n",
      "graphs.graph_store.GraphStore()\n",
      "An abstract class wrapper for graph operations.\n",
      "\n",
      "graphs.hugegraph.HugeGraph([username, ...])\n",
      "HugeGraph wrapper for graph operations.\n",
      "\n",
      "graphs.kuzu_graph.KuzuGraph(db[, database])\n",
      "Kùzu wrapper for graph operations.\n",
      "\n",
      "graphs.memgraph_graph.MemgraphGraph(url, ...)\n",
      "Memgraph wrapper for graph operations.\n",
      "\n",
      "graphs.nebula_graph.NebulaGraph(space[, ...])\n",
      "NebulaGraph wrapper for graph operations.\n",
      "\n",
      "graphs.neo4j_graph.Neo4jGraph([url, ...])\n",
      "Neo4j wrapper for graph operations.\n",
      "\n",
      "graphs.neptune_graph.NeptuneGraph(host[, ...])\n",
      "Neptune wrapper for graph operations.\n",
      "\n",
      "graphs.neptune_graph.NeptuneQueryException(...)\n",
      "A class to handle queries that fail to execute\n",
      "\n",
      "graphs.networkx_graph.KnowledgeTriple(...)\n",
      "A triple in the graph.\n",
      "\n",
      "graphs.networkx_graph.NetworkxEntityGraph([graph])\n",
      "Networkx wrapper for entity graph operations.\n",
      "\n",
      "graphs.rdf_graph.RdfGraph([source_file, ...])\n",
      "RDFlib wrapper for graph operations.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "graphs.arangodb_graph.get_arangodb_client([...])\n",
      "Get the Arango DB client from credentials.\n",
      "\n",
      "graphs.networkx_graph.get_entities(entity_str)\n",
      "Extract entities from entity string.\n",
      "\n",
      "graphs.networkx_graph.parse_triples(...)\n",
      "Parse knowledge triples from the knowledge string.\n",
      "\n",
      "langchain.hub¶\n",
      "Interface with the LangChain Hub.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "hub.pull(owner_repo_commit, *[, api_url, ...])\n",
      "Pulls an object from the hub and returns it as a LangChain object.\n",
      "\n",
      "hub.push(repo_full_name, object, *[, ...])\n",
      "Pushes an object to the hub and returns the URL it can be viewed at in a browser.\n",
      "\n",
      "langchain.indexes¶\n",
      "Code to support various indexing workflows.\n",
      "Provides code to:\n",
      "\n",
      "Create knowledge graphs from data.\n",
      "Support indexing workflows from LangChain data loaders to vectorstores.\n",
      "\n",
      "For indexing workflows, this code is used to avoid writing duplicated content\n",
      "into the vectostore and to avoid over-writing content if it’s unchanged.\n",
      "Importantly, this keeps on working even if the content being written is derived\n",
      "via a set of transformations from some source content (e.g., indexing children\n",
      "documents that were derived from parent documents by chunking.)\n",
      "\n",
      "Classes¶\n",
      "\n",
      "indexes.base.RecordManager(namespace)\n",
      "An abstract base class representing the interface for a record manager.\n",
      "\n",
      "indexes.graph.GraphIndexCreator\n",
      "Functionality to create graph index.\n",
      "\n",
      "indexes.vectorstore.VectorStoreIndexWrapper\n",
      "Wrapper around a vectorstore for easy access.\n",
      "\n",
      "indexes.vectorstore.VectorstoreIndexCreator\n",
      "Logic for creating indexes.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "langchain.llms¶\n",
      "LLM classes provide\n",
      "access to the large language model (LLM) APIs and services.\n",
      "Class hierarchy:\n",
      "BaseLanguageModel --> BaseLLM --> LLM --> <name>  # Examples: AI21, HuggingFaceHub, OpenAI\n",
      "\n",
      "Main helpers:\n",
      "LLMResult, PromptValue,\n",
      "CallbackManagerForLLMRun, AsyncCallbackManagerForLLMRun,\n",
      "CallbackManager, AsyncCallbackManager,\n",
      "AIMessage, BaseMessage\n",
      "\n",
      "Classes¶\n",
      "\n",
      "llms.ai21.AI21\n",
      "AI21 large language models.\n",
      "\n",
      "llms.ai21.AI21PenaltyData\n",
      "Parameters for AI21 penalty data.\n",
      "\n",
      "llms.aleph_alpha.AlephAlpha\n",
      "Aleph Alpha large language models.\n",
      "\n",
      "llms.amazon_api_gateway.AmazonAPIGateway\n",
      "Amazon API Gateway to access LLM models hosted on AWS.\n",
      "\n",
      "llms.amazon_api_gateway.ContentHandlerAmazonAPIGateway()\n",
      "Adapter to prepare the inputs from Langchain to a format that LLM model expects.\n",
      "\n",
      "llms.anthropic.Anthropic\n",
      "Anthropic large language models.\n",
      "\n",
      "llms.anyscale.Anyscale\n",
      "Anyscale large language models.\n",
      "\n",
      "llms.arcee.Arcee\n",
      "Arcee's Domain Adapted Language Models (DALMs).\n",
      "\n",
      "llms.aviary.Aviary\n",
      "Aviary hosted models.\n",
      "\n",
      "llms.aviary.AviaryBackend(backend_url, bearer)\n",
      "Aviary backend.\n",
      "\n",
      "llms.azureml_endpoint.AzureMLEndpointClient(...)\n",
      "AzureML Managed Endpoint client.\n",
      "\n",
      "llms.azureml_endpoint.AzureMLOnlineEndpoint\n",
      "Azure ML Online Endpoint models.\n",
      "\n",
      "llms.azureml_endpoint.ContentFormatterBase()\n",
      "Transform request and response of AzureML endpoint to match with required schema.\n",
      "\n",
      "llms.azureml_endpoint.DollyContentFormatter()\n",
      "Content handler for the Dolly-v2-12b model\n",
      "\n",
      "llms.azureml_endpoint.GPT2ContentFormatter()\n",
      "Content handler for GPT2\n",
      "\n",
      "llms.azureml_endpoint.HFContentFormatter()\n",
      "Content handler for LLMs from the HuggingFace catalog.\n",
      "\n",
      "llms.azureml_endpoint.LlamaContentFormatter()\n",
      "Content formatter for LLaMa\n",
      "\n",
      "llms.azureml_endpoint.OSSContentFormatter()\n",
      "Deprecated: Kept for backwards compatibility\n",
      "\n",
      "llms.baidu_qianfan_endpoint.QianfanLLMEndpoint\n",
      "Baidu Qianfan hosted open source or customized models.\n",
      "\n",
      "llms.bananadev.Banana\n",
      "Banana large language models.\n",
      "\n",
      "llms.baseten.Baseten\n",
      "Baseten models.\n",
      "\n",
      "llms.beam.Beam\n",
      "Beam API for gpt2 large language model.\n",
      "\n",
      "llms.bedrock.Bedrock\n",
      "Bedrock models.\n",
      "\n",
      "llms.bedrock.BedrockBase\n",
      "Base class for Bedrock models.\n",
      "\n",
      "llms.bedrock.LLMInputOutputAdapter()\n",
      "Adapter class to prepare the inputs from Langchain to a format that LLM model expects.\n",
      "\n",
      "llms.bittensor.NIBittensorLLM\n",
      "NIBittensor LLMs\n",
      "\n",
      "llms.cerebriumai.CerebriumAI\n",
      "CerebriumAI large language models.\n",
      "\n",
      "llms.chatglm.ChatGLM\n",
      "ChatGLM LLM service.\n",
      "\n",
      "llms.clarifai.Clarifai\n",
      "Clarifai large language models.\n",
      "\n",
      "llms.cohere.BaseCohere\n",
      "Base class for Cohere models.\n",
      "\n",
      "llms.cohere.Cohere\n",
      "Cohere large language models.\n",
      "\n",
      "llms.ctransformers.CTransformers\n",
      "C Transformers LLM models.\n",
      "\n",
      "llms.ctranslate2.CTranslate2\n",
      "CTranslate2 language model.\n",
      "\n",
      "llms.databricks.Databricks\n",
      "Databricks serving endpoint or a cluster driver proxy app for LLM.\n",
      "\n",
      "llms.deepinfra.DeepInfra\n",
      "DeepInfra models.\n",
      "\n",
      "llms.deepsparse.DeepSparse\n",
      "Neural Magic DeepSparse LLM interface.\n",
      "\n",
      "llms.edenai.EdenAI\n",
      "Wrapper around edenai models.\n",
      "\n",
      "llms.fake.FakeListLLM\n",
      "Fake LLM for testing purposes.\n",
      "\n",
      "llms.fake.FakeStreamingListLLM\n",
      "Fake streaming list LLM for testing purposes.\n",
      "\n",
      "llms.fireworks.Fireworks\n",
      "Fireworks models.\n",
      "\n",
      "llms.forefrontai.ForefrontAI\n",
      "ForefrontAI large language models.\n",
      "\n",
      "llms.gigachat.GigaChat\n",
      "GigaChat large language models API.\n",
      "\n",
      "llms.google_palm.GooglePalm\n",
      "Google PaLM models.\n",
      "\n",
      "llms.gooseai.GooseAI\n",
      "GooseAI large language models.\n",
      "\n",
      "llms.gpt4all.GPT4All\n",
      "GPT4All language models.\n",
      "\n",
      "llms.gradient_ai.GradientLLM\n",
      "Gradient.ai LLM Endpoints.\n",
      "\n",
      "llms.gradient_ai.TrainResult\n",
      "Train result.\n",
      "\n",
      "llms.huggingface_endpoint.HuggingFaceEndpoint\n",
      "HuggingFace Endpoint models.\n",
      "\n",
      "llms.huggingface_hub.HuggingFaceHub\n",
      "HuggingFaceHub  models.\n",
      "\n",
      "llms.huggingface_pipeline.HuggingFacePipeline\n",
      "HuggingFace Pipeline API.\n",
      "\n",
      "llms.huggingface_text_gen_inference.HuggingFaceTextGenInference\n",
      "HuggingFace text generation API.\n",
      "\n",
      "llms.human.HumanInputLLM\n",
      "It returns user input as the response.\n",
      "\n",
      "llms.javelin_ai_gateway.JavelinAIGateway\n",
      "Javelin AI Gateway LLMs.\n",
      "\n",
      "llms.javelin_ai_gateway.Params\n",
      "Parameters for the Javelin AI Gateway LLM.\n",
      "\n",
      "llms.koboldai.KoboldApiLLM\n",
      "Kobold API language model.\n",
      "\n",
      "llms.llamacpp.LlamaCpp\n",
      "llama.cpp model.\n",
      "\n",
      "llms.manifest.ManifestWrapper\n",
      "HazyResearch's Manifest library.\n",
      "\n",
      "llms.minimax.Minimax\n",
      "Wrapper around Minimax large language models.\n",
      "\n",
      "llms.minimax.MinimaxCommon\n",
      "Common parameters for Minimax large language models.\n",
      "\n",
      "llms.mlflow_ai_gateway.MlflowAIGateway\n",
      "Wrapper around completions LLMs in the MLflow AI Gateway.\n",
      "\n",
      "llms.mlflow_ai_gateway.Params\n",
      "Parameters for the MLflow AI Gateway LLM.\n",
      "\n",
      "llms.modal.Modal\n",
      "Modal large language models.\n",
      "\n",
      "llms.mosaicml.MosaicML\n",
      "MosaicML LLM service.\n",
      "\n",
      "llms.nlpcloud.NLPCloud\n",
      "NLPCloud large language models.\n",
      "\n",
      "llms.octoai_endpoint.OctoAIEndpoint\n",
      "OctoAI LLM Endpoints.\n",
      "\n",
      "llms.ollama.Ollama\n",
      "Ollama locally runs large language models.\n",
      "\n",
      "llms.opaqueprompts.OpaquePrompts\n",
      "An LLM wrapper that uses OpaquePrompts to sanitize prompts.\n",
      "\n",
      "llms.openai.AzureOpenAI\n",
      "Azure-specific OpenAI large language models.\n",
      "\n",
      "llms.openai.BaseOpenAI\n",
      "Base OpenAI large language model class.\n",
      "\n",
      "llms.openai.OpenAI\n",
      "OpenAI large language models.\n",
      "\n",
      "llms.openai.OpenAIChat\n",
      "OpenAI Chat large language models.\n",
      "\n",
      "llms.openllm.IdentifyingParams\n",
      "Parameters for identifying a model as a typed dict.\n",
      "\n",
      "llms.openllm.OpenLLM\n",
      "OpenLLM, supporting both in-process model instance and remote OpenLLM servers.\n",
      "\n",
      "llms.openlm.OpenLM\n",
      "OpenLM models.\n",
      "\n",
      "llms.pai_eas_endpoint.PaiEasEndpoint\n",
      "Langchain LLM class to help to access eass llm service.\n",
      "\n",
      "llms.petals.Petals\n",
      "Petals Bloom models.\n",
      "\n",
      "llms.pipelineai.PipelineAI\n",
      "PipelineAI large language models.\n",
      "\n",
      "llms.predibase.Predibase\n",
      "Use your Predibase models with Langchain.\n",
      "\n",
      "llms.predictionguard.PredictionGuard\n",
      "Prediction Guard large language models.\n",
      "\n",
      "llms.promptlayer_openai.PromptLayerOpenAI\n",
      "PromptLayer OpenAI large language models.\n",
      "\n",
      "llms.promptlayer_openai.PromptLayerOpenAIChat\n",
      "Wrapper around OpenAI large language models.\n",
      "\n",
      "llms.replicate.Replicate\n",
      "Replicate models.\n",
      "\n",
      "llms.rwkv.RWKV\n",
      "RWKV language models.\n",
      "\n",
      "llms.sagemaker_endpoint.ContentHandlerBase()\n",
      "A handler class to transform input from LLM to a format that SageMaker endpoint expects.\n",
      "\n",
      "llms.sagemaker_endpoint.LLMContentHandler()\n",
      "Content handler for LLM class.\n",
      "\n",
      "llms.sagemaker_endpoint.LineIterator(stream)\n",
      "A helper class for parsing the byte stream input.\n",
      "\n",
      "llms.sagemaker_endpoint.SagemakerEndpoint\n",
      "Sagemaker Inference Endpoint models.\n",
      "\n",
      "llms.self_hosted.SelfHostedPipeline\n",
      "Model inference on self-hosted remote hardware.\n",
      "\n",
      "llms.self_hosted_hugging_face.SelfHostedHuggingFaceLLM\n",
      "HuggingFace Pipeline API to run on self-hosted remote hardware.\n",
      "\n",
      "llms.stochasticai.StochasticAI\n",
      "StochasticAI large language models.\n",
      "\n",
      "llms.symblai_nebula.Nebula\n",
      "Nebula Service models.\n",
      "\n",
      "llms.textgen.TextGen\n",
      "text-generation-webui models.\n",
      "\n",
      "llms.titan_takeoff.TitanTakeoff\n",
      "Wrapper around Titan Takeoff APIs.\n",
      "\n",
      "llms.titan_takeoff_pro.TitanTakeoffPro\n",
      "Create a new model by parsing and validating input data from keyword arguments.\n",
      "\n",
      "llms.together.Together\n",
      "Wrapper around Together AI models.\n",
      "\n",
      "llms.tongyi.Tongyi\n",
      "Tongyi Qwen large language models.\n",
      "\n",
      "llms.vertexai.VertexAI\n",
      "Google Vertex AI large language models.\n",
      "\n",
      "llms.vertexai.VertexAIModelGarden\n",
      "Large language models served from Vertex AI Model Garden.\n",
      "\n",
      "llms.vllm.VLLM\n",
      "VLLM language model.\n",
      "\n",
      "llms.vllm.VLLMOpenAI\n",
      "vLLM OpenAI-compatible API client\n",
      "\n",
      "llms.writer.Writer\n",
      "Writer large language models.\n",
      "\n",
      "llms.xinference.Xinference\n",
      "Wrapper for accessing Xinference's large-scale model inference service.\n",
      "\n",
      "llms.yandex.YandexGPT\n",
      "Yandex large language models.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "llms.anyscale.create_llm_result(choices, ...)\n",
      "Create the LLMResult from the choices and prompts.\n",
      "\n",
      "llms.anyscale.update_token_usage(keys, ...)\n",
      "Update token usage.\n",
      "\n",
      "llms.aviary.get_completions(model, prompt[, ...])\n",
      "Get completions from Aviary models.\n",
      "\n",
      "llms.aviary.get_models()\n",
      "List available models\n",
      "\n",
      "llms.cohere.acompletion_with_retry(llm, **kwargs)\n",
      "Use tenacity to retry the completion call.\n",
      "\n",
      "llms.cohere.completion_with_retry(llm, **kwargs)\n",
      "Use tenacity to retry the completion call.\n",
      "\n",
      "llms.databricks.get_default_api_token()\n",
      "Gets the default Databricks personal access token.\n",
      "\n",
      "llms.databricks.get_default_host()\n",
      "Gets the default Databricks workspace hostname.\n",
      "\n",
      "llms.databricks.get_repl_context()\n",
      "Gets the notebook REPL context if running inside a Databricks notebook.\n",
      "\n",
      "llms.fireworks.acompletion_with_retry(llm, ...)\n",
      "Use tenacity to retry the completion call.\n",
      "\n",
      "llms.fireworks.acompletion_with_retry_batching(...)\n",
      "Use tenacity to retry the completion call.\n",
      "\n",
      "llms.fireworks.acompletion_with_retry_streaming(...)\n",
      "Use tenacity to retry the completion call for streaming.\n",
      "\n",
      "llms.fireworks.completion_with_retry(llm, ...)\n",
      "Use tenacity to retry the completion call.\n",
      "\n",
      "llms.fireworks.completion_with_retry_batching(...)\n",
      "Use tenacity to retry the completion call.\n",
      "\n",
      "llms.fireworks.conditional_decorator(...)\n",
      "\n",
      "llms.google_palm.generate_with_retry(llm, ...)\n",
      "Use tenacity to retry the completion call.\n",
      "\n",
      "llms.koboldai.clean_url(url)\n",
      "Remove trailing slash and /api from url if present.\n",
      "\n",
      "llms.loading.load_llm(file)\n",
      "Load LLM from file.\n",
      "\n",
      "llms.loading.load_llm_from_config(config)\n",
      "Load LLM from Config Dict.\n",
      "\n",
      "llms.openai.acompletion_with_retry(llm[, ...])\n",
      "Use tenacity to retry the async completion call.\n",
      "\n",
      "llms.openai.completion_with_retry(llm[, ...])\n",
      "Use tenacity to retry the completion call.\n",
      "\n",
      "llms.openai.update_token_usage(keys, ...)\n",
      "Update token usage.\n",
      "\n",
      "llms.symblai_nebula.completion_with_retry(...)\n",
      "Use tenacity to retry the completion call.\n",
      "\n",
      "llms.symblai_nebula.make_request(self, ...)\n",
      "Generate text from the model.\n",
      "\n",
      "llms.tongyi.generate_with_retry(llm, **kwargs)\n",
      "Use tenacity to retry the completion call.\n",
      "\n",
      "llms.tongyi.stream_generate_with_retry(llm, ...)\n",
      "Use tenacity to retry the completion call.\n",
      "\n",
      "llms.utils.enforce_stop_tokens(text, stop)\n",
      "Cut off the text as soon as any stop words occur.\n",
      "\n",
      "llms.vertexai.acompletion_with_retry(llm, *args)\n",
      "Use tenacity to retry the completion call.\n",
      "\n",
      "llms.vertexai.completion_with_retry(llm, *args)\n",
      "Use tenacity to retry the completion call.\n",
      "\n",
      "llms.vertexai.is_codey_model(model_name)\n",
      "Returns True if the model name is a Codey model.\n",
      "\n",
      "llms.vertexai.stream_completion_with_retry(...)\n",
      "Use tenacity to retry the completion call.\n",
      "\n",
      "langchain.memory¶\n",
      "Memory maintains Chain state, incorporating context from past runs.\n",
      "Class hierarchy for Memory:\n",
      "BaseMemory --> BaseChatMemory --> <name>Memory  # Examples: ZepMemory, MotorheadMemory\n",
      "\n",
      "Main helpers:\n",
      "BaseChatMessageHistory\n",
      "\n",
      "Chat Message History stores the chat message history in different stores.\n",
      "Class hierarchy for ChatMessageHistory:\n",
      "BaseChatMessageHistory --> <name>ChatMessageHistory  # Example: ZepChatMessageHistory\n",
      "\n",
      "Main helpers:\n",
      "AIMessage, BaseMessage, HumanMessage\n",
      "\n",
      "Classes¶\n",
      "\n",
      "memory.buffer.ConversationBufferMemory\n",
      "Buffer for storing conversation memory.\n",
      "\n",
      "memory.buffer.ConversationStringBufferMemory\n",
      "Buffer for storing conversation memory.\n",
      "\n",
      "memory.buffer_window.ConversationBufferWindowMemory\n",
      "Buffer for storing conversation memory inside a limited size window.\n",
      "\n",
      "memory.chat_memory.BaseChatMemory\n",
      "Abstract base class for chat memory.\n",
      "\n",
      "memory.chat_message_histories.cassandra.CassandraChatMessageHistory(...)\n",
      "Chat message history that stores history in Cassandra.\n",
      "\n",
      "memory.chat_message_histories.cosmos_db.CosmosDBChatMessageHistory(...)\n",
      "Chat message history backed by Azure CosmosDB.\n",
      "\n",
      "memory.chat_message_histories.dynamodb.DynamoDBChatMessageHistory(...)\n",
      "Chat message history that stores history in AWS DynamoDB.\n",
      "\n",
      "memory.chat_message_histories.elasticsearch.ElasticsearchChatMessageHistory(...)\n",
      "Chat message history that stores history in Elasticsearch.\n",
      "\n",
      "memory.chat_message_histories.file.FileChatMessageHistory(...)\n",
      "Chat message history that stores history in a local file.\n",
      "\n",
      "memory.chat_message_histories.firestore.FirestoreChatMessageHistory(...)\n",
      "Chat message history backed by Google Firestore.\n",
      "\n",
      "memory.chat_message_histories.in_memory.ChatMessageHistory\n",
      "In memory implementation of chat message history.\n",
      "\n",
      "memory.chat_message_histories.momento.MomentoChatMessageHistory(...)\n",
      "Chat message history cache that uses Momento as a backend.\n",
      "\n",
      "memory.chat_message_histories.mongodb.MongoDBChatMessageHistory(...)\n",
      "Chat message history that stores history in MongoDB.\n",
      "\n",
      "memory.chat_message_histories.neo4j.Neo4jChatMessageHistory(...)\n",
      "Chat message history stored in a Neo4j database.\n",
      "\n",
      "memory.chat_message_histories.postgres.PostgresChatMessageHistory(...)\n",
      "Chat message history stored in a Postgres database.\n",
      "\n",
      "memory.chat_message_histories.redis.RedisChatMessageHistory(...)\n",
      "Chat message history stored in a Redis database.\n",
      "\n",
      "memory.chat_message_histories.rocksetdb.RocksetChatMessageHistory(...)\n",
      "Uses Rockset to store chat messages.\n",
      "\n",
      "memory.chat_message_histories.singlestoredb.SingleStoreDBChatMessageHistory(...)\n",
      "Chat message history stored in a SingleStoreDB database.\n",
      "\n",
      "memory.chat_message_histories.sql.BaseMessageConverter()\n",
      "The class responsible for converting BaseMessage to your SQLAlchemy model.\n",
      "\n",
      "memory.chat_message_histories.sql.DefaultMessageConverter(...)\n",
      "The default message converter for SQLChatMessageHistory.\n",
      "\n",
      "memory.chat_message_histories.sql.SQLChatMessageHistory(...)\n",
      "Chat message history stored in an SQL database.\n",
      "\n",
      "memory.chat_message_histories.streamlit.StreamlitChatMessageHistory([key])\n",
      "Chat message history that stores messages in Streamlit session state.\n",
      "\n",
      "memory.chat_message_histories.upstash_redis.UpstashRedisChatMessageHistory(...)\n",
      "Chat message history stored in an Upstash Redis database.\n",
      "\n",
      "memory.chat_message_histories.xata.XataChatMessageHistory(...)\n",
      "Chat message history stored in a Xata database.\n",
      "\n",
      "memory.chat_message_histories.zep.ZepChatMessageHistory(...)\n",
      "Chat message history that uses Zep as a backend.\n",
      "\n",
      "memory.combined.CombinedMemory\n",
      "Combining multiple memories' data together.\n",
      "\n",
      "memory.entity.BaseEntityStore\n",
      "Abstract base class for Entity store.\n",
      "\n",
      "memory.entity.ConversationEntityMemory\n",
      "Entity extractor & summarizer memory.\n",
      "\n",
      "memory.entity.InMemoryEntityStore\n",
      "In-memory Entity store.\n",
      "\n",
      "memory.entity.RedisEntityStore\n",
      "Redis-backed Entity store.\n",
      "\n",
      "memory.entity.SQLiteEntityStore\n",
      "SQLite-backed Entity store\n",
      "\n",
      "memory.entity.UpstashRedisEntityStore\n",
      "Upstash Redis backed Entity store.\n",
      "\n",
      "memory.kg.ConversationKGMemory\n",
      "Knowledge graph conversation memory.\n",
      "\n",
      "memory.motorhead_memory.MotorheadMemory\n",
      "Chat message memory backed by Motorhead service.\n",
      "\n",
      "memory.readonly.ReadOnlySharedMemory\n",
      "A memory wrapper that is read-only and cannot be changed.\n",
      "\n",
      "memory.simple.SimpleMemory\n",
      "Simple memory for storing context or other information that shouldn't ever change between prompts.\n",
      "\n",
      "memory.summary.ConversationSummaryMemory\n",
      "Conversation summarizer to chat memory.\n",
      "\n",
      "memory.summary.SummarizerMixin\n",
      "Mixin for summarizer.\n",
      "\n",
      "memory.summary_buffer.ConversationSummaryBufferMemory\n",
      "Buffer with summarizer for storing conversation memory.\n",
      "\n",
      "memory.token_buffer.ConversationTokenBufferMemory\n",
      "Conversation chat memory with token limit.\n",
      "\n",
      "memory.vectorstore.VectorStoreRetrieverMemory\n",
      "VectorStoreRetriever-backed memory.\n",
      "\n",
      "memory.zep_memory.ZepMemory\n",
      "Persist your chain history to the Zep MemoryStore.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "memory.chat_message_histories.sql.create_message_model(...)\n",
      "Create a message model for a given table name.\n",
      "\n",
      "memory.utils.get_prompt_input_key(inputs, ...)\n",
      "Get the prompt input key.\n",
      "\n",
      "langchain.model_laboratory¶\n",
      "Experiment with different models.\n",
      "\n",
      "Classes¶\n",
      "\n",
      "model_laboratory.ModelLaboratory(chains[, names])\n",
      "Experiment with different models.\n",
      "\n",
      "langchain.output_parsers¶\n",
      "OutputParser classes parse the output of an LLM call.\n",
      "Class hierarchy:\n",
      "BaseLLMOutputParser --> BaseOutputParser --> <name>OutputParser  # ListOutputParser, PydanticOutputParser\n",
      "\n",
      "Main helpers:\n",
      "Serializable, Generation, PromptValue\n",
      "\n",
      "Classes¶\n",
      "\n",
      "output_parsers.boolean.BooleanOutputParser\n",
      "Parse the output of an LLM call to a boolean.\n",
      "\n",
      "output_parsers.combining.CombiningOutputParser\n",
      "Combine multiple output parsers into one.\n",
      "\n",
      "output_parsers.datetime.DatetimeOutputParser\n",
      "Parse the output of an LLM call to a datetime.\n",
      "\n",
      "output_parsers.enum.EnumOutputParser\n",
      "Parse an output that is one of a set of values.\n",
      "\n",
      "output_parsers.ernie_functions.JsonKeyOutputFunctionsParser\n",
      "Parse an output as the element of the Json object.\n",
      "\n",
      "output_parsers.ernie_functions.JsonOutputFunctionsParser\n",
      "Parse an output as the Json object.\n",
      "\n",
      "output_parsers.ernie_functions.OutputFunctionsParser\n",
      "Parse an output that is one of sets of values.\n",
      "\n",
      "output_parsers.ernie_functions.PydanticAttrOutputFunctionsParser\n",
      "Parse an output as an attribute of a pydantic object.\n",
      "\n",
      "output_parsers.ernie_functions.PydanticOutputFunctionsParser\n",
      "Parse an output as a pydantic object.\n",
      "\n",
      "output_parsers.fix.OutputFixingParser\n",
      "Wraps a parser and tries to fix parsing errors.\n",
      "\n",
      "output_parsers.json.SimpleJsonOutputParser\n",
      "Parse the output of an LLM call to a JSON object.\n",
      "\n",
      "output_parsers.openai_functions.JsonKeyOutputFunctionsParser\n",
      "Parse an output as the element of the Json object.\n",
      "\n",
      "output_parsers.openai_functions.JsonOutputFunctionsParser\n",
      "Parse an output as the Json object.\n",
      "\n",
      "output_parsers.openai_functions.OutputFunctionsParser\n",
      "Parse an output that is one of sets of values.\n",
      "\n",
      "output_parsers.openai_functions.PydanticAttrOutputFunctionsParser\n",
      "Parse an output as an attribute of a pydantic object.\n",
      "\n",
      "output_parsers.openai_functions.PydanticOutputFunctionsParser\n",
      "Parse an output as a pydantic object.\n",
      "\n",
      "output_parsers.openai_tools.JsonOutputKeyToolsParser\n",
      "Parse tools from OpenAI response.\n",
      "\n",
      "output_parsers.openai_tools.JsonOutputToolsParser\n",
      "Parse tools from OpenAI response.\n",
      "\n",
      "output_parsers.openai_tools.PydanticToolsParser\n",
      "Parse tools from OpenAI response.\n",
      "\n",
      "output_parsers.pydantic.PydanticOutputParser\n",
      "Parse an output using a pydantic model.\n",
      "\n",
      "output_parsers.rail_parser.GuardrailsOutputParser\n",
      "Parse the output of an LLM call using Guardrails.\n",
      "\n",
      "output_parsers.regex.RegexParser\n",
      "Parse the output of an LLM call using a regex.\n",
      "\n",
      "output_parsers.regex_dict.RegexDictParser\n",
      "Parse the output of an LLM call into a Dictionary using a regex.\n",
      "\n",
      "output_parsers.retry.RetryOutputParser\n",
      "Wraps a parser and tries to fix parsing errors.\n",
      "\n",
      "output_parsers.retry.RetryWithErrorOutputParser\n",
      "Wraps a parser and tries to fix parsing errors.\n",
      "\n",
      "output_parsers.structured.ResponseSchema\n",
      "A schema for a response from a structured output parser.\n",
      "\n",
      "output_parsers.structured.StructuredOutputParser\n",
      "Parse the output of an LLM call to a structured output.\n",
      "\n",
      "output_parsers.xml.XMLOutputParser\n",
      "Parse an output using xml format.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "output_parsers.json.parse_and_check_json_markdown(...)\n",
      "Parse a JSON string from a Markdown string and check that it contains the expected keys.\n",
      "\n",
      "output_parsers.json.parse_json_markdown(...)\n",
      "Parse a JSON string from a Markdown string.\n",
      "\n",
      "output_parsers.json.parse_partial_json(s, *)\n",
      "Parse a JSON string that may be missing closing braces.\n",
      "\n",
      "output_parsers.loading.load_output_parser(config)\n",
      "Load an output parser.\n",
      "\n",
      "langchain.prompts¶\n",
      "Prompt is the input to the model.\n",
      "Prompt is often constructed\n",
      "from multiple components. Prompt classes and functions make constructing\n",
      "\n",
      "and working with prompts easy.\n",
      "\n",
      "Class hierarchy:\n",
      "BasePromptTemplate --> PipelinePromptTemplate\n",
      "                       StringPromptTemplate --> PromptTemplate\n",
      "                                                FewShotPromptTemplate\n",
      "                                                FewShotPromptWithTemplates\n",
      "                       BaseChatPromptTemplate --> AutoGPTPrompt\n",
      "                                                  ChatPromptTemplate --> AgentScratchPadChatPromptTemplate\n",
      "\n",
      "BaseMessagePromptTemplate --> MessagesPlaceholder\n",
      "                              BaseStringMessagePromptTemplate --> ChatMessagePromptTemplate\n",
      "                                                                  HumanMessagePromptTemplate\n",
      "                                                                  AIMessagePromptTemplate\n",
      "                                                                  SystemMessagePromptTemplate\n",
      "\n",
      "PromptValue --> StringPromptValue\n",
      "                ChatPromptValue\n",
      "\n",
      "Classes¶\n",
      "\n",
      "prompts.example_selector.ngram_overlap.NGramOverlapExampleSelector\n",
      "Select and order examples based on ngram overlap score (sentence_bleu score).\n",
      "\n",
      "Functions¶\n",
      "\n",
      "prompts.example_selector.ngram_overlap.ngram_overlap_score(...)\n",
      "Compute ngram overlap score of source and example as sentence_bleu score.\n",
      "\n",
      "langchain.retrievers¶\n",
      "Retriever class returns Documents given a text query.\n",
      "It is more general than a vector store. A retriever does not need to be able to\n",
      "store documents, only to return (or retrieve) it. Vector stores can be used as\n",
      "the backbone of a retriever, but there are other types of retrievers as well.\n",
      "Class hierarchy:\n",
      "BaseRetriever --> <name>Retriever  # Examples: ArxivRetriever, MergerRetriever\n",
      "\n",
      "Main helpers:\n",
      "Document, Serializable, Callbacks,\n",
      "CallbackManagerForRetrieverRun, AsyncCallbackManagerForRetrieverRun\n",
      "\n",
      "Classes¶\n",
      "\n",
      "retrievers.arcee.ArceeRetriever\n",
      "Document retriever for Arcee's Domain Adapted Language Models (DALMs).\n",
      "\n",
      "retrievers.arxiv.ArxivRetriever\n",
      "Arxiv retriever.\n",
      "\n",
      "retrievers.azure_cognitive_search.AzureCognitiveSearchRetriever\n",
      "Azure Cognitive Search service retriever.\n",
      "\n",
      "retrievers.bm25.BM25Retriever\n",
      "BM25 retriever without Elasticsearch.\n",
      "\n",
      "retrievers.chaindesk.ChaindeskRetriever\n",
      "Chaindesk API retriever.\n",
      "\n",
      "retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever\n",
      "ChatGPT plugin retriever.\n",
      "\n",
      "retrievers.cohere_rag_retriever.CohereRagRetriever\n",
      "Cohere Chat API with RAG.\n",
      "\n",
      "retrievers.contextual_compression.ContextualCompressionRetriever\n",
      "Retriever that wraps a base retriever and compresses the results.\n",
      "\n",
      "retrievers.databerry.DataberryRetriever\n",
      "Databerry API retriever.\n",
      "\n",
      "retrievers.docarray.DocArrayRetriever\n",
      "DocArray Document Indices retriever.\n",
      "\n",
      "retrievers.docarray.SearchType(value[, ...])\n",
      "Enumerator of the types of search to perform.\n",
      "\n",
      "retrievers.document_compressors.base.BaseDocumentCompressor\n",
      "Base class for document compressors.\n",
      "\n",
      "retrievers.document_compressors.base.DocumentCompressorPipeline\n",
      "Document compressor that uses a pipeline of Transformers.\n",
      "\n",
      "retrievers.document_compressors.chain_extract.LLMChainExtractor\n",
      "Document compressor that uses an LLM chain to extract the relevant parts of documents.\n",
      "\n",
      "retrievers.document_compressors.chain_extract.NoOutputParser\n",
      "Parse outputs that could return a null string of some sort.\n",
      "\n",
      "retrievers.document_compressors.chain_filter.LLMChainFilter\n",
      "Filter that drops documents that aren't relevant to the query.\n",
      "\n",
      "retrievers.document_compressors.cohere_rerank.CohereRerank\n",
      "Document compressor that uses Cohere Rerank API.\n",
      "\n",
      "retrievers.document_compressors.embeddings_filter.EmbeddingsFilter\n",
      "Document compressor that uses embeddings to drop documents unrelated to the query.\n",
      "\n",
      "retrievers.elastic_search_bm25.ElasticSearchBM25Retriever\n",
      "Elasticsearch retriever that uses BM25.\n",
      "\n",
      "retrievers.embedchain.EmbedchainRetriever\n",
      "Embedchain retriever.\n",
      "\n",
      "retrievers.ensemble.EnsembleRetriever\n",
      "Retriever that ensembles the multiple retrievers.\n",
      "\n",
      "retrievers.google_cloud_documentai_warehouse.GoogleDocumentAIWarehouseRetriever\n",
      "A retriever based on Document AI Warehouse.\n",
      "\n",
      "retrievers.google_vertex_ai_search.GoogleCloudEnterpriseSearchRetriever\n",
      "Google Vertex Search API retriever alias for backwards compatibility.\n",
      "\n",
      "retrievers.google_vertex_ai_search.GoogleVertexAIMultiTurnSearchRetriever\n",
      "Google Vertex AI Search retriever for multi-turn conversations.\n",
      "\n",
      "retrievers.google_vertex_ai_search.GoogleVertexAISearchRetriever\n",
      "Google Vertex AI Search retriever.\n",
      "\n",
      "retrievers.kay.KayAiRetriever\n",
      "Retriever for Kay.ai datasets.\n",
      "\n",
      "retrievers.kendra.AdditionalResultAttribute\n",
      "Additional result attribute.\n",
      "\n",
      "retrievers.kendra.AdditionalResultAttributeValue\n",
      "Value of an additional result attribute.\n",
      "\n",
      "retrievers.kendra.AmazonKendraRetriever\n",
      "Amazon Kendra Index retriever.\n",
      "\n",
      "retrievers.kendra.DocumentAttribute\n",
      "Document attribute.\n",
      "\n",
      "retrievers.kendra.DocumentAttributeValue\n",
      "Value of a document attribute.\n",
      "\n",
      "retrievers.kendra.Highlight\n",
      "Information that highlights the keywords in the excerpt.\n",
      "\n",
      "retrievers.kendra.QueryResult\n",
      "Amazon Kendra Query API search result.\n",
      "\n",
      "retrievers.kendra.QueryResultItem\n",
      "Query API result item.\n",
      "\n",
      "retrievers.kendra.ResultItem\n",
      "Base class of a result item.\n",
      "\n",
      "retrievers.kendra.RetrieveResult\n",
      "Amazon Kendra Retrieve API search result.\n",
      "\n",
      "retrievers.kendra.RetrieveResultItem\n",
      "Retrieve API result item.\n",
      "\n",
      "retrievers.kendra.TextWithHighLights\n",
      "Text with highlights.\n",
      "\n",
      "retrievers.knn.KNNRetriever\n",
      "KNN retriever.\n",
      "\n",
      "retrievers.llama_index.LlamaIndexGraphRetriever\n",
      "LlamaIndex graph data structure retriever.\n",
      "\n",
      "retrievers.llama_index.LlamaIndexRetriever\n",
      "LlamaIndex retriever.\n",
      "\n",
      "retrievers.merger_retriever.MergerRetriever\n",
      "Retriever that merges the results of multiple retrievers.\n",
      "\n",
      "retrievers.metal.MetalRetriever\n",
      "Metal API retriever.\n",
      "\n",
      "retrievers.milvus.MilvusRetriever\n",
      "Milvus API retriever.\n",
      "\n",
      "retrievers.multi_query.LineList\n",
      "List of lines.\n",
      "\n",
      "retrievers.multi_query.LineListOutputParser\n",
      "Output parser for a list of lines.\n",
      "\n",
      "retrievers.multi_query.MultiQueryRetriever\n",
      "Given a query, use an LLM to write a set of queries.\n",
      "\n",
      "retrievers.multi_vector.MultiVectorRetriever\n",
      "Retrieve from a set of multiple embeddings for the same document.\n",
      "\n",
      "retrievers.parent_document_retriever.ParentDocumentRetriever\n",
      "Retrieve small chunks then retrieve their parent documents.\n",
      "\n",
      "retrievers.pinecone_hybrid_search.PineconeHybridSearchRetriever\n",
      "Pinecone Hybrid Search retriever.\n",
      "\n",
      "retrievers.pubmed.PubMedRetriever\n",
      "PubMed API retriever.\n",
      "\n",
      "retrievers.re_phraser.RePhraseQueryRetriever\n",
      "Given a query, use an LLM to re-phrase it.\n",
      "\n",
      "retrievers.remote_retriever.RemoteLangChainRetriever\n",
      "LangChain API retriever.\n",
      "\n",
      "retrievers.self_query.base.SelfQueryRetriever\n",
      "Retriever that uses a vector store and an LLM to generate the vector store queries.\n",
      "\n",
      "retrievers.self_query.chroma.ChromaTranslator()\n",
      "Translate Chroma internal query language elements to valid filters.\n",
      "\n",
      "retrievers.self_query.dashvector.DashvectorTranslator()\n",
      "Logic for converting internal query language elements to valid filters.\n",
      "\n",
      "retrievers.self_query.deeplake.DeepLakeTranslator()\n",
      "Translate DeepLake internal query language elements to valid filters.\n",
      "\n",
      "retrievers.self_query.elasticsearch.ElasticsearchTranslator()\n",
      "Translate Elasticsearch internal query language elements to valid filters.\n",
      "\n",
      "retrievers.self_query.milvus.MilvusTranslator()\n",
      "Translate Milvus internal query language elements to valid filters.\n",
      "\n",
      "retrievers.self_query.myscale.MyScaleTranslator([...])\n",
      "Translate MyScale internal query language elements to valid filters.\n",
      "\n",
      "retrievers.self_query.opensearch.OpenSearchTranslator()\n",
      "Translate OpenSearch internal query domain-specific language elements to valid filters.\n",
      "\n",
      "retrievers.self_query.pinecone.PineconeTranslator()\n",
      "Translate Pinecone internal query language elements to valid filters.\n",
      "\n",
      "retrievers.self_query.qdrant.QdrantTranslator(...)\n",
      "Translate Qdrant internal query language elements to valid filters.\n",
      "\n",
      "retrievers.self_query.redis.RedisTranslator(schema)\n",
      "Translate\n",
      "\n",
      "retrievers.self_query.supabase.SupabaseVectorTranslator()\n",
      "Translate Langchain filters to Supabase PostgREST filters.\n",
      "\n",
      "retrievers.self_query.timescalevector.TimescaleVectorTranslator()\n",
      "Translate the internal query language elements to valid filters.\n",
      "\n",
      "retrievers.self_query.vectara.VectaraTranslator()\n",
      "Translate Vectara internal query language elements to valid filters.\n",
      "\n",
      "retrievers.self_query.weaviate.WeaviateTranslator()\n",
      "Translate Weaviate internal query language elements to valid filters.\n",
      "\n",
      "retrievers.svm.SVMRetriever\n",
      "SVM retriever.\n",
      "\n",
      "retrievers.tavily_search_api.SearchDepth(value)\n",
      "Search depth as enumerator.\n",
      "\n",
      "retrievers.tavily_search_api.TavilySearchAPIRetriever\n",
      "Tavily Search API retriever.\n",
      "\n",
      "retrievers.tfidf.TFIDFRetriever\n",
      "TF-IDF retriever.\n",
      "\n",
      "retrievers.time_weighted_retriever.TimeWeightedVectorStoreRetriever\n",
      "Retriever that combines embedding similarity with recency in retrieving values.\n",
      "\n",
      "retrievers.vespa_retriever.VespaRetriever\n",
      "Vespa retriever.\n",
      "\n",
      "retrievers.weaviate_hybrid_search.WeaviateHybridSearchRetriever\n",
      "Weaviate hybrid search retriever.\n",
      "\n",
      "retrievers.web_research.LineList\n",
      "List of questions.\n",
      "\n",
      "retrievers.web_research.QuestionListOutputParser\n",
      "Output parser for a list of numbered questions.\n",
      "\n",
      "retrievers.web_research.SearchQueries\n",
      "Search queries to research for the user's goal.\n",
      "\n",
      "retrievers.web_research.WebResearchRetriever\n",
      "Google Search API retriever.\n",
      "\n",
      "retrievers.wikipedia.WikipediaRetriever\n",
      "Wikipedia API retriever.\n",
      "\n",
      "retrievers.you.YouRetriever\n",
      "You retriever that uses You.com's search API.\n",
      "\n",
      "retrievers.zep.SearchScope(value[, names, ...])\n",
      "Which documents to search.\n",
      "\n",
      "retrievers.zep.SearchType(value[, names, ...])\n",
      "Enumerator of the types of search to perform.\n",
      "\n",
      "retrievers.zep.ZepRetriever\n",
      "Zep MemoryStore Retriever.\n",
      "\n",
      "retrievers.zilliz.ZillizRetriever\n",
      "Zilliz API retriever.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "retrievers.bm25.default_preprocessing_func(text)\n",
      "\n",
      "retrievers.document_compressors.chain_extract.default_get_input(...)\n",
      "Return the compression chain input.\n",
      "\n",
      "retrievers.document_compressors.chain_filter.default_get_input(...)\n",
      "Return the compression chain input.\n",
      "\n",
      "retrievers.kendra.clean_excerpt(excerpt)\n",
      "Clean an excerpt from Kendra.\n",
      "\n",
      "retrievers.kendra.combined_text(item)\n",
      "Combine a ResultItem title and excerpt into a single string.\n",
      "\n",
      "retrievers.knn.create_index(contexts, embeddings)\n",
      "Create an index of embeddings for a list of contexts.\n",
      "\n",
      "retrievers.milvus.MilvusRetreiver(*args, ...)\n",
      "Deprecated MilvusRetreiver.\n",
      "\n",
      "retrievers.pinecone_hybrid_search.create_index(...)\n",
      "Create an index from a list of contexts.\n",
      "\n",
      "retrievers.pinecone_hybrid_search.hash_text(text)\n",
      "Hash a text using SHA256.\n",
      "\n",
      "retrievers.self_query.deeplake.can_cast_to_float(string)\n",
      "Check if a string can be cast to a float.\n",
      "\n",
      "retrievers.self_query.milvus.process_value(value)\n",
      "Convert a value to a string and add double quotes if it is a string.\n",
      "\n",
      "retrievers.self_query.vectara.process_value(value)\n",
      "Convert a value to a string and add single quotes if it is a string.\n",
      "\n",
      "retrievers.svm.create_index(contexts, embeddings)\n",
      "Create an index of embeddings for a list of contexts.\n",
      "\n",
      "retrievers.zilliz.ZillizRetreiver(*args, ...)\n",
      "Deprecated ZillizRetreiver.\n",
      "\n",
      "langchain.runnables¶\n",
      "\n",
      "Classes¶\n",
      "\n",
      "runnables.hub.HubRunnable\n",
      "An instance of a runnable stored in the LangChain Hub.\n",
      "\n",
      "runnables.openai_functions.OpenAIFunction\n",
      "A function description for ChatOpenAI\n",
      "\n",
      "runnables.openai_functions.OpenAIFunctionsRouter\n",
      "A runnable that routes to the selected function.\n",
      "\n",
      "langchain.smith¶\n",
      "LangSmith utilities.\n",
      "This module provides utilities for connecting to LangSmith. For more information on LangSmith, see the LangSmith documentation.\n",
      "Evaluation\n",
      "LangSmith helps you evaluate Chains and other language model application components using a number of LangChain evaluators.\n",
      "An example of this is shown below, assuming you’ve created a LangSmith dataset called <my_dataset_name>:\n",
      "from langsmith import Client\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.chains import LLMChain\n",
      "from langchain.smith import RunEvalConfig, run_on_dataset\n",
      "\n",
      "# Chains may have memory. Passing in a constructor function lets the\n",
      "# evaluation framework avoid cross-contamination between runs.\n",
      "def construct_chain():\n",
      "    llm = ChatOpenAI(temperature=0)\n",
      "    chain = LLMChain.from_string(\n",
      "        llm,\n",
      "        \"What's the answer to {your_input_key}\"\n",
      "    )\n",
      "    return chain\n",
      "\n",
      "# Load off-the-shelf evaluators via config or the EvaluatorType (string or enum)\n",
      "evaluation_config = RunEvalConfig(\n",
      "    evaluators=[\n",
      "        \"qa\",  # \"Correctness\" against a reference answer\n",
      "        \"embedding_distance\",\n",
      "        RunEvalConfig.Criteria(\"helpfulness\"),\n",
      "        RunEvalConfig.Criteria({\n",
      "            \"fifth-grader-score\": \"Do you have to be smarter than a fifth grader to answer this question?\"\n",
      "        }),\n",
      "    ]\n",
      ")\n",
      "\n",
      "client = Client()\n",
      "run_on_dataset(\n",
      "    client,\n",
      "    \"<my_dataset_name>\",\n",
      "    construct_chain,\n",
      "    evaluation=evaluation_config,\n",
      ")\n",
      "\n",
      "You can also create custom evaluators by subclassing the\n",
      "StringEvaluator\n",
      "or LangSmith’s RunEvaluator classes.\n",
      "from typing import Optional\n",
      "from langchain.evaluation import StringEvaluator\n",
      "\n",
      "class MyStringEvaluator(StringEvaluator):\n",
      "\n",
      "    @property\n",
      "    def requires_input(self) -> bool:\n",
      "        return False\n",
      "\n",
      "    @property\n",
      "    def requires_reference(self) -> bool:\n",
      "        return True\n",
      "\n",
      "    @property\n",
      "    def evaluation_name(self) -> str:\n",
      "        return \"exact_match\"\n",
      "\n",
      "    def _evaluate_strings(self, prediction, reference=None, input=None, **kwargs) -> dict:\n",
      "        return {\"score\": prediction == reference}\n",
      "\n",
      "evaluation_config = RunEvalConfig(\n",
      "    custom_evaluators = [MyStringEvaluator()],\n",
      ")\n",
      "\n",
      "run_on_dataset(\n",
      "    client,\n",
      "    \"<my_dataset_name>\",\n",
      "    construct_chain,\n",
      "    evaluation=evaluation_config,\n",
      ")\n",
      "\n",
      "Primary Functions\n",
      "\n",
      "arun_on_dataset: Asynchronous function to evaluate a chain, agent, or other LangChain component over a dataset.\n",
      "run_on_dataset: Function to evaluate a chain, agent, or other LangChain component over a dataset.\n",
      "RunEvalConfig: Class representing the configuration for running evaluation. You can select evaluators by EvaluatorType or config, or you can pass in custom_evaluators\n",
      "\n",
      "Classes¶\n",
      "\n",
      "smith.evaluation.config.EvalConfig\n",
      "Configuration for a given run evaluator.\n",
      "\n",
      "smith.evaluation.config.RunEvalConfig\n",
      "Configuration for a run evaluation.\n",
      "\n",
      "smith.evaluation.config.SingleKeyEvalConfig\n",
      "Create a new model by parsing and validating input data from keyword arguments.\n",
      "\n",
      "smith.evaluation.progress.ProgressBarCallback(total)\n",
      "A simple progress bar for the console.\n",
      "\n",
      "smith.evaluation.runner_utils.EvalError(...)\n",
      "Your architecture raised an error.\n",
      "\n",
      "smith.evaluation.runner_utils.InputFormatError\n",
      "Raised when the input format is invalid.\n",
      "\n",
      "smith.evaluation.runner_utils.TestResult\n",
      "A dictionary of the results of a single test run.\n",
      "\n",
      "smith.evaluation.string_run_evaluator.ChainStringRunMapper\n",
      "Extract items to evaluate from the run object from a chain.\n",
      "\n",
      "smith.evaluation.string_run_evaluator.LLMStringRunMapper\n",
      "Extract items to evaluate from the run object.\n",
      "\n",
      "smith.evaluation.string_run_evaluator.StringExampleMapper\n",
      "Map an example, or row in the dataset, to the inputs of an evaluation.\n",
      "\n",
      "smith.evaluation.string_run_evaluator.StringRunEvaluatorChain\n",
      "Evaluate Run and optional examples.\n",
      "\n",
      "smith.evaluation.string_run_evaluator.StringRunMapper\n",
      "Extract items to evaluate from the run object.\n",
      "\n",
      "smith.evaluation.string_run_evaluator.ToolStringRunMapper\n",
      "Map an input to the tool.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "smith.evaluation.name_generation.random_name()\n",
      "Generate a random name.\n",
      "\n",
      "smith.evaluation.runner_utils.arun_on_dataset(...)\n",
      "Run the Chain or language model on a dataset and store traces to the specified project name.\n",
      "\n",
      "smith.evaluation.runner_utils.run_on_dataset(...)\n",
      "Run the Chain or language model on a dataset and store traces to the specified project name.\n",
      "\n",
      "langchain.storage¶\n",
      "Implementations of key-value stores and storage helpers.\n",
      "Module provides implementations of various key-value stores that conform\n",
      "to a simple key-value interface.\n",
      "The primary goal of these storages is to support implementation of caching.\n",
      "\n",
      "Classes¶\n",
      "\n",
      "storage.encoder_backed.EncoderBackedStore(...)\n",
      "Wraps a store with key and value encoders/decoders.\n",
      "\n",
      "storage.exceptions.InvalidKeyException\n",
      "Raised when a key is invalid; e.g., uses incorrect characters.\n",
      "\n",
      "storage.file_system.LocalFileStore(root_path)\n",
      "BaseStore interface that works on the local file system.\n",
      "\n",
      "storage.in_memory.InMemoryStore()\n",
      "In-memory implementation of the BaseStore using a dictionary.\n",
      "\n",
      "storage.redis.RedisStore(*[, client, ...])\n",
      "BaseStore implementation using Redis as the underlying store.\n",
      "\n",
      "storage.upstash_redis.UpstashRedisStore(*[, ...])\n",
      "BaseStore implementation using Upstash Redis as the underlying store.\n",
      "\n",
      "langchain.text_splitter¶\n",
      "Text Splitters are classes for splitting text.\n",
      "Class hierarchy:\n",
      "BaseDocumentTransformer --> TextSplitter --> <name>TextSplitter  # Example: CharacterTextSplitter\n",
      "                                             RecursiveCharacterTextSplitter -->  <name>TextSplitter\n",
      "\n",
      "Note: MarkdownHeaderTextSplitter and **HTMLHeaderTextSplitter do not derive from TextSplitter.\n",
      "Main helpers:\n",
      "Document, Tokenizer, Language, LineType, HeaderType\n",
      "\n",
      "Classes¶\n",
      "\n",
      "text_splitter.CharacterTextSplitter([...])\n",
      "Splitting text that looks at characters.\n",
      "\n",
      "text_splitter.ElementType\n",
      "Element type as typed dict.\n",
      "\n",
      "text_splitter.HTMLHeaderTextSplitter(...[, ...])\n",
      "Splitting HTML files based on specified headers.\n",
      "\n",
      "text_splitter.HeaderType\n",
      "Header type as typed dict.\n",
      "\n",
      "text_splitter.Language(value[, names, ...])\n",
      "Enum of the programming languages.\n",
      "\n",
      "text_splitter.LatexTextSplitter(**kwargs)\n",
      "Attempts to split the text along Latex-formatted layout elements.\n",
      "\n",
      "text_splitter.LineType\n",
      "Line type as typed dict.\n",
      "\n",
      "text_splitter.MarkdownHeaderTextSplitter(...)\n",
      "Splitting markdown files based on specified headers.\n",
      "\n",
      "text_splitter.MarkdownTextSplitter(**kwargs)\n",
      "Attempts to split the text along Markdown-formatted headings.\n",
      "\n",
      "text_splitter.NLTKTextSplitter([separator, ...])\n",
      "Splitting text using NLTK package.\n",
      "\n",
      "text_splitter.PythonCodeTextSplitter(**kwargs)\n",
      "Attempts to split the text along Python syntax.\n",
      "\n",
      "text_splitter.RecursiveCharacterTextSplitter([...])\n",
      "Splitting text by recursively look at characters.\n",
      "\n",
      "text_splitter.SentenceTransformersTokenTextSplitter([...])\n",
      "Splitting text to tokens using sentence model tokenizer.\n",
      "\n",
      "text_splitter.SpacyTextSplitter([separator, ...])\n",
      "Splitting text using Spacy package.\n",
      "\n",
      "text_splitter.TextSplitter(chunk_size, ...)\n",
      "Interface for splitting text into chunks.\n",
      "\n",
      "text_splitter.TokenTextSplitter([...])\n",
      "Splitting text to tokens using model tokenizer.\n",
      "\n",
      "text_splitter.Tokenizer(chunk_overlap, ...)\n",
      "Tokenizer data class.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "text_splitter.split_text_on_tokens(*, text, ...)\n",
      "Split incoming text and return chunks using tokenizer.\n",
      "\n",
      "langchain.tools¶\n",
      "Tools are classes that an Agent uses to interact with the world.\n",
      "Each tool has a description. Agent uses the description to choose the right\n",
      "tool for the job.\n",
      "Class hierarchy:\n",
      "ToolMetaclass --> BaseTool --> <name>Tool  # Examples: AIPluginTool, BaseGraphQLTool\n",
      "                               <name>      # Examples: BraveSearch, HumanInputRun\n",
      "\n",
      "Main helpers:\n",
      "CallbackManagerForToolRun, AsyncCallbackManagerForToolRun\n",
      "\n",
      "Classes¶\n",
      "\n",
      "tools.ainetwork.app.AINAppOps\n",
      "Tool for app operations.\n",
      "\n",
      "tools.ainetwork.app.AppOperationType(value)\n",
      "Type of app operation as enumerator.\n",
      "\n",
      "tools.ainetwork.app.AppSchema\n",
      "Schema for app operations.\n",
      "\n",
      "tools.ainetwork.base.AINBaseTool\n",
      "Base class for the AINetwork tools.\n",
      "\n",
      "tools.ainetwork.base.OperationType(value[, ...])\n",
      "Type of operation as enumerator.\n",
      "\n",
      "tools.ainetwork.owner.AINOwnerOps\n",
      "Tool for owner operations.\n",
      "\n",
      "tools.ainetwork.owner.RuleSchema\n",
      "Schema for owner operations.\n",
      "\n",
      "tools.ainetwork.rule.AINRuleOps\n",
      "Tool for owner operations.\n",
      "\n",
      "tools.ainetwork.rule.RuleSchema\n",
      "Schema for owner operations.\n",
      "\n",
      "tools.ainetwork.transfer.AINTransfer\n",
      "Tool for transfer operations.\n",
      "\n",
      "tools.ainetwork.transfer.TransferSchema\n",
      "Schema for transfer operations.\n",
      "\n",
      "tools.ainetwork.value.AINValueOps\n",
      "Tool for value operations.\n",
      "\n",
      "tools.ainetwork.value.ValueSchema\n",
      "Schema for value operations.\n",
      "\n",
      "tools.amadeus.base.AmadeusBaseTool\n",
      "Base Tool for Amadeus.\n",
      "\n",
      "tools.amadeus.closest_airport.AmadeusClosestAirport\n",
      "Tool for finding the closest airport to a particular location.\n",
      "\n",
      "tools.amadeus.closest_airport.ClosestAirportSchema\n",
      "Schema for the AmadeusClosestAirport tool.\n",
      "\n",
      "tools.amadeus.flight_search.AmadeusFlightSearch\n",
      "Tool for searching for a single flight between two airports.\n",
      "\n",
      "tools.amadeus.flight_search.FlightSearchSchema\n",
      "Schema for the AmadeusFlightSearch tool.\n",
      "\n",
      "tools.arxiv.tool.ArxivInput\n",
      "Create a new model by parsing and validating input data from keyword arguments.\n",
      "\n",
      "tools.arxiv.tool.ArxivQueryRun\n",
      "Tool that searches the Arxiv API.\n",
      "\n",
      "tools.azure_cognitive_services.form_recognizer.AzureCogsFormRecognizerTool\n",
      "Tool that queries the Azure Cognitive Services Form Recognizer API.\n",
      "\n",
      "tools.azure_cognitive_services.image_analysis.AzureCogsImageAnalysisTool\n",
      "Tool that queries the Azure Cognitive Services Image Analysis API.\n",
      "\n",
      "tools.azure_cognitive_services.speech2text.AzureCogsSpeech2TextTool\n",
      "Tool that queries the Azure Cognitive Services Speech2Text API.\n",
      "\n",
      "tools.azure_cognitive_services.text2speech.AzureCogsText2SpeechTool\n",
      "Tool that queries the Azure Cognitive Services Text2Speech API.\n",
      "\n",
      "tools.azure_cognitive_services.text_analytics_health.AzureCogsTextAnalyticsHealthTool\n",
      "Tool that queries the Azure Cognitive Services Text Analytics for Health API.\n",
      "\n",
      "tools.bearly.tool.BearlyInterpreterTool(api_key)\n",
      "Tool for evaluating python code in a sandbox environment.\n",
      "\n",
      "tools.bearly.tool.BearlyInterpreterToolArguments\n",
      "Arguments for the BearlyInterpreterTool.\n",
      "\n",
      "tools.bearly.tool.FileInfo\n",
      "Information about a file to be uploaded.\n",
      "\n",
      "tools.bing_search.tool.BingSearchResults\n",
      "Tool that queries the Bing Search API and gets back json.\n",
      "\n",
      "tools.bing_search.tool.BingSearchRun\n",
      "Tool that queries the Bing search API.\n",
      "\n",
      "tools.brave_search.tool.BraveSearch\n",
      "Tool that queries the BraveSearch.\n",
      "\n",
      "tools.clickup.tool.ClickupAction\n",
      "Tool that queries the  Clickup API.\n",
      "\n",
      "tools.dataforseo_api_search.tool.DataForSeoAPISearchResults\n",
      "Tool that queries the DataForSeo Google Search API and get back json.\n",
      "\n",
      "tools.dataforseo_api_search.tool.DataForSeoAPISearchRun\n",
      "Tool that queries the DataForSeo Google search API.\n",
      "\n",
      "tools.ddg_search.tool.DDGInput\n",
      "Create a new model by parsing and validating input data from keyword arguments.\n",
      "\n",
      "tools.ddg_search.tool.DuckDuckGoSearchResults\n",
      "Tool that queries the DuckDuckGo search API and gets back json.\n",
      "\n",
      "tools.ddg_search.tool.DuckDuckGoSearchRun\n",
      "Tool that queries the DuckDuckGo search API.\n",
      "\n",
      "tools.e2b_data_analysis.tool.E2BDataAnalysisTool\n",
      "Tool for running python code in a sandboxed environment for data analysis.\n",
      "\n",
      "tools.e2b_data_analysis.tool.E2BDataAnalysisToolArguments\n",
      "Arguments for the E2BDataAnalysisTool.\n",
      "\n",
      "tools.e2b_data_analysis.tool.UploadedFile\n",
      "Description of the uploaded path with its remote path.\n",
      "\n",
      "tools.e2b_data_analysis.unparse.Unparser(tree)\n",
      "Methods in this class recursively traverse an AST and output source code for the abstract syntax; original formatting is disregarded.\n",
      "\n",
      "tools.edenai.audio_speech_to_text.EdenAiSpeechToTextTool\n",
      "Tool that queries the Eden AI Speech To Text API.\n",
      "\n",
      "tools.edenai.audio_text_to_speech.EdenAiTextToSpeechTool\n",
      "Tool that queries the Eden AI Text to speech API.\n",
      "\n",
      "tools.edenai.edenai_base_tool.EdenaiTool\n",
      "the base tool for all the EdenAI Tools .\n",
      "\n",
      "tools.edenai.image_explicitcontent.EdenAiExplicitImageTool\n",
      "Tool that queries the Eden AI Explicit image detection.\n",
      "\n",
      "tools.edenai.image_objectdetection.EdenAiObjectDetectionTool\n",
      "Tool that queries the Eden AI Object detection API.\n",
      "\n",
      "tools.edenai.ocr_identityparser.EdenAiParsingIDTool\n",
      "Tool that queries the Eden AI  Identity parsing API.\n",
      "\n",
      "tools.edenai.ocr_invoiceparser.EdenAiParsingInvoiceTool\n",
      "Tool that queries the Eden AI Invoice parsing API.\n",
      "\n",
      "tools.edenai.text_moderation.EdenAiTextModerationTool\n",
      "Tool that queries the Eden AI Explicit text detection.\n",
      "\n",
      "tools.eleven_labs.models.ElevenLabsModel(value)\n",
      "Models available for Eleven Labs Text2Speech.\n",
      "\n",
      "tools.eleven_labs.text2speech.ElevenLabsModel(value)\n",
      "Models available for Eleven Labs Text2Speech.\n",
      "\n",
      "tools.eleven_labs.text2speech.ElevenLabsText2SpeechTool\n",
      "Tool that queries the Eleven Labs Text2Speech API.\n",
      "\n",
      "tools.file_management.copy.CopyFileTool\n",
      "Tool that copies a file.\n",
      "\n",
      "tools.file_management.copy.FileCopyInput\n",
      "Input for CopyFileTool.\n",
      "\n",
      "tools.file_management.delete.DeleteFileTool\n",
      "Tool that deletes a file.\n",
      "\n",
      "tools.file_management.delete.FileDeleteInput\n",
      "Input for DeleteFileTool.\n",
      "\n",
      "tools.file_management.file_search.FileSearchInput\n",
      "Input for FileSearchTool.\n",
      "\n",
      "tools.file_management.file_search.FileSearchTool\n",
      "Tool that searches for files in a subdirectory that match a regex pattern.\n",
      "\n",
      "tools.file_management.list_dir.DirectoryListingInput\n",
      "Input for ListDirectoryTool.\n",
      "\n",
      "tools.file_management.list_dir.ListDirectoryTool\n",
      "Tool that lists files and directories in a specified folder.\n",
      "\n",
      "tools.file_management.move.FileMoveInput\n",
      "Input for MoveFileTool.\n",
      "\n",
      "tools.file_management.move.MoveFileTool\n",
      "Tool that moves a file.\n",
      "\n",
      "tools.file_management.read.ReadFileInput\n",
      "Input for ReadFileTool.\n",
      "\n",
      "tools.file_management.read.ReadFileTool\n",
      "Tool that reads a file.\n",
      "\n",
      "tools.file_management.utils.BaseFileToolMixin\n",
      "Mixin for file system tools.\n",
      "\n",
      "tools.file_management.utils.FileValidationError\n",
      "Error for paths outside the root directory.\n",
      "\n",
      "tools.file_management.write.WriteFileInput\n",
      "Input for WriteFileTool.\n",
      "\n",
      "tools.file_management.write.WriteFileTool\n",
      "Tool that writes a file to disk.\n",
      "\n",
      "tools.github.tool.GitHubAction\n",
      "Tool for interacting with the GitHub API.\n",
      "\n",
      "tools.gitlab.tool.GitLabAction\n",
      "Tool for interacting with the GitLab API.\n",
      "\n",
      "tools.gmail.base.GmailBaseTool\n",
      "Base class for Gmail tools.\n",
      "\n",
      "tools.gmail.create_draft.CreateDraftSchema\n",
      "Input for CreateDraftTool.\n",
      "\n",
      "tools.gmail.create_draft.GmailCreateDraft\n",
      "Tool that creates a draft email for Gmail.\n",
      "\n",
      "tools.gmail.get_message.GmailGetMessage\n",
      "Tool that gets a message by ID from Gmail.\n",
      "\n",
      "tools.gmail.get_message.SearchArgsSchema\n",
      "Input for GetMessageTool.\n",
      "\n",
      "tools.gmail.get_thread.GetThreadSchema\n",
      "Input for GetMessageTool.\n",
      "\n",
      "tools.gmail.get_thread.GmailGetThread\n",
      "Tool that gets a thread by ID from Gmail.\n",
      "\n",
      "tools.gmail.search.GmailSearch\n",
      "Tool that searches for messages or threads in Gmail.\n",
      "\n",
      "tools.gmail.search.Resource(value[, names, ...])\n",
      "Enumerator of Resources to search.\n",
      "\n",
      "tools.gmail.search.SearchArgsSchema\n",
      "Input for SearchGmailTool.\n",
      "\n",
      "tools.gmail.send_message.GmailSendMessage\n",
      "Tool that sends a message to Gmail.\n",
      "\n",
      "tools.gmail.send_message.SendMessageSchema\n",
      "Input for SendMessageTool.\n",
      "\n",
      "tools.golden_query.tool.GoldenQueryRun\n",
      "Tool that adds the capability to query using the Golden API and get back JSON.\n",
      "\n",
      "tools.google_cloud.texttospeech.GoogleCloudTextToSpeechTool\n",
      "Tool that queries the Google Cloud Text to Speech API.\n",
      "\n",
      "tools.google_places.tool.GooglePlacesSchema\n",
      "Input for GooglePlacesTool.\n",
      "\n",
      "tools.google_places.tool.GooglePlacesTool\n",
      "Tool that queries the Google places API.\n",
      "\n",
      "tools.google_scholar.tool.GoogleScholarQueryRun\n",
      "Tool that queries the Google search API.\n",
      "\n",
      "tools.google_search.tool.GoogleSearchResults\n",
      "Tool that queries the Google Search API and gets back json.\n",
      "\n",
      "tools.google_search.tool.GoogleSearchRun\n",
      "Tool that queries the Google search API.\n",
      "\n",
      "tools.google_serper.tool.GoogleSerperResults\n",
      "Tool that queries the Serper.dev Google Search API and get back json.\n",
      "\n",
      "tools.google_serper.tool.GoogleSerperRun\n",
      "Tool that queries the Serper.dev Google search API.\n",
      "\n",
      "tools.graphql.tool.BaseGraphQLTool\n",
      "Base tool for querying a GraphQL API.\n",
      "\n",
      "tools.human.tool.HumanInputRun\n",
      "Tool that asks user for input.\n",
      "\n",
      "tools.ifttt.IFTTTWebhook\n",
      "IFTTT Webhook.\n",
      "\n",
      "tools.jira.tool.JiraAction\n",
      "Tool that queries the Atlassian Jira API.\n",
      "\n",
      "tools.json.tool.JsonGetValueTool\n",
      "Tool for getting a value in a JSON spec.\n",
      "\n",
      "tools.json.tool.JsonListKeysTool\n",
      "Tool for listing keys in a JSON spec.\n",
      "\n",
      "tools.json.tool.JsonSpec\n",
      "Base class for JSON spec.\n",
      "\n",
      "tools.memorize.tool.Memorize\n",
      "Create a new model by parsing and validating input data from keyword arguments.\n",
      "\n",
      "tools.memorize.tool.TrainableLLM(*args, **kwargs)\n",
      "\n",
      "tools.metaphor_search.tool.MetaphorSearchResults\n",
      "Tool that queries the Metaphor Search API and gets back json.\n",
      "\n",
      "tools.multion.close_session.CloseSessionSchema\n",
      "Input for UpdateSessionTool.\n",
      "\n",
      "tools.multion.close_session.MultionCloseSession\n",
      "Tool that closes an existing Multion Browser Window with provided fields.\n",
      "\n",
      "tools.multion.create_session.CreateSessionSchema\n",
      "Input for CreateSessionTool.\n",
      "\n",
      "tools.multion.create_session.MultionCreateSession\n",
      "Tool that creates a new Multion Browser Window with provided fields.\n",
      "\n",
      "tools.multion.update_session.MultionUpdateSession\n",
      "Tool that updates an existing Multion Browser Window with provided fields.\n",
      "\n",
      "tools.multion.update_session.UpdateSessionSchema\n",
      "Input for UpdateSessionTool.\n",
      "\n",
      "tools.nuclia.tool.NUASchema\n",
      "Input for Nuclia Understanding API.\n",
      "\n",
      "tools.nuclia.tool.NucliaUnderstandingAPI\n",
      "Tool to process files with the Nuclia Understanding API.\n",
      "\n",
      "tools.office365.base.O365BaseTool\n",
      "Base class for the Office 365 tools.\n",
      "\n",
      "tools.office365.create_draft_message.CreateDraftMessageSchema\n",
      "Input for SendMessageTool.\n",
      "\n",
      "tools.office365.create_draft_message.O365CreateDraftMessage\n",
      "Tool for creating a draft email in Office 365.\n",
      "\n",
      "tools.office365.events_search.O365SearchEvents\n",
      "Class for searching calendar events in Office 365\n",
      "\n",
      "tools.office365.events_search.SearchEventsInput\n",
      "Input for SearchEmails Tool.\n",
      "\n",
      "tools.office365.messages_search.O365SearchEmails\n",
      "Class for searching email messages in Office 365\n",
      "\n",
      "tools.office365.messages_search.SearchEmailsInput\n",
      "Input for SearchEmails Tool.\n",
      "\n",
      "tools.office365.send_event.O365SendEvent\n",
      "Tool for sending calendar events in Office 365.\n",
      "\n",
      "tools.office365.send_event.SendEventSchema\n",
      "Input for CreateEvent Tool.\n",
      "\n",
      "tools.office365.send_message.O365SendMessage\n",
      "Tool for sending an email in Office 365.\n",
      "\n",
      "tools.office365.send_message.SendMessageSchema\n",
      "Input for SendMessageTool.\n",
      "\n",
      "tools.openapi.utils.api_models.APIOperation\n",
      "A model for a single API operation.\n",
      "\n",
      "tools.openapi.utils.api_models.APIProperty\n",
      "A model for a property in the query, path, header, or cookie params.\n",
      "\n",
      "tools.openapi.utils.api_models.APIPropertyBase\n",
      "Base model for an API property.\n",
      "\n",
      "tools.openapi.utils.api_models.APIPropertyLocation(value)\n",
      "The location of the property.\n",
      "\n",
      "tools.openapi.utils.api_models.APIRequestBody\n",
      "A model for a request body.\n",
      "\n",
      "tools.openapi.utils.api_models.APIRequestBodyProperty\n",
      "A model for a request body property.\n",
      "\n",
      "tools.openweathermap.tool.OpenWeatherMapQueryRun\n",
      "Tool that queries the OpenWeatherMap API.\n",
      "\n",
      "tools.playwright.base.BaseBrowserTool\n",
      "Base class for browser tools.\n",
      "\n",
      "tools.playwright.click.ClickTool\n",
      "Tool for clicking on an element with the given CSS selector.\n",
      "\n",
      "tools.playwright.click.ClickToolInput\n",
      "Input for ClickTool.\n",
      "\n",
      "tools.playwright.current_page.CurrentWebPageTool\n",
      "Tool for getting the URL of the current webpage.\n",
      "\n",
      "tools.playwright.extract_hyperlinks.ExtractHyperlinksTool\n",
      "Extract all hyperlinks on the page.\n",
      "\n",
      "tools.playwright.extract_hyperlinks.ExtractHyperlinksToolInput\n",
      "Input for ExtractHyperlinksTool.\n",
      "\n",
      "tools.playwright.extract_text.ExtractTextTool\n",
      "Tool for extracting all the text on the current webpage.\n",
      "\n",
      "tools.playwright.get_elements.GetElementsTool\n",
      "Tool for getting elements in the current web page matching a CSS selector.\n",
      "\n",
      "tools.playwright.get_elements.GetElementsToolInput\n",
      "Input for GetElementsTool.\n",
      "\n",
      "tools.playwright.navigate.NavigateTool\n",
      "Tool for navigating a browser to a URL.\n",
      "\n",
      "tools.playwright.navigate.NavigateToolInput\n",
      "Input for NavigateToolInput.\n",
      "\n",
      "tools.playwright.navigate_back.NavigateBackTool\n",
      "Navigate back to the previous page in the browser history.\n",
      "\n",
      "tools.plugin.AIPlugin\n",
      "AI Plugin Definition.\n",
      "\n",
      "tools.plugin.AIPluginTool\n",
      "Tool for getting the OpenAPI spec for an AI Plugin.\n",
      "\n",
      "tools.plugin.AIPluginToolSchema\n",
      "Schema for AIPluginTool.\n",
      "\n",
      "tools.plugin.ApiConfig\n",
      "API Configuration.\n",
      "\n",
      "tools.powerbi.tool.InfoPowerBITool\n",
      "Tool for getting metadata about a PowerBI Dataset.\n",
      "\n",
      "tools.powerbi.tool.ListPowerBITool\n",
      "Tool for getting tables names.\n",
      "\n",
      "tools.powerbi.tool.QueryPowerBITool\n",
      "Tool for querying a Power BI Dataset.\n",
      "\n",
      "tools.pubmed.tool.PubmedQueryRun\n",
      "Tool that searches the PubMed API.\n",
      "\n",
      "tools.requests.tool.BaseRequestsTool\n",
      "Base class for requests tools.\n",
      "\n",
      "tools.requests.tool.RequestsDeleteTool\n",
      "Tool for making a DELETE request to an API endpoint.\n",
      "\n",
      "tools.requests.tool.RequestsGetTool\n",
      "Tool for making a GET request to an API endpoint.\n",
      "\n",
      "tools.requests.tool.RequestsPatchTool\n",
      "Tool for making a PATCH request to an API endpoint.\n",
      "\n",
      "tools.requests.tool.RequestsPostTool\n",
      "Tool for making a POST request to an API endpoint.\n",
      "\n",
      "tools.requests.tool.RequestsPutTool\n",
      "Tool for making a PUT request to an API endpoint.\n",
      "\n",
      "tools.retriever.RetrieverInput\n",
      "Create a new model by parsing and validating input data from keyword arguments.\n",
      "\n",
      "tools.scenexplain.tool.SceneXplainInput\n",
      "Input for SceneXplain.\n",
      "\n",
      "tools.scenexplain.tool.SceneXplainTool\n",
      "Tool that explains images.\n",
      "\n",
      "tools.searchapi.tool.SearchAPIResults\n",
      "Tool that queries the SearchApi.io search API and returns JSON.\n",
      "\n",
      "tools.searchapi.tool.SearchAPIRun\n",
      "Tool that queries the SearchApi.io search API.\n",
      "\n",
      "tools.searx_search.tool.SearxSearchResults\n",
      "Tool that queries a Searx instance and gets back json.\n",
      "\n",
      "tools.searx_search.tool.SearxSearchRun\n",
      "Tool that queries a Searx instance.\n",
      "\n",
      "tools.shell.tool.ShellInput\n",
      "Commands for the Bash Shell tool.\n",
      "\n",
      "tools.shell.tool.ShellTool\n",
      "Tool to run shell commands.\n",
      "\n",
      "tools.sleep.tool.SleepInput\n",
      "Input for CopyFileTool.\n",
      "\n",
      "tools.sleep.tool.SleepTool\n",
      "Tool that adds the capability to sleep.\n",
      "\n",
      "tools.spark_sql.tool.BaseSparkSQLTool\n",
      "Base tool for interacting with Spark SQL.\n",
      "\n",
      "tools.spark_sql.tool.InfoSparkSQLTool\n",
      "Tool for getting metadata about a Spark SQL.\n",
      "\n",
      "tools.spark_sql.tool.ListSparkSQLTool\n",
      "Tool for getting tables names.\n",
      "\n",
      "tools.spark_sql.tool.QueryCheckerTool\n",
      "Use an LLM to check if a query is correct.\n",
      "\n",
      "tools.spark_sql.tool.QuerySparkSQLTool\n",
      "Tool for querying a Spark SQL.\n",
      "\n",
      "tools.sql_database.tool.BaseSQLDatabaseTool\n",
      "Base tool for interacting with a SQL database.\n",
      "\n",
      "tools.sql_database.tool.InfoSQLDatabaseTool\n",
      "Tool for getting metadata about a SQL database.\n",
      "\n",
      "tools.sql_database.tool.ListSQLDatabaseTool\n",
      "Tool for getting tables names.\n",
      "\n",
      "tools.sql_database.tool.QuerySQLCheckerTool\n",
      "Use an LLM to check if a query is correct.\n",
      "\n",
      "tools.sql_database.tool.QuerySQLDataBaseTool\n",
      "Tool for querying a SQL database.\n",
      "\n",
      "tools.steamship_image_generation.tool.ModelName(value)\n",
      "Supported Image Models for generation.\n",
      "\n",
      "tools.steamship_image_generation.tool.SteamshipImageGenerationTool\n",
      "Tool used to generate images from a text-prompt.\n",
      "\n",
      "tools.tavily_search.tool.TavilyAnswer\n",
      "Tool that queries the Tavily Search API and gets back an answer.\n",
      "\n",
      "tools.tavily_search.tool.TavilyInput\n",
      "Create a new model by parsing and validating input data from keyword arguments.\n",
      "\n",
      "tools.tavily_search.tool.TavilySearchResults\n",
      "Tool that queries the Tavily Search API and gets back json.\n",
      "\n",
      "tools.vectorstore.tool.BaseVectorStoreTool\n",
      "Base class for tools that use a VectorStore.\n",
      "\n",
      "tools.vectorstore.tool.VectorStoreQATool\n",
      "Tool for the VectorDBQA chain.\n",
      "\n",
      "tools.vectorstore.tool.VectorStoreQAWithSourcesTool\n",
      "Tool for the VectorDBQAWithSources chain.\n",
      "\n",
      "tools.wikipedia.tool.WikipediaQueryRun\n",
      "Tool that searches the Wikipedia API.\n",
      "\n",
      "tools.wolfram_alpha.tool.WolframAlphaQueryRun\n",
      "Tool that queries using the Wolfram Alpha SDK.\n",
      "\n",
      "tools.yahoo_finance_news.YahooFinanceNewsTool\n",
      "Tool that searches financial news on Yahoo Finance.\n",
      "\n",
      "tools.youtube.search.YouTubeSearchTool\n",
      "Tool that queries YouTube.\n",
      "\n",
      "tools.zapier.tool.ZapierNLAListActions\n",
      "Returns a list of all exposed (enabled) actions associated with\n",
      "\n",
      "tools.zapier.tool.ZapierNLARunAction\n",
      "Executes an action that is identified by action_id, must be exposed\n",
      "\n",
      "Functions¶\n",
      "\n",
      "tools.ainetwork.utils.authenticate([network])\n",
      "Authenticate using the AIN Blockchain\n",
      "\n",
      "tools.amadeus.utils.authenticate()\n",
      "Authenticate using the Amadeus API\n",
      "\n",
      "tools.azure_cognitive_services.utils.detect_file_src_type(...)\n",
      "Detect if the file is local or remote.\n",
      "\n",
      "tools.azure_cognitive_services.utils.download_audio_from_url(...)\n",
      "Download audio from url to local.\n",
      "\n",
      "tools.bearly.tool.file_to_base64(path)\n",
      "Convert a file to base64.\n",
      "\n",
      "tools.bearly.tool.head_file(path, n)\n",
      "Get the first n lines of a file.\n",
      "\n",
      "tools.bearly.tool.strip_markdown_code(md_string)\n",
      "Strip markdown code from a string.\n",
      "\n",
      "tools.ddg_search.tool.DuckDuckGoSearchTool(...)\n",
      "Deprecated.\n",
      "\n",
      "tools.e2b_data_analysis.tool.add_last_line_print(code)\n",
      "Add print statement to the last line if it's missing.\n",
      "\n",
      "tools.e2b_data_analysis.unparse.interleave(...)\n",
      "Call f on each item in seq, calling inter() in between.\n",
      "\n",
      "tools.e2b_data_analysis.unparse.roundtrip(...)\n",
      "\n",
      "tools.file_management.utils.get_validated_relative_path(...)\n",
      "Resolve a relative path, raising an error if not within the root directory.\n",
      "\n",
      "tools.file_management.utils.is_relative_to(...)\n",
      "Check if path is relative to root.\n",
      "\n",
      "tools.gmail.utils.build_resource_service([...])\n",
      "Build a Gmail service.\n",
      "\n",
      "tools.gmail.utils.clean_email_body(body)\n",
      "Clean email body.\n",
      "\n",
      "tools.gmail.utils.get_gmail_credentials([...])\n",
      "Get credentials.\n",
      "\n",
      "tools.gmail.utils.import_google()\n",
      "Import google libraries.\n",
      "\n",
      "tools.gmail.utils.import_googleapiclient_resource_builder()\n",
      "Import googleapiclient.discovery.build function.\n",
      "\n",
      "tools.gmail.utils.import_installed_app_flow()\n",
      "Import InstalledAppFlow class.\n",
      "\n",
      "tools.interaction.tool.StdInInquireTool(...)\n",
      "Tool for asking the user for input.\n",
      "\n",
      "tools.office365.utils.authenticate()\n",
      "Authenticate using the Microsoft Grah API\n",
      "\n",
      "tools.office365.utils.clean_body(body)\n",
      "Clean body of a message or event.\n",
      "\n",
      "tools.playwright.base.lazy_import_playwright_browsers()\n",
      "Lazy import playwright browsers.\n",
      "\n",
      "tools.playwright.utils.aget_current_page(browser)\n",
      "Asynchronously get the current page of the browser.\n",
      "\n",
      "tools.playwright.utils.create_async_playwright_browser([...])\n",
      "Create an async playwright browser.\n",
      "\n",
      "tools.playwright.utils.create_sync_playwright_browser([...])\n",
      "Create a playwright browser.\n",
      "\n",
      "tools.playwright.utils.get_current_page(browser)\n",
      "Get the current page of the browser.\n",
      "\n",
      "tools.playwright.utils.run_async(coro)\n",
      "Run an async coroutine.\n",
      "\n",
      "tools.plugin.marshal_spec(txt)\n",
      "Convert the yaml or json serialized spec to a dict.\n",
      "\n",
      "tools.render.format_tool_to_openai_function(tool)\n",
      "Format tool into the OpenAI function API.\n",
      "\n",
      "tools.render.format_tool_to_openai_tool(tool)\n",
      "Format tool into the OpenAI function API.\n",
      "\n",
      "tools.render.render_text_description(tools)\n",
      "Render the tool name and description in plain text.\n",
      "\n",
      "tools.render.render_text_description_and_args(tools)\n",
      "Render the tool name, description, and args in plain text.\n",
      "\n",
      "tools.retriever.create_retriever_tool(...)\n",
      "Create a tool to do retrieval of documents.\n",
      "\n",
      "tools.steamship_image_generation.utils.make_image_public(...)\n",
      "Upload a block to a signed URL and return the public URL.\n",
      "\n",
      "langchain.tools.render¶\n",
      "Different methods for rendering Tools to be passed to LLMs.\n",
      "Depending on the LLM you are using and the prompting strategy you are using,\n",
      "you may want Tools to be rendered in a different way.\n",
      "This module contains various ways to render tools.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "tools.render.format_tool_to_openai_function(tool)\n",
      "Format tool into the OpenAI function API.\n",
      "\n",
      "tools.render.format_tool_to_openai_tool(tool)\n",
      "Format tool into the OpenAI function API.\n",
      "\n",
      "tools.render.render_text_description(tools)\n",
      "Render the tool name and description in plain text.\n",
      "\n",
      "tools.render.render_text_description_and_args(tools)\n",
      "Render the tool name, description, and args in plain text.\n",
      "\n",
      "langchain.utilities¶\n",
      "Utilities are the integrations with third-part systems and packages.\n",
      "Other LangChain classes use Utilities to interact with third-part systems\n",
      "and packages.\n",
      "\n",
      "Classes¶\n",
      "\n",
      "utilities.alpha_vantage.AlphaVantageAPIWrapper\n",
      "Wrapper for AlphaVantage API for Currency Exchange Rate.\n",
      "\n",
      "utilities.apify.ApifyWrapper\n",
      "Wrapper around Apify.\n",
      "\n",
      "utilities.arcee.ArceeDocument\n",
      "Arcee document.\n",
      "\n",
      "utilities.arcee.ArceeDocumentAdapter()\n",
      "Adapter for Arcee documents\n",
      "\n",
      "utilities.arcee.ArceeDocumentSource\n",
      "Source of an Arcee document.\n",
      "\n",
      "utilities.arcee.ArceeRoute(value[, names, ...])\n",
      "Routes available for the Arcee API as enumerator.\n",
      "\n",
      "utilities.arcee.ArceeWrapper(arcee_api_key, ...)\n",
      "Wrapper for Arcee API.\n",
      "\n",
      "utilities.arcee.DALMFilter\n",
      "Filters available for a DALM retrieval and generation.\n",
      "\n",
      "utilities.arcee.DALMFilterType(value[, ...])\n",
      "Filter types available for a DALM retrieval as enumerator.\n",
      "\n",
      "utilities.arxiv.ArxivAPIWrapper\n",
      "Wrapper around ArxivAPI.\n",
      "\n",
      "utilities.awslambda.LambdaWrapper\n",
      "Wrapper for AWS Lambda SDK.\n",
      "\n",
      "utilities.bibtex.BibtexparserWrapper\n",
      "Wrapper around bibtexparser.\n",
      "\n",
      "utilities.bing_search.BingSearchAPIWrapper\n",
      "Wrapper for Bing Search API.\n",
      "\n",
      "utilities.brave_search.BraveSearchWrapper\n",
      "Wrapper around the Brave search engine.\n",
      "\n",
      "utilities.clickup.CUList(folder_id, name[, ...])\n",
      "Component class for a list.\n",
      "\n",
      "utilities.clickup.ClickupAPIWrapper\n",
      "Wrapper for Clickup API.\n",
      "\n",
      "utilities.clickup.Component()\n",
      "Base class for all components.\n",
      "\n",
      "utilities.clickup.Member(id, username, ...)\n",
      "Component class for a member.\n",
      "\n",
      "utilities.clickup.Space(id, name, private, ...)\n",
      "Component class for a space.\n",
      "\n",
      "utilities.clickup.Task(id, name, ...)\n",
      "Class for a task.\n",
      "\n",
      "utilities.clickup.Team(id, name, members)\n",
      "Component class for a team.\n",
      "\n",
      "utilities.dalle_image_generator.DallEAPIWrapper\n",
      "Wrapper for OpenAI's DALL-E Image Generator.\n",
      "\n",
      "utilities.dataforseo_api_search.DataForSeoAPIWrapper\n",
      "Wrapper around the DataForSeo API.\n",
      "\n",
      "utilities.duckduckgo_search.DuckDuckGoSearchAPIWrapper\n",
      "Wrapper for DuckDuckGo Search API.\n",
      "\n",
      "utilities.github.GitHubAPIWrapper\n",
      "Wrapper for GitHub API.\n",
      "\n",
      "utilities.gitlab.GitLabAPIWrapper\n",
      "Wrapper for GitLab API.\n",
      "\n",
      "utilities.golden_query.GoldenQueryAPIWrapper\n",
      "Wrapper for Golden.\n",
      "\n",
      "utilities.google_places_api.GooglePlacesAPIWrapper\n",
      "Wrapper around Google Places API.\n",
      "\n",
      "utilities.google_scholar.GoogleScholarAPIWrapper\n",
      "Wrapper for Google Scholar API\n",
      "\n",
      "utilities.google_search.GoogleSearchAPIWrapper\n",
      "Wrapper for Google Search API.\n",
      "\n",
      "utilities.google_serper.GoogleSerperAPIWrapper\n",
      "Wrapper around the Serper.dev Google Search API.\n",
      "\n",
      "utilities.graphql.GraphQLAPIWrapper\n",
      "Wrapper around GraphQL API.\n",
      "\n",
      "utilities.jira.JiraAPIWrapper\n",
      "Wrapper for Jira API.\n",
      "\n",
      "utilities.max_compute.MaxComputeAPIWrapper(client)\n",
      "Interface for querying Alibaba Cloud MaxCompute tables.\n",
      "\n",
      "utilities.metaphor_search.MetaphorSearchAPIWrapper\n",
      "Wrapper for Metaphor Search API.\n",
      "\n",
      "utilities.openapi.HTTPVerb(value[, names, ...])\n",
      "Enumerator of the HTTP verbs.\n",
      "\n",
      "utilities.openapi.OpenAPISpec()\n",
      "OpenAPI Model that removes mis-formatted parts of the spec.\n",
      "\n",
      "utilities.openweathermap.OpenWeatherMapAPIWrapper\n",
      "Wrapper for OpenWeatherMap API using PyOWM.\n",
      "\n",
      "utilities.portkey.Portkey()\n",
      "Portkey configuration.\n",
      "\n",
      "utilities.powerbi.PowerBIDataset\n",
      "Create PowerBI engine from dataset ID and credential or token.\n",
      "\n",
      "utilities.pubmed.PubMedAPIWrapper\n",
      "Wrapper around PubMed API.\n",
      "\n",
      "utilities.python.PythonREPL\n",
      "Simulates a standalone Python REPL.\n",
      "\n",
      "utilities.redis.TokenEscaper([escape_chars_re])\n",
      "Escape punctuation within an input string.\n",
      "\n",
      "utilities.requests.Requests\n",
      "Wrapper around requests to handle auth and async.\n",
      "\n",
      "utilities.requests.RequestsWrapper\n",
      "alias of TextRequestsWrapper\n",
      "\n",
      "utilities.requests.TextRequestsWrapper\n",
      "Lightweight wrapper around requests library.\n",
      "\n",
      "utilities.scenexplain.SceneXplainAPIWrapper\n",
      "Wrapper for SceneXplain API.\n",
      "\n",
      "utilities.searchapi.SearchApiAPIWrapper\n",
      "Wrapper around SearchApi API.\n",
      "\n",
      "utilities.searx_search.SearxResults(data)\n",
      "Dict like wrapper around search api results.\n",
      "\n",
      "utilities.searx_search.SearxSearchWrapper\n",
      "Wrapper for Searx API.\n",
      "\n",
      "utilities.serpapi.HiddenPrints()\n",
      "Context manager to hide prints.\n",
      "\n",
      "utilities.serpapi.SerpAPIWrapper\n",
      "Wrapper around SerpAPI.\n",
      "\n",
      "utilities.spark_sql.SparkSQL([...])\n",
      "SparkSQL is a utility class for interacting with Spark SQL.\n",
      "\n",
      "utilities.sql_database.SQLDatabase(engine[, ...])\n",
      "SQLAlchemy wrapper around a database.\n",
      "\n",
      "utilities.tavily_search.TavilySearchAPIWrapper\n",
      "Wrapper for Tavily Search API.\n",
      "\n",
      "utilities.tensorflow_datasets.TensorflowDatasets\n",
      "Access to the TensorFlow Datasets.\n",
      "\n",
      "utilities.twilio.TwilioAPIWrapper\n",
      "Messaging Client using Twilio.\n",
      "\n",
      "utilities.wikipedia.WikipediaAPIWrapper\n",
      "Wrapper around WikipediaAPI.\n",
      "\n",
      "utilities.wolfram_alpha.WolframAlphaAPIWrapper\n",
      "Wrapper for Wolfram Alpha.\n",
      "\n",
      "utilities.zapier.ZapierNLAWrapper\n",
      "Wrapper for Zapier NLA.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "utilities.anthropic.get_num_tokens_anthropic(text)\n",
      "Get the number of tokens in a string of text.\n",
      "\n",
      "utilities.anthropic.get_token_ids_anthropic(text)\n",
      "Get the token ids for a string of text.\n",
      "\n",
      "utilities.clickup.extract_dict_elements_from_component_fields(...)\n",
      "Extract elements from a dictionary.\n",
      "\n",
      "utilities.clickup.fetch_data(url, access_token)\n",
      "Fetch data from a URL.\n",
      "\n",
      "utilities.clickup.fetch_first_id(data, key)\n",
      "Fetch the first id from a dictionary.\n",
      "\n",
      "utilities.clickup.fetch_folder_id(space_id, ...)\n",
      "Fetch the folder id.\n",
      "\n",
      "utilities.clickup.fetch_list_id(space_id, ...)\n",
      "Fetch the list id.\n",
      "\n",
      "utilities.clickup.fetch_space_id(team_id, ...)\n",
      "Fetch the space id.\n",
      "\n",
      "utilities.clickup.fetch_team_id(access_token)\n",
      "Fetch the team id.\n",
      "\n",
      "utilities.clickup.load_query(query[, ...])\n",
      "Attempts to parse a JSON string and return the parsed object.\n",
      "\n",
      "utilities.clickup.parse_dict_through_component(...)\n",
      "Parse a dictionary by creating a component and then turning it back into a dictionary.\n",
      "\n",
      "utilities.opaqueprompts.desanitize(...)\n",
      "Restore the original sensitive data from the sanitized text.\n",
      "\n",
      "utilities.opaqueprompts.sanitize(input)\n",
      "Sanitize input string or dict of strings by replacing sensitive data with placeholders.\n",
      "\n",
      "utilities.powerbi.fix_table_name(table)\n",
      "Add single quotes around table names that contain spaces.\n",
      "\n",
      "utilities.powerbi.json_to_md(json_contents)\n",
      "Converts a JSON object to a markdown table.\n",
      "\n",
      "utilities.redis.check_redis_module_exist(...)\n",
      "Check if the correct Redis modules are installed.\n",
      "\n",
      "utilities.redis.get_client(redis_url, **kwargs)\n",
      "Get a redis client from the connection url given.\n",
      "\n",
      "utilities.sql_database.truncate_word(...[, ...])\n",
      "Truncate a string to a certain number of words, based on the max string length.\n",
      "\n",
      "utilities.vertexai.get_client_info([module])\n",
      "Returns a custom user agent header.\n",
      "\n",
      "utilities.vertexai.init_vertexai([project, ...])\n",
      "Init vertexai.\n",
      "\n",
      "utilities.vertexai.raise_vertex_import_error([...])\n",
      "Raise ImportError related to Vertex SDK being not available.\n",
      "\n",
      "langchain.utils¶\n",
      "Utility functions for LangChain.\n",
      "These functions do not depend on any other LangChain module.\n",
      "\n",
      "Classes¶\n",
      "\n",
      "utils.ernie_functions.FunctionDescription\n",
      "Representation of a callable function to the Ernie API.\n",
      "\n",
      "utils.ernie_functions.ToolDescription\n",
      "Representation of a callable function to the Ernie API.\n",
      "\n",
      "utils.openai_functions.FunctionDescription\n",
      "Representation of a callable function to the OpenAI API.\n",
      "\n",
      "utils.openai_functions.ToolDescription\n",
      "Representation of a callable function to the OpenAI API.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "utils.env.get_from_dict_or_env(data, key, ...)\n",
      "Get a value from a dictionary or an environment variable.\n",
      "\n",
      "utils.env.get_from_env(key, env_key[, default])\n",
      "Get a value from a dictionary or an environment variable.\n",
      "\n",
      "utils.ernie_functions.convert_pydantic_to_ernie_function(...)\n",
      "Converts a Pydantic model to a function description for the Ernie API.\n",
      "\n",
      "utils.ernie_functions.convert_pydantic_to_ernie_tool(...)\n",
      "Converts a Pydantic model to a function description for the Ernie API.\n",
      "\n",
      "utils.html.extract_sub_links(raw_html, url, *)\n",
      "Extract all links from a raw html string and convert into absolute paths.\n",
      "\n",
      "utils.html.find_all_links(raw_html, *[, pattern])\n",
      "Extract all links from a raw html string.\n",
      "\n",
      "utils.json_schema.dereference_refs(schema_obj, *)\n",
      "Try to substitute $refs in JSON Schema.\n",
      "\n",
      "utils.math.cosine_similarity(X, Y)\n",
      "Row-wise cosine similarity between two equal-width matrices.\n",
      "\n",
      "utils.math.cosine_similarity_top_k(X, Y[, ...])\n",
      "Row-wise cosine similarity with optional top-k and score threshold filtering.\n",
      "\n",
      "utils.openai.is_openai_v1()\n",
      "\n",
      "utils.openai_functions.convert_pydantic_to_openai_function(...)\n",
      "Converts a Pydantic model to a function description for the OpenAI API.\n",
      "\n",
      "utils.openai_functions.convert_pydantic_to_openai_tool(...)\n",
      "Converts a Pydantic model to a function description for the OpenAI API.\n",
      "\n",
      "utils.strings.comma_list(items)\n",
      "Convert a list to a comma-separated string.\n",
      "\n",
      "utils.strings.stringify_dict(data)\n",
      "Stringify a dictionary.\n",
      "\n",
      "utils.strings.stringify_value(val)\n",
      "Stringify a value.\n",
      "\n",
      "langchain.vectorstores¶\n",
      "Vector store stores embedded data and performs vector search.\n",
      "One of the most common ways to store and search over unstructured data is to\n",
      "embed it and store the resulting embedding vectors, and then query the store\n",
      "and retrieve the data that are ‘most similar’ to the embedded query.\n",
      "Class hierarchy:\n",
      "VectorStore --> <name>  # Examples: Annoy, FAISS, Milvus\n",
      "\n",
      "BaseRetriever --> VectorStoreRetriever --> <name>Retriever  # Example: VespaRetriever\n",
      "\n",
      "Main helpers:\n",
      "Embeddings, Document\n",
      "\n",
      "Classes¶\n",
      "\n",
      "vectorstores.alibabacloud_opensearch.AlibabaCloudOpenSearch(...)\n",
      "Alibaba Cloud OpenSearch vector store.\n",
      "\n",
      "vectorstores.alibabacloud_opensearch.AlibabaCloudOpenSearchSettings(...)\n",
      "Alibaba Cloud Opensearch` client configuration.\n",
      "\n",
      "vectorstores.analyticdb.AnalyticDB(...[, ...])\n",
      "AnalyticDB (distributed PostgreSQL) vector store.\n",
      "\n",
      "vectorstores.annoy.Annoy(embedding_function, ...)\n",
      "Annoy vector store.\n",
      "\n",
      "vectorstores.astradb.AstraDB(*, embedding, ...)\n",
      "Wrapper around DataStax Astra DB for vector-store workloads.\n",
      "\n",
      "vectorstores.atlas.AtlasDB(name[, ...])\n",
      "Atlas vector store.\n",
      "\n",
      "vectorstores.awadb.AwaDB([table_name, ...])\n",
      "AwaDB vector store.\n",
      "\n",
      "vectorstores.azure_cosmos_db.AzureCosmosDBVectorSearch(...)\n",
      "Azure Cosmos DB for MongoDB vCore vector store.\n",
      "\n",
      "vectorstores.azure_cosmos_db.CosmosDBSimilarityType(value)\n",
      "Cosmos DB Similarity Type as enumerator.\n",
      "\n",
      "vectorstores.azuresearch.AzureSearch(...[, ...])\n",
      "Azure Cognitive Search vector store.\n",
      "\n",
      "vectorstores.azuresearch.AzureSearchVectorStoreRetriever\n",
      "Retriever that uses Azure Cognitive Search.\n",
      "\n",
      "vectorstores.bageldb.Bagel([cluster_name, ...])\n",
      "BagelDB.ai vector store.\n",
      "\n",
      "vectorstores.baiducloud_vector_search.BESVectorStore(...)\n",
      "Baidu Elasticsearch vector store.\n",
      "\n",
      "vectorstores.cassandra.Cassandra(embedding, ...)\n",
      "Wrapper around Apache Cassandra(R) for vector-store workloads.\n",
      "\n",
      "vectorstores.chroma.Chroma([...])\n",
      "ChromaDB vector store.\n",
      "\n",
      "vectorstores.clarifai.Clarifai([user_id, ...])\n",
      "Clarifai AI vector store.\n",
      "\n",
      "vectorstores.clickhouse.Clickhouse(embedding)\n",
      "ClickHouse VectorSearch vector store.\n",
      "\n",
      "vectorstores.clickhouse.ClickhouseSettings\n",
      "ClickHouse client configuration.\n",
      "\n",
      "vectorstores.dashvector.DashVector(...)\n",
      "DashVector vector store.\n",
      "\n",
      "vectorstores.deeplake.DeepLake([...])\n",
      "Activeloop Deep Lake vector store.\n",
      "\n",
      "vectorstores.dingo.Dingo(embedding, text_key, *)\n",
      "Dingo vector store.\n",
      "\n",
      "vectorstores.docarray.base.DocArrayIndex(...)\n",
      "Base class for DocArray based vector stores.\n",
      "\n",
      "vectorstores.docarray.hnsw.DocArrayHnswSearch(...)\n",
      "HnswLib storage using DocArray package.\n",
      "\n",
      "vectorstores.docarray.in_memory.DocArrayInMemorySearch(...)\n",
      "In-memory DocArray storage for exact search.\n",
      "\n",
      "vectorstores.elastic_vector_search.ElasticKnnSearch(...)\n",
      "[Deprecated]  [DEPRECATED] Elasticsearch with k-nearest neighbor search (k-NN) vector store.\n",
      "\n",
      "vectorstores.elastic_vector_search.ElasticVectorSearch(...)\n",
      "ElasticVectorSearch uses the brute force method of searching on vectors.\n",
      "\n",
      "vectorstores.elasticsearch.ApproxRetrievalStrategy([...])\n",
      "Approximate retrieval strategy using the HNSW algorithm.\n",
      "\n",
      "vectorstores.elasticsearch.BaseRetrievalStrategy()\n",
      "Base class for Elasticsearch retrieval strategies.\n",
      "\n",
      "vectorstores.elasticsearch.ElasticsearchStore(...)\n",
      "Elasticsearch vector store.\n",
      "\n",
      "vectorstores.elasticsearch.ExactRetrievalStrategy()\n",
      "Exact retrieval strategy using the script_score query.\n",
      "\n",
      "vectorstores.elasticsearch.SparseRetrievalStrategy([...])\n",
      "Sparse retrieval strategy using the text_expansion processor.\n",
      "\n",
      "vectorstores.epsilla.Epsilla(client, embeddings)\n",
      "Wrapper around Epsilla vector database.\n",
      "\n",
      "vectorstores.faiss.FAISS(embedding_function, ...)\n",
      "Meta Faiss vector store.\n",
      "\n",
      "vectorstores.hippo.Hippo(embedding_function)\n",
      "Hippo vector store.\n",
      "\n",
      "vectorstores.hologres.Hologres(...[, ndims, ...])\n",
      "Hologres API vector store.\n",
      "\n",
      "vectorstores.hologres.HologresWrapper(...)\n",
      "Hologres API wrapper.\n",
      "\n",
      "vectorstores.lancedb.LanceDB(connection, ...)\n",
      "LanceDB vector store.\n",
      "\n",
      "vectorstores.llm_rails.LLMRails([...])\n",
      "Implementation of Vector Store using LLMRails.\n",
      "\n",
      "vectorstores.llm_rails.LLMRailsRetriever\n",
      "Retriever for LLMRails.\n",
      "\n",
      "vectorstores.marqo.Marqo(client, index_name)\n",
      "Marqo vector store.\n",
      "\n",
      "vectorstores.matching_engine.MatchingEngine(...)\n",
      "Google Vertex AI Matching Engine vector store.\n",
      "\n",
      "vectorstores.meilisearch.Meilisearch(embedding)\n",
      "Meilisearch vector store.\n",
      "\n",
      "vectorstores.milvus.Milvus(embedding_function)\n",
      "Milvus vector store.\n",
      "\n",
      "vectorstores.momento_vector_index.MomentoVectorIndex(...)\n",
      "Momento Vector Index (MVI) vector store.\n",
      "\n",
      "vectorstores.mongodb_atlas.MongoDBAtlasVectorSearch(...)\n",
      "MongoDB Atlas Vector Search vector store.\n",
      "\n",
      "vectorstores.myscale.MyScale(embedding[, config])\n",
      "MyScale vector store.\n",
      "\n",
      "vectorstores.myscale.MyScaleSettings\n",
      "MyScale client configuration.\n",
      "\n",
      "vectorstores.myscale.MyScaleWithoutJSON(...)\n",
      "MyScale vector store without metadata column\n",
      "\n",
      "vectorstores.neo4j_vector.Neo4jVector(...[, ...])\n",
      "Neo4j vector index.\n",
      "\n",
      "vectorstores.neo4j_vector.SearchType(value)\n",
      "Enumerator of the Distance strategies.\n",
      "\n",
      "vectorstores.nucliadb.NucliaDB(...[, ...])\n",
      "NucliaDB vector store.\n",
      "\n",
      "vectorstores.opensearch_vector_search.OpenSearchVectorSearch(...)\n",
      "Amazon OpenSearch Vector Engine vector store.\n",
      "\n",
      "vectorstores.pgembedding.BaseModel(**kwargs)\n",
      "Base model for all SQL stores.\n",
      "\n",
      "vectorstores.pgembedding.CollectionStore(...)\n",
      "Collection store.\n",
      "\n",
      "vectorstores.pgembedding.EmbeddingStore(**kwargs)\n",
      "Embedding store.\n",
      "\n",
      "vectorstores.pgembedding.PGEmbedding(...[, ...])\n",
      "Postgres with the pg_embedding extension as a vector store.\n",
      "\n",
      "vectorstores.pgembedding.QueryResult()\n",
      "Result from a query.\n",
      "\n",
      "vectorstores.pgvecto_rs.PGVecto_rs(...[, ...])\n",
      "\n",
      "vectorstores.pgvector.BaseModel(**kwargs)\n",
      "Base model for the SQL stores.\n",
      "\n",
      "vectorstores.pgvector.DistanceStrategy(value)\n",
      "Enumerator of the Distance strategies.\n",
      "\n",
      "vectorstores.pgvector.PGVector(...[, ...])\n",
      "Postgres/PGVector vector store.\n",
      "\n",
      "vectorstores.pinecone.Pinecone(index, ...[, ...])\n",
      "Pinecone vector store.\n",
      "\n",
      "vectorstores.qdrant.Qdrant(client, ...[, ...])\n",
      "Qdrant vector store.\n",
      "\n",
      "vectorstores.qdrant.QdrantException\n",
      "Qdrant related exceptions.\n",
      "\n",
      "vectorstores.redis.base.Redis(redis_url, ...)\n",
      "Redis vector database.\n",
      "\n",
      "vectorstores.redis.base.RedisVectorStoreRetriever\n",
      "Retriever for Redis VectorStore.\n",
      "\n",
      "vectorstores.redis.filters.RedisFilter()\n",
      "Collection of RedisFilterFields.\n",
      "\n",
      "vectorstores.redis.filters.RedisFilterExpression([...])\n",
      "A logical expression of RedisFilterFields.\n",
      "\n",
      "vectorstores.redis.filters.RedisFilterField(field)\n",
      "Base class for RedisFilterFields.\n",
      "\n",
      "vectorstores.redis.filters.RedisFilterOperator(value)\n",
      "RedisFilterOperator enumerator is used to create RedisFilterExpressions.\n",
      "\n",
      "vectorstores.redis.filters.RedisNum(field)\n",
      "A RedisFilterField representing a numeric field in a Redis index.\n",
      "\n",
      "vectorstores.redis.filters.RedisTag(field)\n",
      "A RedisFilterField representing a tag in a Redis index.\n",
      "\n",
      "vectorstores.redis.filters.RedisText(field)\n",
      "A RedisFilterField representing a text field in a Redis index.\n",
      "\n",
      "vectorstores.redis.schema.FlatVectorField\n",
      "Schema for flat vector fields in Redis.\n",
      "\n",
      "vectorstores.redis.schema.HNSWVectorField\n",
      "Schema for HNSW vector fields in Redis.\n",
      "\n",
      "vectorstores.redis.schema.NumericFieldSchema\n",
      "Schema for numeric fields in Redis.\n",
      "\n",
      "vectorstores.redis.schema.RedisDistanceMetric(value)\n",
      "Distance metrics for Redis vector fields.\n",
      "\n",
      "vectorstores.redis.schema.RedisField\n",
      "Base class for Redis fields.\n",
      "\n",
      "vectorstores.redis.schema.RedisModel\n",
      "Schema for Redis index.\n",
      "\n",
      "vectorstores.redis.schema.RedisVectorField\n",
      "Base class for Redis vector fields.\n",
      "\n",
      "vectorstores.redis.schema.TagFieldSchema\n",
      "Schema for tag fields in Redis.\n",
      "\n",
      "vectorstores.redis.schema.TextFieldSchema\n",
      "Schema for text fields in Redis.\n",
      "\n",
      "vectorstores.rocksetdb.Rockset(client, ...)\n",
      "Rockset vector store.\n",
      "\n",
      "vectorstores.scann.ScaNN(embedding, index, ...)\n",
      "ScaNN vector store.\n",
      "\n",
      "vectorstores.semadb.SemaDB(collection_name, ...)\n",
      "SemaDB vector store.\n",
      "\n",
      "vectorstores.singlestoredb.SingleStoreDB(...)\n",
      "SingleStore DB vector store.\n",
      "\n",
      "vectorstores.sklearn.BaseSerializer(persist_path)\n",
      "Base class for serializing data.\n",
      "\n",
      "vectorstores.sklearn.BsonSerializer(persist_path)\n",
      "Serializes data in binary json using the bson python package.\n",
      "\n",
      "vectorstores.sklearn.JsonSerializer(persist_path)\n",
      "Serializes data in json using the json package from python standard library.\n",
      "\n",
      "vectorstores.sklearn.ParquetSerializer(...)\n",
      "Serializes data in Apache Parquet format using the pyarrow package.\n",
      "\n",
      "vectorstores.sklearn.SKLearnVectorStore(...)\n",
      "Simple in-memory vector store based on the scikit-learn library NearestNeighbors implementation.\n",
      "\n",
      "vectorstores.sklearn.SKLearnVectorStoreException\n",
      "Exception raised by SKLearnVectorStore.\n",
      "\n",
      "vectorstores.sqlitevss.SQLiteVSS(table, ...)\n",
      "Wrapper around SQLite with vss extension as a vector database.\n",
      "\n",
      "vectorstores.starrocks.StarRocks(embedding)\n",
      "StarRocks vector store.\n",
      "\n",
      "vectorstores.starrocks.StarRocksSettings\n",
      "StarRocks client configuration.\n",
      "\n",
      "vectorstores.supabase.SupabaseVectorStore(...)\n",
      "Supabase Postgres vector store.\n",
      "\n",
      "vectorstores.tair.Tair(embedding_function, ...)\n",
      "Tair vector store.\n",
      "\n",
      "vectorstores.tencentvectordb.ConnectionParams(...)\n",
      "Tencent vector DB Connection params.\n",
      "\n",
      "vectorstores.tencentvectordb.IndexParams(...)\n",
      "Tencent vector DB Index params.\n",
      "\n",
      "vectorstores.tencentvectordb.TencentVectorDB(...)\n",
      "Initialize wrapper around the tencent vector database.\n",
      "\n",
      "vectorstores.tigris.Tigris(client, ...)\n",
      "Tigris vector store.\n",
      "\n",
      "vectorstores.tiledb.TileDB(embedding, ...[, ...])\n",
      "Wrapper around TileDB vector database.\n",
      "\n",
      "vectorstores.timescalevector.TimescaleVector(...)\n",
      "VectorStore implementation using the timescale vector client to store vectors in Postgres.\n",
      "\n",
      "vectorstores.typesense.Typesense(...[, ...])\n",
      "Typesense vector store.\n",
      "\n",
      "vectorstores.usearch.USearch(embedding, ...)\n",
      "USearch vector store.\n",
      "\n",
      "vectorstores.utils.DistanceStrategy(value[, ...])\n",
      "Enumerator of the Distance strategies for calculating distances between vectors.\n",
      "\n",
      "vectorstores.vald.Vald(embedding[, host, ...])\n",
      "Wrapper around Vald vector database.\n",
      "\n",
      "vectorstores.vearch.Vearch(embedding_function)\n",
      "Initialize vearch vector store flag 1 for cluster,0 for standalone\n",
      "\n",
      "vectorstores.vectara.Vectara([...])\n",
      "Vectara API vector store.\n",
      "\n",
      "vectorstores.vectara.VectaraRetriever\n",
      "Retriever class for Vectara.\n",
      "\n",
      "vectorstores.vespa.VespaStore(app[, ...])\n",
      "Vespa vector store.\n",
      "\n",
      "vectorstores.weaviate.Weaviate(client, ...)\n",
      "Weaviate vector store.\n",
      "\n",
      "vectorstores.xata.XataVectorStore(api_key, ...)\n",
      "Xata vector store.\n",
      "\n",
      "vectorstores.zep.CollectionConfig(name, ...)\n",
      "Configuration for a Zep Collection.\n",
      "\n",
      "vectorstores.zep.ZepVectorStore(...[, ...])\n",
      "Zep vector store.\n",
      "\n",
      "vectorstores.zilliz.Zilliz(embedding_function)\n",
      "Zilliz vector store.\n",
      "\n",
      "Functions¶\n",
      "\n",
      "vectorstores.alibabacloud_opensearch.create_metadata(fields)\n",
      "Create metadata from fields.\n",
      "\n",
      "vectorstores.annoy.dependable_annoy_import()\n",
      "Import annoy if available, otherwise raise error.\n",
      "\n",
      "vectorstores.clickhouse.has_mul_sub_str(s, *args)\n",
      "Check if a string contains multiple substrings.\n",
      "\n",
      "vectorstores.faiss.dependable_faiss_import([...])\n",
      "Import faiss if available, otherwise raise error.\n",
      "\n",
      "vectorstores.myscale.has_mul_sub_str(s, *args)\n",
      "Check if a string contains multiple substrings.\n",
      "\n",
      "vectorstores.neo4j_vector.check_if_not_null(...)\n",
      "Check if the values are not None or empty string\n",
      "\n",
      "vectorstores.neo4j_vector.sort_by_index_name(...)\n",
      "Sort first element to match the index_name if exists\n",
      "\n",
      "vectorstores.qdrant.sync_call_fallback(method)\n",
      "Decorator to call the synchronous method of the class if the async method is not implemented.\n",
      "\n",
      "vectorstores.redis.base.check_index_exists(...)\n",
      "Check if Redis index exists.\n",
      "\n",
      "vectorstores.redis.filters.check_operator_misuse(func)\n",
      "Decorator to check for misuse of equality operators.\n",
      "\n",
      "vectorstores.redis.schema.read_schema(...)\n",
      "Reads in the index schema from a dict or yaml file.\n",
      "\n",
      "vectorstores.scann.dependable_scann_import()\n",
      "Import scann if available, otherwise raise error.\n",
      "\n",
      "vectorstores.scann.normalize(x)\n",
      "Normalize vectors to unit length.\n",
      "\n",
      "vectorstores.starrocks.debug_output(s)\n",
      "Print a debug message if DEBUG is True.\n",
      "\n",
      "vectorstores.starrocks.get_named_result(...)\n",
      "Get a named result from a query.\n",
      "\n",
      "vectorstores.starrocks.has_mul_sub_str(s, *args)\n",
      "Check if a string has multiple substrings.\n",
      "\n",
      "vectorstores.tiledb.dependable_tiledb_import()\n",
      "Import tiledb-vector-search if available, otherwise raise error.\n",
      "\n",
      "vectorstores.tiledb.get_documents_array_uri(uri)\n",
      "\n",
      "vectorstores.tiledb.get_documents_array_uri_from_group(group)\n",
      "\n",
      "vectorstores.tiledb.get_vector_index_uri(uri)\n",
      "\n",
      "vectorstores.tiledb.get_vector_index_uri_from_group(group)\n",
      "\n",
      "vectorstores.usearch.dependable_usearch_import()\n",
      "Import usearch if available, otherwise raise error.\n",
      "\n",
      "vectorstores.utils.filter_complex_metadata(...)\n",
      "Filter out metadata types that are not supported for a vector store.\n",
      "\n",
      "vectorstores.utils.maximal_marginal_relevance(...)\n",
      "Calculate maximal marginal relevance.\n",
      "\n",
      "            © 2023, Harrison Chase.\n",
      "          Last updated on Nov 23, 2023.\n",
      "          Show this page source\n"
     ]
    }
   ],
   "source": [
    "print(docs_from_api[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c961d6ff-b6e2-4931-bad3-2dd6f55bbc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###load() start\n",
      "###requests.get start\n",
      "Loaded 1 docs from API\n"
     ]
    }
   ],
   "source": [
    "docs_from_wiki = load_wiki_docs()\n",
    "print(f\"Loaded {len(docs_from_wiki)} docs from API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2eac1218-e26c-4207-a653-54c9f6f35f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03. 검색 인프라 - 추천 검색 플랫폼 - Global Site\n",
      "\n",
      "Skip to main content\n",
      "assistive.skiplink.to.breadcrumbs\n",
      "assistive.skiplink.to.header.menu\n",
      "assistive.skiplink.to.action.menu\n",
      "assistive.skiplink.to.quick.search\n",
      "\n",
      "윤태형(TaeHyoung Yun)/커머스플랫폼개발팀/SKP\n",
      "\n",
      "\t\t\t                                Personal space\n",
      "                    \n",
      "\n",
      "\t\t\t                                Recently viewed\n",
      "                    \n",
      "\n",
      "\t\t\t                                Recently worked on\n",
      "                    \n",
      "\n",
      "\t\t\t                                User dashboard\n",
      "                    \n",
      "\n",
      "\t\t\t                                Profile\n",
      "                    \n",
      "\n",
      "\t\t\t                                Tasks\n",
      "                    \n",
      "\n",
      "\t\t\t                                Saved for later\n",
      "                    \n",
      "\n",
      "\t\t\t                                Watches\n",
      "                    \n",
      "\n",
      "\t\t\t                                Drafts\n",
      "                    \n",
      "\n",
      "\t\t\t                                Network\n",
      "                    \n",
      "\n",
      "\t\t\t                                Settings\n",
      "                    \n",
      "\n",
      "\t\t\t                                Atlassian Marketplace\n",
      "                    \n",
      "\n",
      "\t\t\t                                Gliffy Diagram\n",
      "                    \n",
      "\n",
      "\t\t\t                                Log Out\n",
      "                    \n",
      "\n",
      "Help / Tips\n",
      "Help\n",
      "\n",
      "\t\t\t                                Online Help\n",
      "                    \n",
      "\n",
      "\t\t\t                                Keyboard Shortcuts\n",
      "                    \n",
      "\n",
      "\t\t\t                                Feed Builder\n",
      "                    \n",
      "\n",
      "\t\t\t                                What’s new\n",
      "                    \n",
      "\n",
      "\t\t\t                                Available Gadgets\n",
      "                    \n",
      "\n",
      "\t\t\t                                About Confluence\n",
      "                    \n",
      "\n",
      "Search\n",
      "\n",
      "Quick Search\n",
      "\n",
      "Hit enter to search\n",
      "\n",
      "Create content\n",
      "\n",
      "                            추천 검색 플랫폼\n",
      "                                                                            \n",
      "\n",
      "Content\n",
      "\n",
      "Pages\n",
      "\n",
      "Blog\n",
      "\n",
      "Calendars\n",
      "\n",
      "Reorder Pages\n",
      "\n",
      "Space Tools\n",
      "\n",
      "\t\t  \t\t  \t\t\t\t  Settings\n",
      "\t\t\t\t\t  \t\t\n",
      "\n",
      "\t\t  \t\t  \t\t\t\t  Content Tools\n",
      "\t\t\t\t\t  \t\t\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "   \n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "295656385\n",
      "personalTF\n",
      " \n",
      "3\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Edit\n",
      "             \n",
      "\n",
      "                                Save for later\n",
      "             \n",
      "\n",
      "Watch\n",
      "             \n",
      "\n",
      "Share\n",
      "             \n",
      "\n",
      " \n",
      "\n",
      "                                Attachments (1)\n",
      "             \n",
      "\n",
      "                                Page History\n",
      "             \n",
      "\n",
      "                                Restrictions\n",
      "             \n",
      "\n",
      "                                People who can view\n",
      "             \n",
      "\n",
      "                                Page Information\n",
      "             \n",
      "\n",
      "                                Resolved comments\n",
      "             \n",
      "\n",
      "                                View in Hierarchy\n",
      "             \n",
      "\n",
      "                                View Storage Format\n",
      "             \n",
      "\n",
      "                                View Source\n",
      "             \n",
      "\n",
      "                                Export to PDF\n",
      "             \n",
      "\n",
      "                                Export to Word\n",
      "             \n",
      "\n",
      "                                Import Word Document\n",
      "             \n",
      "\n",
      "                                Hide Inline Comments\n",
      "             \n",
      "\n",
      "                                Copy\n",
      "             \n",
      "\n",
      "                                Move\n",
      "             \n",
      "\n",
      "                                Delete\n",
      "             \n",
      "\n",
      "Pages\n",
      "\n",
      "추천 검색 플랫폼\n",
      "\n",
      "Skip to end of banner\n",
      "\n",
      "Jira links\n",
      "Go to start of banner\n",
      "\n",
      "03. 검색 인프라\n",
      "\n",
      "            \n",
      "        \n",
      "    \n",
      "    \n",
      "        \n",
      "    \n",
      "        \n",
      "            \n",
      "            Created by  Unknown User (1002383), last modified by  윤태형(TaeHyoung Yun)/커머스플랫폼개발팀/SKP on Aug 11, 2023\n",
      "\n",
      " \n",
      "\n",
      "검색 | Overview검색 | 사업/기획 요건 개발검색 | 자체 신규 개발검색 | API검색 | 검색 데이터 관리검색 | 검색 사전 관리검색 | 배치, 통계검색 | Admin검색 | Infra검색 | 운영검색 | 시험검색 | 발표 / 공유자료검색 | QA검색 | 통합테스트BMCC - M3199 (커머스_공통)조직 및 R&R팀명구성원역할커머스플랫폼개발팀윤태형, 최해성, 정옥균이동철T검색 인프라 Ownership검색 색인, API, Admin (SODAR BE) 개발검색 NLU 개발AWS OpenSearch 구축검색 품질 관리커머스&솔루션기획팀정인숙OCB, Syrup 검색 기획ICT사업팀정윤정V컬러링 검색 기획\n",
      "\n",
      "            No labels\n",
      "        \n",
      "\n",
      "Edit Labels\n",
      "\n",
      "Write a comment...\n",
      "\n",
      "Add Comment\n",
      "\n",
      " \n",
      "\n",
      "null\n"
     ]
    }
   ],
   "source": [
    "print(docs_from_wiki[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845cea67-1ccc-4bbf-a6a3-9a6d59250376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cf3023e1-000c-410d-8db7-ce621d75a0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5b1e3f8c-048d-442d-ab8d-b3f0eadc8568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-28 15:00:28\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "print(now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7bf8b033-8ac7-4e6e-b13b-ecdfb0055c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_selected = [ docs_from_documentaion[0], docs_from_documentaion[1], docs_from_documentaion[2], docs_from_documentaion[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "25998290-1cc3-4cb6-b34e-d17d56e88c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start docs transform\n",
      "Start index\n",
      "Indexing stats:  {'num_added': 20, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\n"
     ]
    }
   ],
   "source": [
    "ingest_docs(docs_selected, docs_from_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fdfeac76-14f8-466c-943f-fc1cd6948bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-28 15:00:34\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "print(now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0877ebb1-abcb-4e20-8119-00f2156be72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = record_manager.list_keys()\n",
    "len(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "eca57235-71fe-4a08-84db-bf17dcbb3c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectorstore.similarity_search(\"initial\", k=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4bc7b7bc-839f-422b-8d93-e623e804535f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "Initial document\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b824b892-e9d3-4fba-be30-9ce54338b040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af0b35f-0a07-4086-8683-717b21c080bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ac4c60-7f79-42c3-8b69-2dcbc71deb34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa62c7b-57e9-457d-8a2d-e79308235c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542a7967-07e8-403a-af36-c46d13fcfc42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dee1da0-ffe0-4817-906d-179ab7c1a1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121590cc-16a3-4b3a-a0ef-dfbaf653f6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
